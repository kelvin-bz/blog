[
  
  {
    "title": "It's Quiz Time! üéØ",
    "url": "/posts/programming-quiz-hub/",
    "categories": "mern, mongodb",
    "tags": "mern, mongodb",
    "date": "2024-09-21 07:00:00 +0700",
    





    
    "snippet": "Welcome to Programming Quiz Hub üéØBoost your skills with interactive quizzes across programming and tech topics! Test your knowledge, explore new concepts, and track your progress with fun and engag...",
    "content": "Welcome to Programming Quiz Hub üéØBoost your skills with interactive quizzes across programming and tech topics! Test your knowledge, explore new concepts, and track your progress with fun and engaging challenges. Start learning today! üöÄNo result tracking, no sign-up required. Just pick a quiz and start learning! üìö(https://kelvin-bz.github.io/quiz/)"
  },
  
  {
    "title": "MERN - MongoDB Fundamental",
    "url": "/posts/mongodb-fundamental/",
    "categories": "mern, mongodb",
    "tags": "mern, mongodb",
    "date": "2024-09-20 07:00:00 +0700",
    





    
    "snippet": "MongoDB ArchitectureMongoDB is a popular NoSQL database designed for high performance, high availability, and easy scalability. It stores data in flexible, JSON-like documents, making it easy to wo...",
    "content": "MongoDB ArchitectureMongoDB is a popular NoSQL database designed for high performance, high availability, and easy scalability. It stores data in flexible, JSON-like documents, making it easy to work with structured, semi-structured, and unstructured data.  Database: A container for collections.  Collection: A group of MongoDB documents.  Document: A set of key-value pairs (similar to JSON objects).graph   subgraph mongoDB[\"fa:fa-database MongoDB\"]    database[\"fa:fa-folder Database\"]    database --&gt; collection[\"fa:fa-folder-open Collection\"]    collection --&gt; document[\"fa:fa-file Document\"]  end  style mongoDB stroke:#333,stroke-width:2px  style database fill:#ccf,stroke:#f66,stroke-width:2px  style collection fill:#add8e6,stroke:#333,stroke-width:2px  style document fill:#9cf,stroke:#333,stroke-width:2pxCRUD OperationsCRUD stands for Create, Read, Update, and Delete. These are the basic operations for interacting with data in MongoDB.graph  subgraph crudOperations[\"fa:fa-tools CRUD Operations\"]    create[\"fa:fa-plus-circle Create\"] --&gt; read[\"fa:fa-eye Read\"]    read --&gt; update[\"fa:fa-edit Update\"]    update --&gt; delete[\"fa:fa-trash Delete\"]  end  style crudOperations stroke:#333,stroke-width:2px  style create fill:#ccf,stroke:#f66,stroke-width:2px  style read fill:#add8e6,stroke:#333,stroke-width:2px  style update fill:#9cf,stroke:#333,stroke-width:2px  style delete fill:#faa,stroke:#333,stroke-width:2pxCreateTo insert a new document into a collection, you use the insertOne() or insertMany() methods.insertOnedb.collection('users').insertOne({ name: 'Alice', age: 25 });Auto-Generated _id FieldIf you don‚Äôt specify an _id field, MongoDB automatically generates a unique identifier for each document. To specify your own _id value, you can include it in the document.db.collection('users').insertOne({ _id: 1, name: 'Alice', age: 25 });Make sure to handle duplicate _id values to avoid conflicts.insertManyTo insert multiple documents, you can use the insertMany() method.db.collection('users').insertMany([  { name: 'Alice', age: 25 },  { name: 'Bob', age: 30 }]);If one of the documents fails to insert, the operation will be aborted unless you specify the ordered: false option.For example if you have schema vadiation like this. Go to Schema Validationdb.createCollection('users', {  validator: {    $jsonSchema: {      bsonType: 'object',      required: ['name', 'age'],      properties: {        name: { bsonType: 'string' },        age: { bsonType: 'int' }      }    }  }});db.collection('users').insertMany([  { name: 'Alice', age: 25 },  { name: 'Bob', age: 30 },  { name: 'Charlie' } // missing age field], { ordered: false });Third document will be ignored and first two will be inserted.The result would be{  acknowledged: true,  insertedIds: {    '0': ObjectId(\"...\"),  // ID for Alice    '1': ObjectId(\"...\")   // ID for Bob  }}ReadTo read documents from a collection, you use the find() method. The find() method retrives multiple documents and returns a cursor which can be iterated to access the documents.findconst cursor = db.collection('users').find({ age: { $gte: 18 } });In Node.js, you can convert the cursor to an array using the toArray() method.const docs = await cursor.toArray();console.log(docs)Result:[  { \"_id\": 1, \"name\": \"Alice\", \"age\": 25 },  { \"_id\": 2, \"name\": \"Bob\", \"age\": 30 }]findOneTo retrieve a single document, you can use the findOne() method.db.collection('users').findOne({ name: 'Alice' });Result:{ \"_id\": 1, \"name\": \"Alice\", \"age\": 25 }ProjectionYou can specify which fields to include or exclude in the result using the projection parameter.db.collection('users').find({}, { projection: { name: 1, age: 1 } });Result:[  { \"_id\": 1, \"name\": \"Alice\", \"age\": 25 },  { \"_id\": 2, \"name\": \"Bob\", \"age\": 30 }]Using projection is a critical practice, especially when working with large collections. It reduces the amount of data transferred over the network and improves query performance. It also prevent unintentional exposure of sensitive data.UpdateTo update existing documents, you use the updateOne() or updateMany() methods.db.collection('users').updateOne({ name: 'Alice' }, { $set: { age: 26 } });db.collection('users').updateMany({ city: 'New York' }, { $set: { city: 'San Francisco' } });Result Object{  \"acknowledged\": true,  \"matchedCount\": 5,  \"modifiedCount\": 5,  \"upsertedId\": null,  \"upsertedCount\": 0}Base on the result object you can return proper response.const result = await db.collection('users').updateOne(  { _id: userId },  { $set: { age: 30 } });if (result.matchedCount === 0) {  return { success: false, message: 'No matching document found' };}if (result.modifiedCount === 0) {  return { success: true, message: 'Document already up-to-date' };}return { success: true, message: 'Document updated successfully' };UpsertIf you want to insert a new document when no matching document is found, you can use the upsert option.db.collection('users').updateOne(  { name: 'Alice' },  { $set: { age: 26 }  },  { upsert: true });Result Object{  \"acknowledged\": true,  \"matchedCount\": 1,  \"modifiedCount\": 1,  \"upsertedId\": ObjectId(\"...\"),  \"upsertedCount\": 1}findOneAndUpdateThis method is a combination of findOne() and updateOne() as atomic operation. Atomic operation are operations that are performed as a single unit of work. This helps to prevent race conditions and ensure data consistency.// Update or create a user profiledb.users.findOneAndUpdate(    { email: \"alice@example.com\" },    { $set: { age: 26 } },    { returnDocument: 'after' });Result ObjectreturnDocument can be ‚Äòbefore‚Äô or ‚Äòafter‚Äô to return the document before or after the update.{    \"value\": {        \"_id\": ObjectId(\"5f7d3b1c8e1f9a1c9c8e1f9a\"),        \"email\": \"alice@example.com\",        \"age\": 26,        \"name\": \"Alice\"    },    \"lastErrorObject\": {        \"updatedExisting\": true,        \"n\": 1    },    \"ok\": 1}Update Operatorsgraph LR  subgraph updateOperators[\"fa:fa-sync Update Operators\"]    set[\"fa:fa-equals $set\"]    inc[\"fa:fa-plus $inc\"]    mul[\"fa:fa-times $mul\"]    unset[\"fa:fa-trash-alt $unset\"]    rename[\"fa:fa-i-cursor $rename\"]  end  style updateOperators stroke:#333,stroke-width:2px  style set fill:#ccf,stroke:#f66,stroke-width:2px  style inc fill:#add8e6,stroke:#333,stroke-width:2px  style mul fill:#9cf,stroke:#333,stroke-width:2px  style unset fill:#faa,stroke:#333,stroke-width:2px  style rename fill:#f9f,stroke:#333,stroke-width:2px// $set operator to update fieldsdb.collection('users').updateOne({ name: 'Alice' }, { $set: { age: 26, city: 'New York' } });// $set operator to update nested fieldsdb.collection('users').updateOne({ name: 'Alice' }, { $set: { 'address.city': 'New York' } });// $inc operator to increment a fielddb.collection('users').updateOne({ name: 'Alice' }, { $inc: { age: 1 } });// $mul operator to multiply a field valuedb.collection('users').updateOne({ name: 'Alice' }, { $mul: { age: 2 } });// $unset operator to remove a fielddb.collection('users').updateOne({ name: 'Alice' }, { $unset: { city: '' } });// $rename operator to rename a fielddb.collection('users').updateOne({ name: 'Alice' }, { $rename: { city: 'location' } });DeleteTo delete documents from a collection, you use the deleteOne() or deleteMany() methods.db.collection('users').deleteOne({ name: 'Alice' }); // delete first matching documentdb.collection('users').deleteMany({ city: 'New York' });Result Object{  \"acknowledged\": true,  \"deletedCount\": 1}Based on the result object you can return proper response.const result = await db.collection('users').deleteOne({ _id: userId });if (result.deletedCount === 0) {  return { success: false, message: 'No matching document found' };}return { success: true, message: 'Document deleted successfully' };Backup your data: Before deleting documents, make sure to back up your data to prevent accidental data loss.Soft Delete: Instead of permanently deleting documents, you can mark them as deleted by adding a deletedAt field. This allows you to retain the data for auditing or recovery purposes.db.collection('users').updateOne(  { _id: userId },  { $set: { deletedAt: new Date() } });Precise Filter: When deleting documents, use a precise filter to avoid unintentionally deleting more documents than intended. You can use find() to preview the documents that will be deleted before running the delete operation.Schema ValidationModel vs DAO PatternSample Todo ModelModel Pattern  Encapsulates both data and behavior. This may include validation, business logic, and database operations. This can become ‚Äúfat‚Äù but simpler implementation.graph TD    subgraph \"Model Approach\"        A1[routes/todo.routes.js] --&gt; B1[controllers/todo.controller.js]        B1 --&gt; C1[models/todo.model.js]        C1 --&gt; D1[config/database.js]        style C1 fill:#f9f,stroke:#333    endThis is sample of files structure for Model Patternsrc/‚îú‚îÄ‚îÄ config/‚îÇ   ‚îî‚îÄ‚îÄ database.js‚îÇ‚îú‚îÄ‚îÄ models/‚îÇ   ‚îî‚îÄ‚îÄ todo.model.js‚îÇ‚îú‚îÄ‚îÄ controllers/‚îÇ   ‚îî‚îÄ‚îÄ todo.controller.js‚îÇ‚îú‚îÄ‚îÄ routes/‚îÇ   ‚îî‚îÄ‚îÄ todo.routes.js‚îÇ‚îú‚îÄ‚îÄ middleware/‚îÇ   ‚îî‚îÄ‚îÄ auth.middleware.js‚îÇ‚îî‚îÄ‚îÄ app.jsIf the model becomes too complex, you can split it into multiple files or classes. For example, you can have separate files for validation, business logic, and database operations.Data Configuration// src/config/database.jsconst { MongoClient } = require('mongodb');const dbConfig = {  url: process.env.MONGODB_URI || 'mongodb://localhost:27017',  dbName: 'todoapp'};let db = null;const connectDB = async () =&gt; {  try {    const client = await MongoClient.connect(dbConfig.url, {      useUnifiedTopology: true    });    db = client.db(dbConfig.dbName);    console.log('Connected to MongoDB successfully');    return db;  } catch (error) {    console.error('MongoDB connection error:', error);    process.exit(1);  }};const getDB = () =&gt; {  if (!db) {    throw new Error('Database not initialized');  }  return db;};module.exports = { connectDB, getDB };ModelThe model file contains the data structure, validation, and database operations for a todo item. This contains both data and behavior related to todos.// src/models/todo.model.jsonst { ObjectId } = require('mongodb');const { getDB } = require('../config/database');const COLLECTION_NAME = 'todos';// Validation functionsconst validateTodoData = (todoData) =&gt; {  const errors = [];  if (!todoData.title) {    errors.push('Title is required');  } else if (todoData.title.length &lt; 3) {    errors.push('Title must be at least 3 characters long');  }  if (todoData.status &amp;&amp; !['pending', 'in-progress', 'completed'].includes(todoData.status)) {    errors.push('Invalid status. Must be pending, in-progress, or completed');  }  if (todoData.dueDate &amp;&amp; new Date(todoData.dueDate) &lt; new Date()) {    errors.push('Due date cannot be in the past');  }  if (todoData.priority &amp;&amp; !['low', 'medium', 'high'].includes(todoData.priority)) {    errors.push('Invalid priority. Must be low, medium, or high');  }  return errors;};const todoModel = {  async create(todoData) {    const errors = validateTodoData(todoData);    if (errors.length &gt; 0) {      throw new Error(`Validation failed: ${errors.join(', ')}`);    }    const db = getDB();    const todo = {      ...todoData,      title: todoData.title.trim(),      status: todoData.status || 'pending',      priority: todoData.priority || 'medium',      createdAt: new Date(),      updatedAt: new Date(),      completedAt: null    };    const result = await db.collection(COLLECTION_NAME).insertOne(todo);    return { ...todo, _id: result.insertedId };  },  async findById(id) {    if (!ObjectId.isValid(id)) {      throw new Error('Invalid todo ID');    }    const db = getDB();    const todo = await db.collection(COLLECTION_NAME).findOne({       _id: new ObjectId(id)     });    if (!todo) {      throw new Error('Todo not found');    }    return todo;  },  async find(query = {}, options = {}) {    const db = getDB();    const {       page = 1,       limit = 10,      sortBy = 'createdAt',      sortOrder = -1    } = options;    if (page &lt; 1 || limit &lt; 1) {      throw new Error('Invalid pagination parameters');    }    const skip = (page - 1) * limit;    const sortOptions = { [sortBy]: sortOrder };    // Apply filters    const filters = { ...query };    if (filters.priority) {      if (!['low', 'medium', 'high'].includes(filters.priority)) {        throw new Error('Invalid priority filter');      }    }    if (filters.status) {      if (!['pending', 'in-progress', 'completed'].includes(filters.status)) {        throw new Error('Invalid status filter');      }    }    const [todos, totalCount] = await Promise.all([      db.collection(COLLECTION_NAME)        .find(filters)        .sort(sortOptions)        .skip(skip)        .limit(limit)        .toArray(),      db.collection(COLLECTION_NAME)        .countDocuments(filters)    ]);    return {      todos,      pagination: {        total: totalCount,        page,        limit,        pages: Math.ceil(totalCount / limit)      }    };  },  async update(id, updateData) {    if (!ObjectId.isValid(id)) {      throw new Error('Invalid todo ID');    }    const errors = validateTodoData(updateData);    if (errors.length &gt; 0) {      throw new Error(`Validation failed: ${errors.join(', ')}`);    }    const db = getDB();    const existingTodo = await this.findById(id);    // Business logic for status changes    if (updateData.status === 'completed' &amp;&amp; existingTodo.status !== 'completed') {      updateData.completedAt = new Date();    }    if (updateData.status &amp;&amp; updateData.status !== 'completed') {      updateData.completedAt = null;    }    const result = await db.collection(COLLECTION_NAME).findOneAndUpdate(      { _id: new ObjectId(id) },      {         $set: {          ...updateData,          updatedAt: new Date()        }      },      { returnDocument: 'after' }    );    if (!result.value) {      throw new Error('Todo not found');    }    return result.value;  },  async delete(id) {    if (!ObjectId.isValid(id)) {      throw new Error('Invalid todo ID');    }    const db = getDB();    const result = await db.collection(COLLECTION_NAME).deleteOne({      _id: new ObjectId(id)    });    if (result.deletedCount === 0) {      throw new Error('Todo not found');    }    return true;  },  // Additional business logic methods  async markAsComplete(id) {    return await this.update(id, {       status: 'completed'    });  },  async findOverdue() {    const db = getDB();    return await db.collection(COLLECTION_NAME).find({      dueDate: { $lt: new Date() },      status: { $ne: 'completed' }    }).toArray();  }};Sample Todo DAODAO Pattern  Data Access logic is separated from business logic.  Service layer for business logic and DAO layer for data access.  Better for testing by mocking the data access layer.  Better support dependency injection.  More complex implementation and code repetition.graph TD      subgraph \"DAO Pattern Approach\"        A2[routes/todo.routes.js] --&gt; B2[controllers/todo.controller.js]        B2 --&gt; C2[services/todo.service.js]        C2 --&gt; D2[daos/todo.dao.js]        D2 --&gt; E2[models/todo.entity.js]        D2 --&gt; F2[config/database.js]        style C2 fill:#f9f,stroke:#333        style D2 fill:#f9f,stroke:#333        style E2 fill:#f9f,stroke:#333    endsrc/‚îú‚îÄ‚îÄ config/‚îÇ   ‚îî‚îÄ‚îÄ database.js         # Database connection configuration‚îÇ‚îú‚îÄ‚îÄ models/‚îÇ   ‚îî‚îÄ‚îÄ todo.entity.js      # Data structure/schema definition‚îÇ‚îú‚îÄ‚îÄ daos/‚îÇ   ‚îî‚îÄ‚îÄ todo.dao.js         # Data Access Object - handles database operations‚îÇ‚îú‚îÄ‚îÄ services/‚îÇ   ‚îî‚îÄ‚îÄ todo.service.js     # Business logic layer‚îÇ‚îú‚îÄ‚îÄ controllers/‚îÇ   ‚îî‚îÄ‚îÄ todo.controller.js  # Request handling &amp; response formatting‚îÇ‚îú‚îÄ‚îÄ routes/‚îÇ   ‚îî‚îÄ‚îÄ todo.routes.js      # Route definitions‚îÇ‚îî‚îÄ‚îÄ app.js                  # Application entry pointDatabase Configuration// src/config/database.jsconst { MongoClient } = require('mongodb');class Database {  constructor(config) {    this.config = {      url: config.url || process.env.MONGODB_URI || 'mongodb://localhost:27017',      dbName: config.dbName || process.env.DB_NAME || 'todoapp',      options: {        useUnifiedTopology: true,        ...config.options      }    };    this.client = null;    this.db = null;  }  async connect() {    try {      this.client = await MongoClient.connect(this.config.url, this.config.options);      this.db = this.client.db(this.config.dbName);      console.log('Connected to MongoDB successfully');      return this.db;    } catch (error) {      console.error('MongoDB connection error:', error);      throw new DatabaseError('Failed to connect to database', error);    }  }  async disconnect() {    try {      if (this.client) {        await this.client.close();        this.client = null;        this.db = null;        console.log('Disconnected from MongoDB');      }    } catch (error) {      console.error('MongoDB disconnection error:', error);      throw new DatabaseError('Failed to disconnect from database', error);    }  }  getDB() {    if (!this.db) {      throw new DatabaseError('Database not initialized. Call connect() first.');    }    return this.db;  }}Entity// src/models/todo.entity.jsclass Todo {  static STATUS = {    PENDING: 'pending',    IN_PROGRESS: 'in-progress',    COMPLETED: 'completed'  };  static PRIORITY = {    LOW: 'low',    MEDIUM: 'medium',    HIGH: 'high'  };  constructor(data = {}) {    this._id = data._id || null;    this.title = data.title || '';    this.description = data.description || '';    this.status = data.status || Todo.STATUS.PENDING;    this.priority = data.priority || Todo.PRIORITY.MEDIUM;    this.dueDate = data.dueDate ? new Date(data.dueDate) : null;    this.createdAt = data.createdAt ? new Date(data.createdAt) : new Date();    this.updatedAt = data.updatedAt ? new Date(data.updatedAt) : new Date();    this.completedAt = data.completedAt ? new Date(data.completedAt) : null;    this.tags = Array.isArray(data.tags) ? [...data.tags] : [];    this.assignedTo = data.assignedTo || null;  }  validate() {    const errors = [];    if (!this.title?.trim()) {      errors.push('Title is required');    } else if (this.title.trim().length &lt; 3) {      errors.push('Title must be at least 3 characters long');    }    if (this.status &amp;&amp; !Object.values(Todo.STATUS).includes(this.status)) {      errors.push(`Invalid status. Must be one of: ${Object.values(Todo.STATUS).join(', ')}`);    }    if (this.dueDate) {      if (!(this.dueDate instanceof Date) || isNaN(this.dueDate.getTime())) {        errors.push('Invalid due date format');      } else if (this.dueDate &lt; new Date()) {        errors.push('Due date cannot be in the past');      }    }    if (this.priority &amp;&amp; !Object.values(Todo.PRIORITY).includes(this.priority)) {      errors.push(`Invalid priority. Must be one of: ${Object.values(Todo.PRIORITY).join(', ')}`);    }    if (this.tags &amp;&amp; !Array.isArray(this.tags)) {      errors.push('Tags must be an array');    }    return errors;  }  isOverdue() {    return this.dueDate &amp;&amp; this.dueDate &lt; new Date() &amp;&amp; this.status !== Todo.STATUS.COMPLETED;  }  toJSON() {    return {      _id: this._id,      title: this.title,      description: this.description,      status: this.status,      priority: this.priority,      dueDate: this.dueDate,      createdAt: this.createdAt,      updatedAt: this.updatedAt,      completedAt: this.completedAt,      tags: this.tags,      assignedTo: this.assignedTo    };  }}Validation Errors Class// src/errors/index.jsclass ValidationError extends Error {  constructor(message) {    super(message);    this.name = 'ValidationError';    this.status = 400;  }}class DatabaseError extends Error {  constructor(message, originalError = null) {    super(message);    this.name = 'DatabaseError';    this.status = 500;    this.originalError = originalError;  }}class NotFoundError extends Error {  constructor(message) {    super(message);    this.name = 'NotFoundError';    this.status = 404;  }}DAOClass-Based with Constructor InjectionThis style defines a BaseDAO class that provides a clean, reusable interface with methods like findOne, insertOne, and updateOne. Each DAO (like TodoDAO) extends BaseDAO, passing in a specific collection nameclassDiagram    class DatabaseError {        +String message        +Error originalError        +constructor(message, error)    }    class BaseDAO {        -Database db        -Collection collection        +constructor(db, collectionName)        +findOne(filter) Promise~Document~        +find(filter, options) Promise~Document[]~        +insertOne(data) Promise~Result~        +updateOne(filter, update, options) Promise~Result~        +deleteOne(filter) Promise~Result~    }    class TodoDAO {        +constructor(db)        +findByStatus(status) Promise~Document[]~        +findByPriority(priority) Promise~Document[]~    }    class UserDAO {        +constructor(db)        +findByEmail(email) Promise~Document~        +findByUsername(username) Promise~Document~    }    BaseDAO &lt;|-- TodoDAO : extends    BaseDAO &lt;|-- UserDAO : extends    BaseDAO ..&gt; DatabaseError : throwsPros:  Clean and reusable: BaseDAO is a good foundation, especially if you have multiple DAOs that follow similar structures.  Constructor validation: Ensures db and collectionName are provided, helping catch issues early.  Readability and maintainability: The CRUD operations are well-encapsulated and can be reused easily across different DAOs by extending BaseDAO.Cons:  More boilerplate code  Every DAO must extend BaseDAO, which might limit flexibility for cases where a DAO might need additional initialization or a unique structure.// src/daos/base.dao.jsclass BaseDAO {  constructor(db, collectionName) {    if (!db) {      throw new Error('Database connection is required');    }    if (!collectionName) {      throw new Error('Collection name is required');    }    this.db = db;    this.collection = this.db.collection(collectionName);  }  async findOne(filter) {    try {      return await this.collection.findOne(filter);    } catch (error) {      throw new DatabaseError('Database query failed', error);    }  }  async find(filter = {}, options = {}) {    try {      return await this.collection.find(filter, options).toArray();    } catch (error) {      throw new DatabaseError('Database query failed', error);    }  }  async insertOne(data) {    try {      return await this.collection.insertOne(data);    } catch (error) {      throw new DatabaseError('Database insert failed', error);    }  }  async updateOne(filter, update, options = {}) {    try {      return await this.collection.updateOne(filter, update, options);    } catch (error) {      throw new DatabaseError('Database update failed', error);    }  }  async deleteOne(filter) {    try {      return await this.collection.deleteOne(filter);    } catch (error) {      throw new DatabaseError('Database delete failed', error);    }  }}// src/daos/todo.dao.jsclass TodoDAO extends BaseDAO {  constructor(db) {    super(db, 'todos');  }  async create(todoData) {    const todo = new Todo(todoData);    const errors = todo.validate();        if (errors.length &gt; 0) {      throw new ValidationError(errors.join(', '));    }    const todoToInsert = {      ...todo.toJSON(),      title: todo.title.trim(),      createdAt: new Date(),      updatedAt: new Date()    };    try {      const result = await this.insertOne(todoToInsert);      return new Todo({ ...todoToInsert, _id: result.insertedId });    } catch (error) {      throw new DatabaseError('Failed to create todo', error);    }  }  async findById(id) {    if (!ObjectId.isValid(id)) {      throw new ValidationError('Invalid todo ID');    }    const todo = await this.findOne({ _id: new ObjectId(id) });        if (!todo) {      throw new NotFoundError('Todo not found');    }    return new Todo(todo);  }  async find(query = {}, options = {}) {    const {       page = 1,       limit = 10,      sortBy = 'createdAt',      sortOrder = -1,      status,      priority,      searchTerm,      fromDate,      toDate,      tags    } = options;    if (page &lt; 1 || limit &lt; 1) {      throw new ValidationError('Invalid pagination parameters');    }    const filter = this._buildFilter({      ...query,      status,      priority,      searchTerm,      fromDate,      toDate,      tags    });    const skip = (page - 1) * limit;    const sortOptions = { [sortBy]: sortOrder };    try {      const [todos, totalCount] = await Promise.all([        this.collection          .find(filter)          .sort(sortOptions)          .skip(skip)          .limit(limit)          .toArray(),        this.collection.countDocuments(filter)      ]);      return {        todos: todos.map(todo =&gt; new Todo(todo)),        pagination: {          total: totalCount,          page,          limit,          pages: Math.ceil(totalCount / limit)        }      };    } catch (error) {      throw new DatabaseError('Failed to fetch todos', error);    }  }  async update(id, updateData) {    const existingTodo = await this.findById(id);        const updatedTodo = new Todo({      ...existingTodo,      ...updateData,      _id: existingTodo._id,      updatedAt: new Date()    });    const errors = updatedTodo.validate();    if (errors.length &gt; 0) {      throw new ValidationError(errors.join(', '));    }    const result = await this.updateOne(      { _id: new ObjectId(id) },      { $set: updatedTodo.toJSON() },      { returnDocument: 'after' }    );    if (result.matchedCount === 0) {      throw new NotFoundError('Todo not found');    }    return updatedTodo;  }  async delete(id) {    if (!ObjectId.isValid(id)) {      throw new ValidationError('Invalid todo ID');    }    const result = await this.deleteOne({ _id: new ObjectId(id) });    if (result.deletedCount === 0) {      throw new NotFoundError('Todo not found');    }    return true;  }  _buildFilter(options) {    const filter = {};    if (options.status) {      if (!Object.values(Todo.STATUS).includes(options.status)) {        throw new ValidationError('Invalid status filter');      }      filter.status = options.status;    }    if (options.priority) {      if (!Object.values(Todo.PRIORITY).includes(options.priority)) {        throw new ValidationError('Invalid priority filter');      }      filter.priority = options.priority;    }    if (options.searchTerm) {      filter.$or = [        { title: { $regex: options.searchTerm, $options: 'i' } },        { description: { $regex: options.searchTerm, $options: 'i' } }      ];    }    if (options.fromDate || options.toDate) {      filter.dueDate = {};      if (options.fromDate) {        filter.dueDate.$gte = new Date(options.fromDate);      }      if (options.toDate) {        filter.dueDate.$lte = new Date(options.toDate);      }    }    if (options.tags &amp;&amp; Array.isArray(options.tags)) {      filter.tags = { $all: options.tags };    }    return filter;  }}Static Method with Injected ConnectionSimplified Usage: Static methods remove the need to instantiate the class, reducing memory overhead.Testing Challenges: Static methods are less flexible with dependency injection, making mocks and tests trickier.Limited Reusability: Static methods don‚Äôt support inheritance, so shared logic across DAOs is harder to centralize.import { ObjectId } from 'mongodb';let todos;export default class TodoDAO {  static async injectDB(conn) {    if (todos) return;    try {      todos = await conn.db(process.env.DB_NAME).collection('todos');    } catch (error) {      console.error(`Unable to establish collection handles in TodoDAO: ${error}`);    }  }  static async create(todoData) {    // Initialize and validate the Todo, build the data object, and insert into the collection  }  static async findById(id) {    // Validate ID, query by `_id`, throw `NotFoundError` if not found  }  static async find(query = {}, options = {}) {    // Construct filter, pagination, and sort options, execute the find query with pagination  }  static async update(id, updateData) {    // Fetch the todo, validate and update with `updateOne`  }  static async delete(id) {    // Validate ID, delete by `_id`, and handle `NotFoundError`  }  static _buildFilter(options) {    // Generate the filter object for queries based on status, priority, search terms, etc.  }}Service// src/services/todo.service.jsclass TodoService {  constructor(todoDAO) {    if (!todoDAO) {      throw new Error('TodoDAO is required');    }    this.todoDAO = todoDAO;  }  async createTodo(todoData) {    return await this.todoDAO.create(todoData);  }  async getTodoById(id) {    return await this.todoDAO.findById(id);  }  async updateTodo(id, updateData) {    return await this.todoDAO.update(id, updateData);  }  async deleteTodo(id) {    return await this.todoDAO.delete(id);  }  async updateTodoStatus(id, status) {    const todo = await this.todoDAO.findById(id);        const updates = {       status,      updatedAt: new Date()    };        if (status === Todo.STATUS.COMPLETED &amp;&amp; todo.status !== Todo.STATUS.COMPLETED) {      updates.completedAt = new Date();    } else if (status !== Todo.STATUS.COMPLETED &amp;&amp; todo.status === Todo.STATUS.COMPLETED) {      updates.completedAt = null;    }    return await this.todoDAO.update(id, updates);  }  async findTodos(options = {}) {    return await this.todoDAO.find({}, options);  }  async findOverdueTodos() {    const now = new Date();    return await this.todoDAO.find({      dueDate: { $lt: now },      status: { $ne: Todo.STATUS.COMPLETED }    });  }  async findTodosByPriority(priority) {    return await this.todoDAO.find({ priority });  }  async assignTodo(id, userId) {    return await this.todoDAO.update(id, {      assignedTo: userId,      updatedAt: new Date()    });  }  async addTags(id, tags) {    const todo = await this.todoDAO.findById(id);    const uniqueTags = [...new Set([...todo.tags, ...tags])];    return await this.todoDAO.update(id, { tags: uniqueTags });  }  async removeTags(id, tags) {    const todo = await this.todoDAO.findById(id);    const updatedTags = todo.tags.filter(tag =&gt; !tags.includes(tag));    return await this.todoDAO.update(id, { tags: updatedTags });  }}IndexingIndexing in MongoDB improves query performance by creating efficient data structures for faster data retrieval.graph LR  subgraph indexing[\"fa:fa-search Indexing\"]    singleField[\"fa:fa-file $singleField\"]    compound[\"fa:fa-layer-group $compound\"]    multikey[\"fa:fa-list $multikey\"]    text[\"fa:fa-font $text\"]  end  style indexing stroke:#333,stroke-width:2px  style singleField fill:#ccf,stroke:#f66,stroke-width:2px  style compound fill:#add8e6,stroke:#333,stroke-width:2px  style multikey fill:#9cf,stroke:#333,stroke-width:2px  style text fill:#faa,stroke:#333,stroke-width:2px  Single Field Index: Index on a single field of a document.  Compound Index: Index on multiple fields.  Multikey Index: Index on array fields.  Text Index: Supports text search queries on string content.Single Field IndexTo create an index, you use the createIndex() method.db.collection('users').createIndex({ name: 1 });Compound IndexA compound index in MongoDB is an index on multiple fields in a document.graph LR  subgraph compoundIndex[\"fa:fa-layer-group Compound Index\"]    field1[\"fa:fa-file Field 1\"]    field2[\"fa:fa-file Field 2\"]    field3[\"fa:fa-file Field 3\"]  end  style compoundIndex stroke:#333,stroke-width:2px  style field1 fill:#ccf,stroke:#f66,stroke-width:2px  style field2 fill:#add8e6,stroke:#333,stroke-width:2px  style field3 fill:#9cf,stroke:#333,stroke-width:2pxdb.collection('users').createIndex({ name: 1, age: 1, city: 1, country: 1, hobbies: 1 });Prefixes of a compound index can be used to satisfy queries that match the index fields from left to right.db.collection('users').find({ name: 'Alice', age: 25 });db.collection('users').find({ name: 'Alice', age: 25, city: 'New York' });Multikey IndexA multikey index in MongoDB is an index on an array field.graph LR  subgraph multikeyIndex[\"fa:fa-list Multikey Index\"]    arrayField[\"fa:fa-list Array Field\"]  end  style multikeyIndex stroke:#333,stroke-width:2px  style arrayField fill:#ccf,stroke:#f66,stroke-width:2pxdb.collection('users').createIndex({ hobbies: 1 });Limitations of Multikey Indexes  you cannot create a compound index if more than one field is an array.db.collection('users').createIndex({ \"hobbies\": 1, \"tags\": 1 }); // Not allowed if both hobbies and tags are arraysText IndexText indexes in MongoDB provide a way to perform text search queries on string content. Each collection can have at most one text index.db.products.createIndex({   name: \"text\",   description: \"text\",   tags: \"text\" });// Sample Datadb.products.insertMany([  {    name: \"MacBook Pro 16-inch\",    description: \"Powerful laptop for developers\",    tags: [\"apple\", \"laptop\", \"computer\"]  },  {    name: \"iPhone 15 Pro\",    description: \"Latest smartphone with great camera\",    tags: [\"apple\", \"smartphone\", \"mobile\"]  },  {    name: \"Gaming Laptop ROG\",    description: \"High-performance gaming laptop\",    tags: [\"asus\", \"laptop\", \"gaming\"]  }]);Exact word matchesdb.products.find({ $text: { $search: \"laptop\" }}); // Will find MacBook and ROGMultiple words (OR)// By default, multiple words are treated as logical OR// Will find documents containing \"laptop\" or \"gaming\"db.products.find({ $text: { $search: \"laptop gaming\" }});Multiple Words (Logical AND)// Using the plus sign to ensure both terms are present// Will find documents containing both \"laptop\" and \"gaming\"db.products.find({ $text: { $search: \"+laptop +gaming\" }});Exact phrases// Using double quotes to search for an exact phrase// Will find documents containing the exact phrase \"gaming laptop\"db.products.find({ $text: { $search: \"\\\"gaming laptop\\\"\" }});Word exclusionsdb.products.find({ $text: { $search: ‚Äúlaptop -gaming‚Äù }}); // Laptops but not gaming onesCase-insensitive matches// Text search is case-insensitive by default// Will find all documents containing \"laptop\" regardless of casedb.products.find({ $text: { $search: \"LAPTOP\" }});Diacritic-insensitive// Text search ignores diacritic marks by default// Will match \"cafe\" and \"caf√©\"db.products.find({ $text: { $search: \"caf√©\" }});Partial Word Matches// Searching for \"lap\" won't find \"laptop\"db.products.find({ $text: { $search: \"lap\" }});Wildcard Searches// Wildcards are not supported in text searchdb.products.find({ $text: { $search: \"lap*\" }});Fuzzy Matching (Typos)// Regular expressions cannot be used inside $text searchdb.products.find({ $text: { $search: \"/lap.*/\" }});Complex Boolean Expressions// Nested boolean logic is not supported in $text searchdb.products.find({ $text: { $search: \"(laptop AND gaming) OR (smartphone AND camera)\" }});Case-Sensitive Searches// Cannot perform case-sensitive searches using $textdb.products.find({ $text: { $search: \"MacBook\", caseSensitive: true }});Atlas SearchAtlas Search is a fully managed search service that allows you to build fast, relevant, full-text search capabilities on top of your MongoDB data. Compare to text indexes, Atlas Search provides more advanced features like faceted search, fuzzy matching, and relevance scoring.//  Basic Atlas Search Setupdb.products.createSearchIndex({  \"mappings\": {    \"dynamic\": true,    \"fields\": {      \"name\": {        \"type\": \"string\",        \"analyzer\": \"lucene.standard\"      },      \"description\": {        \"type\": \"string\",        \"analyzer\": \"lucene.english\"      }    }  }});Fuzzy Matching - Handles typos and misspellingsdb.products.aggregate([  {    $search: {      text: {        query: \"labtop\",  // Will match \"laptop\"        path: \"name\",        fuzzy: { maxEdits: 1 }      }    }  }]);Autocomplete - Real-time suggestionsdb.products.aggregate([  {    $search: {      autocomplete: {        query: \"mac\",     // Will suggest \"macbook\", \"machine\", etc.        path: \"name\"      }    }  }]);Custom Scoring - Boost relevanceCan boost the relevance of certain documents based on specific criteria.db.products.aggregate([  {    $search: {      compound: {        must: [          { text: { query: \"laptop\", path: \"name\" } }        ],        should: [          {            text: {              query: \"gaming\",              path: \"category\",              score: { boost: { value: 2.0 } }  // Boost gaming laptops            }          }        ]      }    }  }]);  Documents must contain ‚Äúlaptop‚Äù in the name field  Documents containing ‚Äúgaming‚Äù in the description field are boosted.Multi-language Supportdb.products.aggregate([  {    $search: {      text: {        query: \"ordinateur portable\",  // French for \"laptop\"        path: \"description\",        analyzer: \"lucene.french\"      }    }  }]);Faceted Search - Filter resultsFaceted search allows users to filter search results based on categories, price ranges, and other attributes.db.products.aggregate([  {    $searchMeta: {      facet: {        operator: { text: { query: \"laptop\", path: \"description\" } },        facets: {          categories: { type: \"string\", path: \"category\" },          priceRanges: {            type: \"number\",            path: \"price\",            boundaries: [500, 1000, 2000]          }        }      }    }  }]);**Facets:**- categoryFacet: Groups results by the category field.- priceFacet: Groups results into price ranges defined by the boundaries.Trade-offsBesides the benefits, Atlas Search also has some trade-offs compared to traditional text indexes.  Requires M10+ clusters  Additional cost  More complex setup  Higher resource usageArray OperationsMongoDB supports a variety of array operations for working with arrays in documents.graph   subgraph      each[\"fa:fa-list $each\"]    in[\"fa:fa-rectangle-list $in\"]    gt[\"fa:fa-angle-double-right $gt\"]    ne[\"fa:fa-angle-double-left $ne\"]  end  subgraph         pull[\"fa:fa-trash-alt $pull\"]    pop[\"fa:fa-trash-alt $pop\"]    pullAll[\"fa:fa-trash-alt $pullAll\"]  end    subgraph arrayOperations[\"fa:fa-list Array Operations\"]    elementMatch[\"fa:fa-equals $elementMatch\"]    push[\"fa:fa-plus $push\"]    set[\"fa:fa-hammer $set\"]    addToSet[\"fa:fa-plus-square $addToSet\"]  end  style elementMatch fill:#add8e6, stroke:#333,stroke-width:2px  style arrayOperations stroke:#333,stroke-width:2px  style push fill:#ccf,stroke:#f66,stroke-width:2px  style addToSet fill:#add8e6,stroke:#333,stroke-width:2px  style each fill:#9cf,stroke:#333,stroke-width:2px  style pull fill:#9cf,stroke:#333,stroke-width:2px  style pop fill:#faa,stroke:#333,stroke-width:2px  style pullAll fill:#ccf,stroke:#f66,stroke-width:2px  style in fill:#add8e6,stroke:#333,stroke-width:2px  style gt fill:#9cf,stroke:#333,stroke-width:2px  style ne fill:#faa,stroke:#333,stroke-width:2pxAdding Elements to an ArrayTo add elements to an array in a document, you use the $push operator.db.collection('users').updateOne({ name: 'Alice' }, { $push: { hobbies: 'Reading' } });Querying Arrays with $elemMatchGiven a collection of students with scores in different subjects:{  \"_id\": ObjectId(\"64d39a7a8b0e8c284a2c1234\"),  \"name\": \"Alice\",  \"scores\": [    { \"subject\": \"Math\", \"score\": 95 },    { \"subject\": \"English\", \"score\": 88 }  ]},{  \"_id\": ObjectId(\"64d39a808b0e8c284a2c1235\"),  \"name\": \"Bob\",  \"scores\": [    { \"subject\": \"Math\", \"score\": 78 },    { \"subject\": \"English\", \"score\": 92 }  ]}Querying with $elemMatch:To find students who specifically scored above 90 in Math, we need $elemMatch:db.students.find({   scores: {     $elemMatch: { subject: \"Math\", score: { $gt: 90 } }   } })Result: This will return only Alice, as she is the only student with a score above 90 in the ‚ÄúMath‚Äù subject. $elemMatch ensures that all the conditions within the array element must be met.Without $elemMatch - This will return both Alice and Bob, as they both have scores above 90 in different subjects and not necessarily in the ‚ÄúMath‚Äù subject.graph   subgraph query[\"Query\"]    elemMatchMath[\"scores { $elemMatch: { subject: 'Math', score: { $gt: 90 } } }\"]  end  subgraph documents[\"Documents\"]    alice[\"Alice: { scores: [{ subject: 'Math', score: 95 }, { subject: 'English', score: 88 }] }\"]    bob[\"Bob: { scores: [{ subject: 'Math', score: 78 }, { subject: 'English', score: 92 }] }\"]  end  elemMatchMath --&gt;  alice --&gt; found[\"‚úÖ\"]  elemMatchMath --&gt; bob   bob --&gt; |With $elemMatch|notFound[\"‚ùå\"]   bob --&gt; |Without $elemMatch|found2[\"‚úÖ\"]  style found fill:#90EE90  style notFound fill:#FFCCCCAdd Unique - $addToSetThe $addToSet operator in MongoDB is used to add elements to an array only if they are not already present. This prevents duplicate entries in the array.db.collection('users').updateOne({ name: 'Alice' }, { $addToSet: { hobbies: 'Reading' } });Add Multiple  $push and $eachThe $push operator in MongoDB is used to add elements to an array. The $each modifier allows you to add multiple elements to the array.db.collection('users').updateOne({ name: 'Alice' }, { $push: { hobbies: { $each: ['Reading', 'Swimming'] } } });Add Sorted -  $push and $sortThe $push operator in MongoDB is used to add elements to an array. The $sort modifier allows you to sort the array elements.db.collection('users').updateOne({ name: 'Alice' }, { $push: { scores: { $each: [85, 90], $sort: -1 } } });Push and Sort Array of Objects by a specific fielddb.collection('users').updateOne(  { name: 'Alice' },  {    $push: {      scores: {        $each: [          { score: 85, date: \"2023-03-01\" },          { score: 90, date: \"2023-04-01\" }        ],        $sort: { date: -1 }      }    }  });Add Limited -  $push and $sliceThe $push operator in MongoDB is used to add elements to an array. The $slice modifier allows you to limit the number of elements in the array.db.collection('users').updateOne({ name: 'Alice' }, { $push: { scores: { $each: [85, 90], $slice: -3 } } });If Alice currently has a scores array like [70, 75, 80], this query will push 85 and 90, making it [70, 75, 80, 85, 90]. The $slice: -3 will then trim it to the last 3 elements, resulting in [80, 85, 90].Removing Elements from an ArrayTo remove elements from an array in a document, you use the $pull operator.db.collection('users').updateOne({ name: 'Alice' }, { $pull: { hobbies: 'Reading' } });Remove the first or last element - $popThe $pop operator in MongoDB is used to remove the first or last element from an array.db.collection('users').updateOne({ name: 'Alice' }, { $pop: { hobbies: 1 } });Remove Multiple -  $pullAllThe $pullAll operator in MongoDB is used to remove all occurrences of specified values from an array.db.collection('users').updateOne({ name: 'Alice' }, { $pullAll: { hobbies: ['Reading', 'Swimming'] } });Remove Multiple - $pull and $inThe $pull operator in MongoDB is used to remove elements from an array. The $in modifier allows you to specify multiple values to remove.db.collection('users').updateOne({ name: 'Alice' }, { $pull: { hobbies: { $in: ['Reading', 'Swimming'] } } });Remove Condition - $pull and $gtThe $pull operator in MongoDB is used to remove elements from an array. The $gt modifier allows you to specify a condition for removing elements.db.collection('users').updateOne({ name: 'Alice' }, { $pull: { scores: { $gt: 85 } } });Remove Not Equal - $pull and $neThe $pull operator in MongoDB is used to remove elements from an array. The $ne modifier allows you to specify a condition for removing elements that are not equal to a value.db.collection('users').updateOne({ name: 'Alice' }, { $pull: { scores: { $ne: 85 } } });Update Condition - $set and $The $set operator in MongoDB is used to update fields in a document. The $ positional operator allows you to update the first element that matches a condition in an array.db.collection('users').updateOne({ name: 'Alice', 'scores.subject': 'Math' }, { $set: { 'scores.$.score': 90 } });Update All - $[]The $[] operator in MongoDB is used to update all elements in an array that match a condition.db.collection('users').updateOne({ name: 'Alice' }, { $set: { 'scores.$[].score': 90 } });To increment a field in all elements of an array, you can use the $[] operator with the $inc operator.db.collection('users').updateOne({ name: 'Alice' }, { $inc: { 'scores.$[].score': 5 } });Update Filtered - $[&lt;identifier&gt;]The $[&lt;identifier&gt;] operator in MongoDB is used to update elements in an array that match a condition.db.collection('users').updateOne({ name: 'Alice' }, { $set: { 'scores.$[elem].score': 90 } }, { arrayFilters: [{ 'elem.subject': 'Math' }] });AggregationAggregation operations process data records and return computed results. Aggregation allows you to perform complex data processing and transformation.Aggregation PipelineThe aggregation framework in MongoDB uses a pipeline approach, where multiple stages transform the documents.db.collection('orders').aggregate([  { $match: { status: 'A' } },  { $group: { _id: '$cust_id', total: { $sum: '$amount' } } },  { $sort: { total: -1 } }]);graph  subgraph aggregation[\"fa:fa-chart-line Aggregation\"]    stage1[\"fa:fa-filter Match\"] --&gt; stage2[\"fa:fa-layer-group Group\"]    stage2 --&gt; stage3[\"fa:fa-sort Sort\"]  end  style aggregation stroke:#333,stroke-width:2px  style stage1 fill:#ccf,stroke:#f66,stroke-width:2px  style stage2 fill:#add8e6,stroke:#333,stroke-width:2px  style stage3 fill:#9cf,stroke:#333,stroke-width:2pxJoin CollectionsMongoDB does not support joins like relational databases. Instead, you can use the $lookup operator to perform a left outer join between two collections.db.collection('orders').aggregate([  {    $lookup: {      from: 'customers',      localField: 'cust_id',      foreignField: '_id',      as: 'customer'    }  }]);graph  subgraph joinCollections[\"fa:fa-link Join Collections\"]    orders[\"fa:fa-folder-open Orders\"]    customers[\"fa:fa-folder-open Customers\"]  end  orders --&gt; |$lookup| customers  style joinCollections stroke:#333,stroke-width:2px  style orders fill:#ccf,stroke:#f66,stroke-width:2px  style customers fill:#add8e6,stroke:#333,stroke-width:2pxUnwind ArraysThe $unwind operator in MongoDB is used to deconstruct an array field into multiple documents.db.collection('orders').aggregate([  { $unwind: '$items' }]);Unwind and Groupdb.collection('orders').aggregate([  { $unwind: '$items' },  {    $group: {      _id: '$items.productId',      totalQuantity: { $sum: '$items.quantity' },      totalRevenue: { $sum: { $multiply: ['$items.price', '$items.quantity'] } },      ordersCount: { $sum: 1 }    }  }]);Unwind Multiple Arraysdb.collection('restaurants').aggregate([  { $unwind: '$categories' },  { $unwind: '$reviews' },  {    $group: {      _id: '$categories',      averageRating: { $avg: '$reviews.rating' },      reviewCount: { $sum: 1 }    }  }]);Group DocumentsThe $group operator in MongoDB is used to group documents by a specified key.db.collection('orders').aggregate([  { $group: { _id: '$cust_id', total: { $sum: '$amount' } } }]);Group and Countdb.collection('orders').aggregate([  { $group: { _id: '$status', count: { $sum: 1 } } }]);Group and Sumdb.collection('orders').aggregate([  { $group: { _id: '$status', total: { $sum: '$amount' } } }]);Group and Averagedb.collection('orders').aggregate([  { $group: { _id: '$status', average: { $avg: '$amount' } } }]);Group and Pushdb.collection('orders').aggregate([  { $group: { _id: '$cust_id', items: { $push: '$item' } } }]);Group with Multiple Fieldsdb.collection('orders').aggregate([  {    $group: {      _id: {         status: '$status',        category: '$category'      },      count: { $sum: 1 },      totalAmount: { $sum: '$amount' }    }  }]);Group with Date Operationsdb.collection('orders').aggregate([  {    $group: {      _id: {        year: { $year: '$orderDate' },        month: { $month: '$orderDate' }      },      totalOrders: { $sum: 1 },      revenue: { $sum: '$amount' }    }  }]);Project FieldsThe $project operator in MongoDB is used to include, exclude, or rename fields in the output documents.db.collection('orders').aggregate([  { $project: { _id: 0, cust_id: 1, amount: 1 } }]);Project with Computed Fieldsdb.collection('orders').aggregate([  {    $project: {      _id: 0,      cust_id: 1,      amount: 1,      discount: { $subtract: ['$total', '$amount'] }    }  }]);Project With Array Operationsdb.collection('orders').aggregate([  {    $project: {      orderId: 1,      itemCount: { $size: '$items' },      firstItem: { $arrayElemAt: ['$items', 0] },      lastItem: { $arrayElemAt: ['$items', -1] },      items: {        $map: {          input: '$items',          as: 'item',          in: {            name: '$$item.name',            subtotal: {              $multiply: ['$$item.price', '$$item.quantity']            }          }        }      }    }  }]);Project With String Operationsdb.collection('users').aggregate([  {    $project: {      fullName: { $concat: ['$firstName', ' ', '$lastName'] },      email: { $toLower: '$email' },      age: { $toString: '$age' }    }  }]);Project With Conditional Fieldsdb.collection('users').aggregate([  {    $project: {      name: 1,      status: {        $cond: {          if: { $gte: ['$age', 18] },          then: 'Adult',          else: 'Minor'        }      }    }  }]);Run Multiple AggregationsYou can run multiple aggregation pipelines in a single query using the $facet operator.db.collection('orders').aggregate([  {    $facet: {      totalAmount: [        { $group: { _id: null, total: { $sum: '$amount' } } }      ],      averageAmount: [        { $group: { _id: null, average: { $avg: '$amount' } } }      ]    }  }]);TransactionsTransactions in MongoDB allow you to perform multiple operations as a single, all-or-nothing unit of work. They ensure data integrity and consistency across multiple documents and collections.graph LR  start[\"fa:fa-play Start\"]  subgraph transaction[\"fa:fa-exchange-alt Transaction\"]    op1[\"fa:fa-cog Operation 1\"]    op2[\"fa:fa-cog Operation 2\"]    op3[\"fa:fa-cog Operation 3\"]  end  start --&gt; transaction  transaction --&gt; commit[\"fa:fa-check Commit\"]  transaction --&gt; abort[\"fa:fa-times Abort\"]  style transaction stroke:#333,stroke-width:2px  style start fill:#ccf,stroke:#f66,stroke-width:2px  style op1 fill:#add8e6,stroke:#333,stroke-width:2px  style op2 fill:#add8e6,stroke:#333,stroke-width:2px  style op3 fill:#add8e6,stroke:#333,stroke-width:2px  style commit fill:#9cf,stroke:#333,stroke-width:2px  style abort fill:#fcc,stroke:#333,stroke-width:2pxTransaction Properties (ACID)graph LR    subgraph acidProperties[\"üß™ ACID Properties\"]        atomicity[\"üí• Atomicity\"]        consistency[\"üîÑ Consistency\"]        isolation[\"üîí Isolation\"]        durability[\"üíæ Durability\"]    end    atomicity --&gt; |\"All or Nothing\"| atomicityDesc[\"Operations complete successfully or have no effect\"]    consistency --&gt; |\"Before and After\"| consistencyDesc[\"Database remains in a consistent state\"]    isolation --&gt; |\"Concurrent Transactions\"| isolationDesc[\"Do not interfere with each other\"]    durability --&gt; |\"Committed Changes\"| durabilityDesc[\"Persist despite system failures\"]    style acidProperties fill:#f0f0ff,stroke:#333,stroke-width:2px    style atomicity fill:#e6f2ff,stroke:#333,stroke-width:2px    style consistency fill:#e6ffe6,stroke:#333,stroke-width:2px    style isolation fill:#fff0f0,stroke:#333,stroke-width:2px    style durability fill:#fff0ff,stroke:#333,stroke-width:2px    style atomicityDesc fill:#f0f8ff,stroke:#333,stroke-width:1px    style consistencyDesc fill:#f0fff0,stroke:#333,stroke-width:1px    style isolationDesc fill:#fff5f5,stroke:#333,stroke-width:1px    style durabilityDesc fill:#fdf0ff,stroke:#333,stroke-width:1pxUsing TransactionsTo use transactions in MongoDB, you typically follow these steps:  Start a session  Start a transaction  Perform operations  Commit or abort the transaction// Define a clientconst { MongoClient } = require('mongodb');const client = new MongoClient('mongodb://localhost:27017');//...// Start a sessionconst session = client.startSession();try {  session.startTransaction();  // Perform multiple operations  await collection1.updateOne({ _id: 1 }, { $set: { status: 'processing' } }, { session });  await collection2.insertOne({ orderId: 1, items: ['item1', 'item2'] }, { session });  // Commit the transaction  await session.commitTransaction();} catch (error) {  // If an error occurred, abort the transaction  await session.abortTransaction();  console.error('Transaction aborted:', error);} finally {  // End the session  session.endSession();}Considerations for Transactions  Performance: Transactions may impact performance, especially for write-heavy workloads.  Timeout: Transactions have a default timeout of 60 seconds.  Replica Sets: Transactions require a replica set configuration.  Sharded Clusters: Transactions on sharded clusters have additional considerations and limitations.graph TD  subgraph transactionConsiderations[\"fa:fa-exclamation-triangle Transaction Considerations\"]    performance[\"fa:fa-tachometer-alt Performance Impact\"]    timeout[\"fa:fa-clock Timeout\"]    replicaSet[\"fa:fa-server Replica Set Required\"]    sharding[\"fa:fa-cubes Sharding Limitations\"]  end  style transactionConsiderations stroke:#333,stroke-width:2px  style performance fill:#fcc,stroke:#333,stroke-width:2px  style timeout fill:#ffc,stroke:#333,stroke-width:2px  style replicaSet fill:#cfc,stroke:#333,stroke-width:2px  style sharding fill:#ccf,stroke:#333,stroke-width:2pxBy using transactions, you can ensure data consistency and integrity across multiple operations in MongoDB, especially when dealing with complex data models or critical business logic.Replica SetsA replica set is a group of MongoDB instances that maintain the same data set. Replica sets provide redundancy and high availability.Components of a Replica Set  Primary: Receives all write operations.  Secondary: Replicates data from the primary. Can be used for read operations.  Arbiter: Participates in elections for primary but does not hold data.graph  subgraph replicaSet[\"fa:fa-server Replica Set\"]    primary[\"fa:fa-database Primary\"]    secondary1[\"fa:fa-database Secondary\"]    secondary2[\"fa:fa-database Secondary\"]    arbiter[\"fa:fa-balance-scale Arbiter\"]  end  primary --&gt; secondary1  primary --&gt; secondary2  primary --&gt; arbiter  style replicaSet stroke:#333,stroke-width:2px  style primary fill:#ccf,stroke:#f66,stroke-width:2px  style secondary1 fill:#add8e6,stroke:#333,stroke-width:2px  style secondary2 fill:#add8e6,stroke:#333,stroke-width:2px  style arbiter fill:#9cf,stroke:#333,stroke-width:2pxReplica Set ConfigurationTo configure a replica set, you use the rs.initiate() method.rs.initiate({  _id: 'rs0',  members: [    { _id: 0, host: 'mongo1:27017' },    { _id: 1, host: 'mongo2:27017' },    { _id: 2, host: 'mongo3:27017', arbiterOnly: true }  ]});Read PreferenceRead preference in MongoDB determines how read operations are distributed across the replica set.graph LR  subgraph readPreference[\"fa:fa-eye Read Preference\"]    primary[\"fa:fa-database Primary\"]    secondary[\"fa:fa-database Secondary\"]    primaryPreferred[\"fa:fa-database PrimaryPreferred\"]    secondaryPreferred[\"fa:fa-database SecondaryPreferred\"]    nearest[\"fa:fa-database Nearest\"]  end  style readPreference stroke:#333,stroke-width:2px  style primary fill:#ccf,stroke:#f66,stroke-width:2px  style secondary fill:#add8e6,stroke:#333,stroke-width:2px  style primaryPreferred fill:#9cf,stroke:#333,stroke-width:2px  style secondaryPreferred fill:#9cf,stroke:#333,stroke-width:2px  style nearest fill:#9cf,stroke:#333,stroke-width:2px  Primary: Reads from the primary.  Secondary: Reads from the secondary.  PrimaryPreferred: Reads from the primary if available, otherwise from the secondary.  SecondaryPreferred: Reads from the secondary if available, otherwise from the primary.  Nearest: Reads from the nearest member of the replica set.db.collection('users').find().readPref('secondary');Write ConcernWrite concern in MongoDB determines the level of acknowledgment for write operations.graph LR  subgraph writeConcern[\"fa:fa-sync Write Concern\"]    w0[\"fa:fa-times w: 0\"]    w1[\"fa:fa-check w: 1\"]    wMajority[\"fa:fa-check w: majority\"]  end  style writeConcern stroke:#333,stroke-width:2px  style w0 fill:#ccf,stroke:#f66,stroke-width:2px  style w1 fill:#add8e6,stroke:#333,stroke-width:2px  style wMajority fill:#9cf,stroke:#333,stroke-width:2px  w: 0: No acknowledgment.  w: 1: Acknowledgment from the primary.  w: majority: Acknowledgment from the majority of the replica set.db.collection('users').insertOne({ name: 'Alice' }, { writeConcern: { w: 'majorAutomatic FailoverMongoDB uses a heartbeat mechanism to detect the availability of replica set members. If the primary becomes unavailable, a new primary is elected.  Primary is elected based on the number of votes from the replica set members.  Secondary can be promoted to primary if the primary is unavailable.  Arbiter is used to break the tie in elections.graph  subgraph failover[\"fa:fa-sync Failover\"]    primary[\"fa:fa-database Primary\"]    secondary1[\"fa:fa-database Secondary\"]    secondary2[\"fa:fa-database Secondary\"]    arbiter[\"fa:fa-balance-scale Arbiter\"]  end  primary --&gt; secondary1  primary --&gt; secondary2  primary --&gt; arbiter  style failover stroke:#333,stroke-width:2px  style primary fill:#ccf,stroke:#f66,stroke-width:2px  style secondary1 fill:#add8e6,stroke:#333,stroke-width:2px  style secondary2 fill:#add8e6,stroke:#333,stroke-width:2px  style arbiter fill:#9cf,stroke:#333,stroke-width:2pxManual FailoverYou can initiate a manual failover in MongoDB by forcing a replica set member to become the primary.rs.stepDown();ShardingSharding is a method for distributing data across multiple machines. It allows you to scale horizontally by adding more machines to your system.A Collection is divided into chunks, and each chunk is stored on a different shard.Each Shard is a subset of the data in a sharded cluster.graph  subgraph sharding[\"fa:fa-database Sharding\"]    collection[\"fa:fa-folder-open Collection\"]    chunk1[\"fa:fa-cube Chunk 1\"]    chunk2[\"fa:fa-cube Chunk 2\"]    chunk3[\"fa:fa-cube Chunk 3\"]  end  collection --&gt; chunk1  collection --&gt; chunk2  collection --&gt; chunk3  style sharding stroke:#333,stroke-width:2px  style collection fill:#ccf,stroke:#f66,stroke-width:2px  style chunk1 fill:#add8e6,stroke:#333,stroke-width:2px  style chunk2 fill:#9cf,stroke:#333,stroke-width:2px  style chunk3 fill:#faa,stroke:#333,stroke-width:2pxComponents of Sharding  Shard: A subset of the data in a sharded cluster.  Config Server: Stores metadata and configuration settings for the cluster.  Query Router: Routes queries to the appropriate shard.graph  subgraph sharding[\"fa:fa-database Sharding\"]    shard1[\"fa:fa-database Shard 1\"]    shard2[\"fa:fa-database Shard 2\"]    shard3[\"fa:fa-database Shard 3\"]    configServer[\"fa:fa-cogs Config Server\"]    queryRouter[\"fa:fa-route Query Router\"]  end  shard1 --&gt; configServer  shard2 --&gt; configServer  shard3 --&gt; configServer  queryRouter --&gt; shard1  queryRouter --&gt; shard2  queryRouter --&gt; shard3  style sharding stroke:#333,stroke-width:2px  style shard1 fill:#ccf,stroke:#f66,stroke-width:2px  style shard2 fill:#add8e6,stroke:#333,stroke-width:2px  style shard3 fill:#9cf,stroke:#333,stroke-width:2px  style configServer fill:#ccf,stroke:#f66,stroke-width:2px  style queryRouter fill:#add8e6,stroke:#333,stroke-width:2pxSharding KeyThe sharding key is the field used to distribute data across the shards. It should be chosen carefully to ensure a balanced distribution of data.db.collection.createIndex({ _id: 'hashed' });When selecting a shard key, consider the following factors:  Cardinality: The number of unique values in the shard key.  Write Scaling: The ability to distribute write operations across shards.  Query Isolation: The ability to target specific shards for read operations.Shard Key Strategies  Hashed Sharding: Distributes data evenly across the shards using a hash function.  Range Sharding: Distributes data based on a range of values in the shard key.  Compound Sharding: Distributes data based on multiple fields in the shard key.db.collection.createIndex({ _id: 'hashed' });db.collection.createIndex({ date: 1 });db.collection.createIndex({ country: 1, city: 1 });MongooseMongoose is an Object Data Modeling (ODM) library for MongoDB and Node.js. It provides a schema-based solution to model your application data.graph   subgraph mongoose[\"fa:fa-database Mongoose\"]    connect[\"fa:fa-plug Connect\"]    schema[\"fa:fa-file Schema\"]    model[\"fa:fa-cube Model\"]    insert[\"fa:fa-plus-circle Insert\"]    find[\"fa:fa-eye Find\"]    update[\"fa:fa-edit Update\"]    delete[\"fa:fa-trash Delete\"]  end  connect --&gt; schema  schema --&gt; model  model --&gt; insert  model --&gt; find  model --&gt; update  model --&gt; delete  style mongoose stroke:#333,stroke-width:2px  style connect fill:#ccf,stroke:#f66,stroke-width:2px  style schema fill:#add8e6,stroke:#333,stroke-width:2px  style model fill:#9cf,stroke:#333,stroke-width:2px  style insert fill:#ccf,stroke:#f66,stroke-width:2px  style find fill:#add8e6,stroke:#333,stroke-width:2px  style update fill:#9cf,stroke:#333,stroke-width:2px  style delete fill:#faa,stroke:#333,stroke-width:2pxConnecting to MongoDBTo connect to MongoDB using Mongoose, you use the connect() method.const mongoose = require('mongoose');mongoose.connect('mongodb://localhost:27017/myapp', { useNewUrlParser: true, useUnifiedTopology: true });Defining a SchemaA Mongoose schema defines the structure of the documents in a collection.const userSchema = new mongoose.Schema({  name: String,  age: Number});Creating a ModelA Mongoose model is a class that represents a collection in MongoDB.const User = mongoose.model('User', userSchema);Inserting DocumentsTo insert a document into a collection, you create an instance of the model and call the save() method.const user = new User({ name: 'Alice', age: 25 });user.save();Querying DocumentsTo query documents from a collection, you use the find() method.User.find({ name: 'Alice' });With Projection:User.find({ name: 'Alice' }, { name: 1, age: 1 });Updating DocumentsTo update documents in a collection, you use the updateOne() method.User.updateOne({ name: 'Alice' }, { age: 26 });Deleting DocumentsTo delete documents from a collection, you use the deleteOne() method.User.deleteOne({ name: 'Alice' });MiddlewareMongoose middleware are functions that are executed before or after certain operations.userSchema.pre('save', function(next) {  console.log('Saving user...');  next();});VirtualsMongoose virtuals are document properties that you can get and set but that do not get persisted to MongoDB.userSchema.virtual('fullName').get(function() {  return this.name + ' ' + this.age;});PluginsMongoose plugins are reusable pieces of schema middleware that can be added to any schema.const timestampPlugin = require('./plugins/timestamp');userSchema.plugin(timestampPlugin);TransactionsMongoose transactions allow you to perform multiple operations on multiple documents in a single transaction.const session = await mongoose.startSession();session.startTransaction();try {  await User.create({ name: 'Alice' }, { session });  await User.create({ name: 'Bob' }, { session });  await session.commitTransaction();} catch (error) {  await session.abortTransaction();} finally {  session.endSession();}AggregationMongoose provides a fluent API for building aggregation pipelines.const result = await User.aggregate([  { $match: { name: 'Alice' } },  { $group: { _id: '$name', total: { $sum: '$age' } }]);IndexesMongoose allows you to define indexes on your schemas.userSchema.index({ name: 1 });PopulationMongoose population allows you to reference documents in other collections.const userSchema = new mongoose.Schema({  name: String,  posts: [{ type: mongoose.Schema.Types.ObjectId, ref: 'Post' }]});ValidationMongoose provides built-in validation for schema fields.const userSchema = new mongoose.Schema({  name: { type: String, required: true }});Patterns  Bucket Pattern  Attribute Pattern  Outlier Pattern  Subset Pattern            Pattern      Description      Use Case      Advantages      Disadvantages                  Bucket Pattern      Groups related documents into fixed-size ‚Äúbuckets‚Äù or arrays      Time-series data, IoT sensor readings      - Reduces number of documents- Improves query performance for range scans      - Complex to update individual items- May lead to document growth              Attribute Pattern      Stores a set of fields with similar access patterns as an embedded document      Products with varying attributes      - Flexible schema- Efficient querying of common attributes      - More complex queries for specific attributes- Potential for unused fields              Outlier Pattern      Stores common data in one collection and rare, oversized data in another      Social media posts with varying engagement levels      - Optimizes for common case performance- Prevents document size issues      - Requires two queries for outliers- More complex application logic              Subset Pattern      Stores a subset of fields from a document in a separate collection      User profiles with frequently accessed fields      - Improves read performance for common queries- Reduces working set size      - Data duplication- Requires keeping subsets in sync      Q&amp;AWhat is .exec() in Mongoose?The exec() function in Mongoose is used to execute a query and return a promise. It allows you to chain query methods and then execute the query at the end.User.find({ name: 'Alice' }).exec();You can run the query without exec(), by callback or using async/await.User.find({ name: 'Alice' }, (error, users) =&gt; {  console.log(users);});const users = await User.find({ name: 'Alice' });What is the difference between findOne() and find() in Mongoose?  find(): Returns an array of all documents that match the query criteria.  findOne(): Returns the first document that matches the query criteria.What is the difference between Model.create() and new Model().save() in Mongoose?  Model.create(): Creates a new document and saves it to the database in a single step.User.create({ name: 'Alice' });  new Model().save(): reates a new instance of the model but doesn‚Äôt save it to the database immediately. You can modify the instance, perform validations, or run any other operations before calling .save() to persist the changes.const doc = new Model({ name: 'John', age: 30 });doc.age = 31; // Modify the documentawait doc.save(); // Save the document after modificationWhat is the purpose of the lean() method in Mongoose queries, and when should it be used?The lean() method in Mongoose queries returns plain JavaScript objects instead of Mongoose documents which come with a lot of additional features, such as getters, setters, and methods that are useful for working with the document . It should be used when you don‚Äôt need the full Mongoose document features and want to improve query performance.User.find({ name: 'Alice' }).lean();How to implement soft deletes in Mongoose?Soft deletes in Mongoose involve marking documents as deleted instead of physically removing them from the database. You can achieve this by adding a deleted field to your schema and setting it to true when a document is deleted.const userSchema = new mongoose.Schema({  name: String,  deleted: { type: Boolean, default: false }});Use pre middleware to exclude deleted documents from query results.userSchema.pre(/^find/, function(next) {  this.where({ deleted: false });  next();});Add a method to ‚Äúsoft delete‚Äù a document.userSchema.methods.softDelete = function() {  this.deleted = true;  return this.save();};"
  },
  
  {
    "title": "MERN - Express.js Fundamentals",
    "url": "/posts/expressjs-fundamental/",
    "categories": "mern, expressjs",
    "tags": "expressjs, js, nodejs, mern",
    "date": "2024-09-19 07:00:00 +0700",
    





    
    "snippet": "FundamentalsExpress.js is a popular web application framework for Node.js that simplifies building web applications and APIs.      Middleware: Functions that access the request and response objects...",
    "content": "FundamentalsExpress.js is a popular web application framework for Node.js that simplifies building web applications and APIs.      Middleware: Functions that access the request and response objects to modify, add data, or trigger other functions.        Router: A mini-app that only deals with routing. It can have its middleware and routing logic.        Handler: A function that handles a specific route or endpoint.        Error Middleware: Middleware functions that have an extra parameter for error handling.  graph     subgraph expressApp[\"fa:fa-server Express.js Application\"]            request[\"üî¥ Request\"]            middleware1[\"üö™Middleware 1\"]            middleware2[\"üö™Middleware 2\"]            router[\"fa:fa-sitemap Router\"]            routerMiddleware[\"üö™Router Middleware\"]            handler[\"fa:fa-wrench Handler\"]            errorMiddleware[\"üö® Error Middleware\"]            response[\"üîµ Response\"]    end    request --&gt; middleware1 --&gt; middleware2 --&gt; router     router --&gt; routerMiddleware --&gt; handler     handler --&gt; errorMiddleware --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style middleware1 fill:#ccf,stroke:#f66,stroke-width:2px    style middleware2 fill:#ff9,stroke:#333,stroke-width:2px    style router fill:#ccf,stroke:#f66,stroke-width:2px    style routerMiddleware fill:#add8e6,stroke:#333,stroke-width:2px    style handler fill:#9cf,stroke:#333,stroke-width:2px    style errorMiddleware stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2px    classDef requestClass fill:#f9f,stroke:#333,stroke-width:2px;    classDef middlewareClass fill:#ccf,stroke:#f66,stroke-width:2px;    classDef routerClass fill:#ccf,stroke:#f66,stroke-width:2px;    classDef routerMiddlewareClass fill:#add8e6,stroke:#333,stroke-width:2px;    classDef handlerClass fill:#9cf,stroke:#333,stroke-width:2px;    classDef errorClass stroke:#333,stroke-width:2px;    classDef responseClass fill:#9cf,stroke:#333,stroke-width:2px;MiddlewareA request-response cycle in Express.js involves a series of middleware functions that execute sequentially. Each middleware can modify the request and response objects, end the request-response cycle, or call the next middleware in the stack.graph     subgraph expressApp[\"fa:fa-server Express.js Application\"]            request[\"üî¥ Request\"]            middleware1[\"üö™ Middleware 1\"]            middleware2[\"üö™Middleware 2\"]            endRequest[\"fa:fa-stop End Request\"]            throwError[\"fa:fa-exclamation-triangle Error Middleware\"]    end    request --&gt; middleware1    middleware1 --&gt; |chain| middleware2    middleware1 --&gt; |end| endRequest    middleware1 --&gt; |error| throwError    style request fill:#f9f,stroke:#333,stroke-width:2px    style middleware1 fill:#ccf,stroke:#f66,stroke-width:2px    style middleware2 fill:#ff9,stroke:#333,stroke-width:2px    style endRequest fill:#fcc,stroke:#333,stroke-width:2px    style throwError stroke:#333,stroke-width:2px  Middleware Functions: Execute sequentially, each modifying the request/response objects or ending the request-response cycle. Examples: Logging, authentication, parsing data  const express = require('express');  const app = express();  app.use((req, res, next) =&gt; { // ( üî¥, üîµ, üö™)    console.log('Middleware 1');    next();  });  app.use((req, res, next) =&gt; {    console.log('Middleware 2');    res.send('Hello, Middleware Flow!');  });  app.listen(3000);Below is an example of how middleware functions involve in the request-response cycle.graph TD        subgraph           cors1[cors üåê] --&gt; passportInitialize[\"passport.initialize() (Passport) üõÇ\"]        passportInitialize --&gt; authLimiter[\"authLimiter (Rate Limiter) üö¶\"]        authLimiter --&gt; routes[\"v1 Routes üõ£Ô∏è\"]        routes --&gt; docsRoute[\"Docs Route üìÑ\"]        docsRoute --&gt; notFoundHandler[\"404 Handler ‚ö†Ô∏è\"]        notFoundHandler --&gt; errorConverter[\"errorConverter ‚öôÔ∏è\"]        errorConverter --&gt; errorHandler[\"errorHandler üö´\"]        errorHandler --&gt; response[\"Response ‚û°Ô∏è\"]        style passportInitialize fill:#ccf,stroke:#f66,stroke-width:2px        style authLimiter fill:#ff9,stroke:#333,stroke-width:2px        style routes fill:#9cf,stroke:#333,stroke-width:2px        style docsRoute stroke:#333,stroke-width:2px        style notFoundHandler fill:#ccc,stroke:#333,stroke-width:2px        style errorConverter fill:#fcc,stroke:#333,stroke-width:2px        style errorHandler fill:#faa,stroke:#333,stroke-width:2px        style response fill:#9cf,stroke:#333,stroke-width:2px    end    subgraph Middleware[\"Middleware Flow\"]        request[\"Request üåê\"] --&gt; morgan[\"Morgan (Logger) üìù\"]        morgan --&gt; helmet[\"Helmet (Security) üîí\"]        helmet --&gt; expressJson[\"express.json() üìÑ\"]        expressJson --&gt; expressUrlEncoded[\"express.urlencoded() üìù\"]        expressUrlEncoded --&gt; expressFileupload[\"express-fileupload üìÅ\"]        expressFileupload --&gt; xss[\"xss-clean üßπ\"]        xss --&gt; mongoSanitize[\"express-mongo-sanitize üõ°Ô∏è\"]        mongoSanitize --&gt; compression[\"compression üóúÔ∏è\"]        compression --&gt; cors[\"cors üåê\"]        style request fill:#f9f,stroke:#333,stroke-width:2px        style morgan fill:#ccf,stroke:#f66,stroke-width:2px        style helmet fill:#ff9,stroke:#333,stroke-width:2px        style expressJson fill:#9cf,stroke:#333,stroke-width:2px        style expressUrlEncoded stroke:#333,stroke-width:2px        style expressFileupload fill:#ccc,stroke:#333,stroke-width:2px        style xss fill:#fcc,stroke:#333,stroke-width:2px        style mongoSanitize fill:#faa,stroke:#333,stroke-width:2px        style compression fill:#9cf,stroke:#333,stroke-width:2px        style cors fill:#afa,stroke:#333,stroke-width:2px    endRoutinggraph subgraph expressApp[\"fa:fa-server Express.js Application\"]  request[\"üî¥ Request\"] --&gt; middleware1[\"üö™Middlewares\"]   middleware1 --&gt; router  subgraph router[\"fa:fa-sitemap Router\"]    authRoute[\"/auth üîë\"]    userRoute[\"/user üßë\"]    clientRoute[\"/client üè¢\"]    remains[\"...\"]    notFoundRoute[\"404\"]  end  router --&gt; errorMiddleware[\"üö® Error Middleware\"]  errorMiddleware --&gt; response[\"üîµ Response\"]endstyle request fill:#f9f,stroke:#333,stroke-width:2pxstyle middleware1 fill:#ccf,stroke:#f66,stroke-width:2pxstyle router fill:#add8e6,stroke:#333,stroke-width:2pxstyle authRoute fill:#9cf,stroke:#333,stroke-width:2pxstyle userRoute stroke:#333,stroke-width:2pxstyle clientRoute fill:#ccc,stroke:#333,stroke-width:2pxstyle notFoundRoute fill:#faa,stroke:#333,stroke-width:2pxstyle errorMiddleware stroke:#333,stroke-width:2pxstyle response fill:#9cf,stroke:#333,stroke-width:2pxclassDef requestClass fill:#f9f,stroke:#333,stroke-width:2px;classDef middlewareClass fill:#ccf,stroke:#f66,stroke-width:2px;classDef routerClass fill:#add8e6,stroke:#333,stroke-width:2px;classDef routeHandlerClass fill:#9cf,stroke:#333,stroke-width:2px;classDef errorClass stroke:#333,stroke-width:2px;classDef responseClass fill:#9cf,stroke:#333,stroke-width:2px;const express = require('express');const router = express.Router();// Import Route Modules (Assume these contain route handlers)const authRoute = require('./auth.route'); const userRoute = require('./user.route');const clientRoute = require('./client.route');// Mount Routes on the Routerrouter.use('/auth', authRoute);     // Authentication routes (e.g., login, signup)router.use('/user', userRoute);     // User management routesrouter.use('/client', clientRoute); // Client-related routes// ... other routes (omitted for brevity)// 404 Not Found Handlerrouter.use((req, res, next) =&gt; {  // ... (Logic for handling 404 errors)});module.exports = router;// ... In your main app.js file:const app = express();// ... (Other middleware like body-parser, cors, etc.)// Mount the Routerapp.use('/api', router); // Prefix all routes with '/api'// ... (Error handling middleware)app.listen(3000, () =&gt; {  console.log('Server is running on port 3000');});  Modular Routes: Each route module (authRoute, userRoute, clientRoute) is responsible for a specific set of endpoints, promoting organization and maintainability.      Router Middleware: You can add middleware functions directly to the router using router.use(). These middleware will only apply to the routes defined within this router.    404 Handler: The router.use() at the end acts as a catch-all route to handle requests that don‚Äôt match any defined routes. It would typically send a ‚ÄúNot Found‚Äù (404) response.Pamaeters and Query Strings  Route Parameters: Extracted from the URL path using a colon followed by the parameter name (e.g., /users/:id).  Query Strings: Extracted from the URL query string using the req.query object (e.g., /users?name=John).// Route Parametersapp.get('/users/:id', (req, res) =&gt; {  const { id } = req.params;  res.send(`User ID: ${id}`);});// Query Stringsapp.get('/users', (req, res) =&gt; {  const { name } = req.query;  res.send(`User Name: ${name}`);});graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        routeParams[\"/users/:id\"]        queryParams[\"/users?name=John\"]        routeHandler1[\"fa:fa-wrench Route Handler\"]        routeHandler2[\"fa:fa-wrench Route Handler\"]    end    request --&gt; routeParams    routeParams --&gt; routeHandler1    request --&gt; queryParams    queryParams --&gt; routeHandler2    style request fill:#f9f,stroke:#333,stroke-width:2px    style routeParams fill:#ccf,stroke:#f66,stroke-width:2px    style queryParams fill:#ff9,stroke:#333,stroke-width:2pxPriority of Routes  Exact Match: Routes with exact matches take precedence over dynamic routes.  Dynamic Routes: Routes with dynamic parameters (e.g., /users/:id) are matched next.  Wildcard Routes: Routes with wildcards (e.g., /users/*) are matched last.app.get('/users', (req, res) =&gt; {    res.send('All users');});app.get('/users/new', (req, res) =&gt; {    res.send('New user form');});app.get('/users/:id', (req, res) =&gt; {    res.send(`User ID: ${req.params.id}`);});app.get('/users/*', (req, res) =&gt; {    res.send('Wildcard route');});graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        usersRoute[\"/users\"]        newUserRoute[\"/users/new\"]        userIdRoute[\"/users/:id\"]        wildcardRoute[\"/users/*\"]        response[\"üîµ Response\"]    end    request --&gt; usersRoute    usersRoute --&gt; response    request --&gt; newUserRoute    newUserRoute --&gt; response    request --&gt; userIdRoute    userIdRoute --&gt; response    request --&gt; wildcardRoute    wildcardRoute --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style usersRoute fill:#ccf,stroke:#f66,stroke-width:2px    style newUserRoute fill:#ff9,stroke:#333,stroke-width:2px    style userIdRoute fill:#9cf,stroke:#333,stroke-width:2px    style wildcardRoute fill:#9cf,stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2pxRoute Composition  Route Composition: Use express.Router() to create modular, mountable route handlers. This allows you to define routes in separate files and mount them at any path.// routes/auth.route.jsconst express = require('express');const router = express.Router();router.post('/login', (req, res) =&gt; {  // Login logic});router.post('/signup', (req, res) =&gt; {  // Signup logic});module.exports = router;// routes/user.route.jsconst express = require('express');const router = express.Router();router.get('/', (req, res) =&gt; {  // Get all users});router.get('/:id', (req, res) =&gt; {  // Get user by ID});module.exports = router;// app.jsconst express = require('express');const app = express();const authRoute = require('./routes/auth.route');const userRoute = require('./routes/user.route');app.use('/auth', authRoute);app.use('/users', userRoute);app.listen(3000);graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        authRoute[\"/auth\"]        userRoute[\"/users\"]        authHandler[\"fa:fa-wrench Auth Handler\"]        userHandler[\"fa:fa-wrench User Handler\"]    end    request --&gt; authRoute    authRoute --&gt; authHandler    request --&gt; userRoute    userRoute --&gt; userHandler    style request fill:#f9f,stroke:#333,stroke-width:2px    style authRoute fill:#ccf,stroke:#f66,stroke-width:2px    style userRoute fill:#ff9,stroke:#333,stroke-width:2px    style authHandler fill:#9cf,stroke:#333,stroke-width:2px    style userHandler fill:#9cf,stroke:#333,stroke-width:2pxViewsgraph subgraph views[\"üñºÔ∏è Views\"]    engine[\"Engine\"] --&gt; template[\"üìÑ Template\"]    template --&gt; data[\"Data\"]    data --&gt; rendered[\"üñåÔ∏è  Rendered HTML\"]end  Templates: Use template engines like Pug, EJS, or Handlebars to create dynamic HTML.    const express = require('express');const app = express();app.set('view engine', 'pug');app.get('/', (req, res) =&gt; {  res.render('index', { title: 'Express', message: 'Hello there!' });});app.listen(3000);        Rendering: Generates and returns HTML based on the templates and data provided.    // views/index.pughtml  head    title= title  body    h1= message      Static FilesStatic files are assets that don‚Äôt change dynamically, such as images, CSS stylesheets, and client-side JavaScript files. express.static() is a middleware function, meaning it intercepts requests before they reach your route handlers.  const express = require('express');  const app = express();  app.use(express.static(path.join(__dirname, 'public')));   app.listen(3000);Directory: Specify the directory from which to serve static assets.graph LR    root[\"my-app/ üìÇ\"] --&gt; public[\"public/ üìÅ\"];    public --&gt; images[\"images/ üñºÔ∏è\"];    public --&gt; css[\"css/ üé®\"];    public --&gt; js[\"js/ ‚öôÔ∏è\"];    public --&gt; csv[\"data.csv üìÑ\"];    classDef folder fill:#f9f,stroke:#333,stroke-width:2px;    classDef file fill:#ccf,stroke:#f66,stroke-width:2px;        style root fill:#F0E68C    style public fill:#90EE90    style images fill:#ADD8E6    style css fill:#F08080    style js fill:#98FB98    style csv fill:#F0F8FFUse Stream to download files  in public directory.const fs = require('fs');app.get('/download-csv', (req, res) =&gt; {  const filePath = path.join(__dirname, 'public', 'path/to/your/file.csv');  // Check if file exists  if (!fs.existsSync(filePath)) {    return res.status(404).send('File not found');  }  // Set headers for download  res.setHeader('Content-Disposition', 'attachment; filename=file.csv');  res.setHeader('Content-Type', 'text/csv');  // Pipe the file to the response  const fileStream = fs.createReadStream(filePath);  fileStream.pipe(res);});LoggingLog errors to the console or a file for debugging and monitoring. Datadog, Sentry, or other services can be used for more advanced error logging.function logErrors(err, req, res, next) {  console.error(err.stack); // Log to console in development  // You can replace this with logging to a file or external service  next(err); }Modern applications need standardized logging to monitor and debug issues effectively. Winston and Morgan are popular logging libraries for Node.js applications. Key requirements for logging include:  Structured Format: Log messages should be in a structured format (e.g., JSON) for easy parsing and analysis.  Log Levels: Different log levels (e.g., info, warn, error) help categorize log messages based on severity.‚Äô  Timestamps: Include timestamps in log messages to track when events occurred.  Context: Additional metadata.  Monitoring: Centralized logging solutions (e.g., Datadog, Sentry) for monitoring and alerting.// src/config/logger.jsconst winston = require('winston');const path = require('path');const logger = winston.createLogger({  level: process.env.LOG_LEVEL || 'info',  format: winston.format.combine(    winston.format.timestamp(),    winston.format.json()  ),  transports: [    // Console transport    new winston.transports.Console({      format: winston.format.combine(        winston.format.colorize(),        winston.format.simple()      )    }),    // Error file transport    new winston.transports.File({      filename: path.join(__dirname, '../../logs/error.log'),      level: 'error'    }),    // Combined file transport    new winston.transports.File({      filename: path.join(__dirname, '../../logs/combined.log')    })  ]});module.exports = logger;Log Outputs// Example usage in your application:try {  // Some operation that might fail  throw new Error('Database connection failed');} catch (error) {  // Error level (most serious issues)  logger.error('Database connection error', {    error: error.message,    stack: error.stack,    database: 'postgres'  });  /* Output:  {    \"level\": \"error\",    \"message\": \"Database connection error\",    \"timestamp\": \"2024-11-08T10:30:45.123Z\",    \"error\": \"Database connection failed\",    \"stack\": \"Error: Database connection failed\\n    at ...\",    \"database\": \"postgres\"  }  */  // Warning level (potential issues)  logger.warn('High API latency detected', {    latency: 1500,    endpoint: '/api/users'  });  /* Output:  {    \"level\": \"warn\",    \"message\": \"High API latency detected\",    \"timestamp\": \"2024-11-08T10:30:45.123Z\",    \"latency\": 1500,    \"endpoint\": \"/api/users\"  }  */  // Info level (normal operations)  logger.info('API request received', {    method: 'GET',    path: '/api/users'  });  /* Output:  {    \"level\": \"info\",    \"message\": \"API request received\",    \"timestamp\": \"2024-11-08T10:30:45.123Z\",    \"method\": \"GET\",    \"path\": \"/api/users\"  }  */}Error HandlingLogging Errorsgraph TD        routeHandler --error--&gt; nextError[\"fa:fa-exclamation next(err)\"]    request[\"üî¥\"] --&gt; middleware1[\"üö™Middleware 1\"] --error--&gt; nextError[\"fa:fa-door-open next\"]    middleware2 --error--&gt; nextError    middleware1 --&gt; middleware2[\"üö™Middleware 2\"]    middleware2 --&gt; routeHandler[\"fa:fa-wrench Route Handler\"]    routeHandler --&gt; response[\"üîµ\"]    nextError --&gt; logErrors[\"logErrors üìù\"]    logErrors --&gt; clientErrorHandler[\"clientErrorHandler üë•\"]    clientErrorHandler --&gt; errorHandler[\"errorHandler üö´\"]    errorHandler --&gt; response        style request fill:#f9f,stroke:#333,stroke-width:2px    style middleware1 fill:#ccf,stroke:#f66,stroke-width:2px    style middleware2 fill:#ff9,stroke:#333,stroke-width:2px    style routeHandler fill:#ccf,stroke:#f66,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2px    style nextError stroke:#333,stroke-width:2px    style logErrors fill:#a0d2eb,stroke:#333,stroke-width:2px    style clientErrorHandler fill:#f0e68c,stroke:#333,stroke-width:2px    style errorHandler stroke:#333,stroke-width:2pxFor small to medium applications, it‚Äôs better to centralize logging in middleware for consistency and easier maintenance. Here‚Äôs why:// ‚ùå First Version - Don't mix logging and throwingconst logger = require('../config/logger');class UserService {  async getUser(id) {    const user = await User.findById(id);        if (!user.isActive) {      // Bad practice: Logging before throwing      logger.error('Inactive user attempted access', {          userId: user.id,        status: user.status      });      throw new Error('Inactive user access denied');    }    return user;  }}// ‚úÖ Second Version - Better approach with centralized logging// errors/AppError.jsclass AppError extends Error {  constructor(message, code) {    super(message);    this.code = code;  }}class InactiveUserError extends AppError {  constructor(userId, status) {    super('Inactive user access denied', 'INACTIVE_USER');    this.userId = userId;    this.status = status;  }}// services/userService.jsclass UserService {  async getUser(id) {    const user = await User.findById(id);        if (!user.isActive) {      throw new InactiveUserError(user.id, user.status);    }    return user;  }}// middleware/errorMiddleware.jsconst logger = require('../config/logger');function logErrors(err, req, res, next) {  logger.error('Error occurred', {    // Error details    error: {      message: err.message,      stack: process.env.NODE_ENV === 'development' ? err.stack : undefined,      code: err.code || 'UNKNOWN_ERROR',      // Include any custom properties from custom error classes      ...(err instanceof InactiveUserError ? {        userId: err.userId,        status: err.status      } : {})    },    // Request context    request: {      path: req.path,      method: req.method,      params: req.params,      query: req.query,      ip: req.ip    },    // User context (if available)    user: req.user ? {      id: req.user.id,      email: req.user.email    } : 'anonymous',    // Timestamp and environment    timestamp: new Date().toISOString(),    env: process.env.NODE_ENV  });  next(err);}// Example of how to use in Express app:app.use(logErrors);Benefits of centralized logging:  Single place to modify logging logic  Consistent log format  Easier to maintain  Cleaner service code  Separation of concernsThe only exception might be audit logging or business-specific logging that needs to be in the service layer.Handling different types of errorsTo handle different types of errors, you can create custom error classes that extend the built-in Error class. This approach allows you to categorize errors based on their type and provide a consistent error structure.graph TD    subgraph errorClasses[\"üö® Error Classes\"]        appError[\"AppError\"]        databaseError[\"DatabaseError\"]        validationError[\"ValidationError\"]        notFoundError[\"NotFoundError\"]        authError[\"AuthError\"]    end    subgraph errorMiddleware[\"üö® Error Middleware\"]        logErrors[\"logErrors üìù\"]        errorHandler[\"errorHandler üö´\"]    end    appError --&gt; databaseError    appError --&gt; validationError    appError --&gt; notFoundError    appError --&gt; authError    logErrors --&gt; errorHandler    style appError fill:#f9f,stroke:#333,stroke-width:2px    style databaseError fill:#ccf,stroke:#f66,stroke-width:2px    style validationError fill:#ff9,stroke:#333,stroke-width:2px    style notFoundError fill:#9cf,stroke:#333,stroke-width:2px    style authError fill:#9cf,stroke:#333,stroke-width:2px    style logErrors fill:#ccf,stroke:#f66,stroke-width:2px    style errorHandler fill:#ff9,stroke:#333,stroke-width:2px// src/utils/AppError.jsclass AppError extends Error {  constructor(message, statusCode, code) {    super(message);    this.statusCode = statusCode;    this.code = code;  }}class DatabaseError extends AppError {  constructor(message = 'Database error occurred') {    super(message, 500, 'DATABASE_ERROR');  }}class ValidationError extends AppError {  constructor(message = 'Validation failed') {    super(message, 400, 'VALIDATION_ERROR');  }}class NotFoundError extends AppError {  constructor(message = 'Resource not found') {    super(message, 404, 'NOT_FOUND');  }}class AuthError extends AppError {  constructor(message = 'Authentication failed') {    super(message, 401, 'AUTH_ERROR');  }}module.exports = {  AppError,  DatabaseError,  ValidationError,  NotFoundError,  AuthError};// Usage in your service// userService.jsconst { DatabaseError, NotFoundError } = require('../utils/AppError');class UserService {  async getUser(id) {    try {      const user = await User.findById(id);            if (!user) {        throw new NotFoundError('User not found');      }            return user;    } catch (error) {      if (error.name === 'MongoError') {        throw new DatabaseError('Database query failed');      }      throw error;    }  }}// Then in your error middleware// middleware/errorMiddleware.jsconst logger = require('../config/logger');function logErrors(err, req, res, next) {  logger.error('Error occurred', {    error: {      message: err.message,      code: err.code,      statusCode: err.statusCode,      stack: process.env.NODE_ENV === 'development' ? err.stack : undefined    },    request: {      path: req.path,      method: req.method    },    user: req.user?.id || 'anonymous'  });  next(err);}function errorHandler(err, req, res, next) {  const statusCode = err.statusCode || 500;    res.status(statusCode).json({    status: 'error',    code: err.code || 'UNKNOWN_ERROR',    message: err.message  });}This approach:  Keeps error types organized  Provides consistent error structure  Makes error handling predictable  Easy to add new error types  Simple to use in servicesQ&amp;AHow does Express.js determine whether to call the next middleware or an error-handling middleware?It depends on how the next function is called:  next(): Without arguments, it proceeds to the next regular middleware.  next(err): With an error argument, it skips to the error-handling middleware.app.use((req, res, next) =&gt; {    next(); // Calls the next regular middleware});app.use((req, res, next) =&gt; {    next(new Error('Error occurred')); // Calls the error-handling middleware});app.use((err, req, res, next) =&gt; {    res.status(500).send('Something broke!');});graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        middleware1[\"üîµMiddleware 1\"]        middleware2[\"üö™Middleware 2\"]        errorHandler[\"‚ö†Ô∏è Error Handler\"]        finalHandler[\"‚úÖ Final Handler\"]    end    request --&gt; |\"next()\"| middleware1    middleware1 --&gt; |\"next()\"| middleware2    middleware1 --&gt; |\"next(err)\"| errorHandler    middleware2 --&gt; |\"next()\"| finalHandler    middleware2 --&gt; |\"next(err)\"| errorHandler    style request fill:#f9f,stroke:#333,stroke-width:2px    style middleware1 fill:#ccf,stroke:#f66,stroke-width:2px    style middleware2 fill:#ff9,stroke:#333,stroke-width:2px    style errorHandler fill:#f66,stroke:#333,stroke-width:2px    style finalHandler fill:#cfc,stroke:#333,stroke-width:2pxWhat are the differences between req.query and req.params ?  req.query: Contains the query parameters in the URL (e.g., /users?name=John&amp;age=30).  req.params: Contains route parameters defined in the route path (e.g., /users/:id).app.get('/users', (req, res) =&gt; {    const { name, age } = req.query;    res.send(`Name: ${name}, Age: ${age}`);});app.get('/users/:id', (req, res) =&gt; {    const { id } = req.params;    res.send(`User ID: ${id}`);});graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        queryRoute[\"/users?name=John&amp;age=30\"]        paramsRoute[\"/users/:id\"]        queryHandler[\"fa:fa-wrench Query Handler\"]        paramsHandler[\"fa:fa-wrench Params Handler\"]        response[\"üîµ Response\"]    end    request --&gt; queryRoute    queryRoute --&gt; queryHandler    queryHandler --&gt; response    request --&gt; paramsRoute    paramsRoute --&gt; paramsHandler    paramsHandler --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style queryRoute fill:#ccf,stroke:#f66,stroke-width:2px    style paramsRoute fill:#ff9,stroke:#333,stroke-width:2px    style queryHandler fill:#9cf,stroke:#333,stroke-width:2px    style paramsHandler fill:#9cf,stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2pxHow to parse the request body?  express.json(): Middleware to parse JSON bodies.  express.urlencoded(): Middleware to parse URL-encoded bodies.  express.text(): Middleware to parse text bodies.  express.raw(): Middleware to parse raw bodies.const express = require('express');const app = express();app.use(express.json()); // Parse JSON bodiesapp.use(express.urlencoded({ extended: true })); // Parse URL-encoded bodiesapp.post('/users', (req, res) =&gt; {    const { name, age } = req.body;    res.send(`Name: ${name}, Age: ${age}`);});app.listen(3000);graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        jsonBody[\"{ name: 'John', age: 30 }\"]        urlEncodedBody[\"name=John&amp;age=30\"]        jsonMiddleware[\"{ } express.json()\"]        urlEncodedMiddleware[\"&amp;  express.urlencoded() \"]        response[\"üîµ Response\"]    end    request --&gt; jsonBody    jsonBody --&gt; jsonMiddleware    jsonMiddleware --&gt; response    request --&gt; urlEncodedBody    urlEncodedBody --&gt; urlEncodedMiddleware    urlEncodedMiddleware --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style jsonBody fill:#ccf,stroke:#f66,stroke-width:2px    style urlEncodedBody fill:#ff9,stroke:#333,stroke-width:2px    style jsonMiddleware fill:#9cf,stroke:#333,stroke-width:2px    style urlEncodedMiddleware fill:#9cf,stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2pxExplain the order of router precedence ?  Exact Match: Routes with exact matches take precedence over dynamic routes.  Dynamic Routes: Routes with dynamic parameters (e.g., /users/:id) are matched next.  Wildcard Routes: Routes with wildcards (e.g., /users/*) are matched last.app.get('/users', (req, res) =&gt; {    res.send('All users');});app.get('/users/new', (req, res) =&gt; {    res.send('New user form');});app.get('/users/:id', (req, res) =&gt; {    res.send(`User ID: ${req.params.id}`);});app.get('/users/*', (req, res) =&gt; {    res.send('Wildcard route');});graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        usersRoute[\"/users\"]        newUserRoute[\"/users/new\"]        userIdRoute[\"/users/:id\"]        wildcardRoute[\"/users/*\"]        response[\"üîµ Response\"]    end    request --&gt; usersRoute    usersRoute --&gt; response    request --&gt; newUserRoute    newUserRoute --&gt; response    request --&gt; userIdRoute    userIdRoute --&gt; response    request --&gt; wildcardRoute    wildcardRoute --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style usersRoute fill:#ccf,stroke:#f66,stroke-width:2px    style newUserRoute fill:#ff9,stroke:#333,stroke-width:2px    style userIdRoute fill:#9cf,stroke:#333,stroke-width:2px    style wildcardRoute fill:#9cf,stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2pxHow to handle file uploads ?  express-fileupload: Middleware to handle file uploads.  req.files: Object containing uploaded files.const express = require('express');const fileUpload = require('express-fileupload');const app = express();app.use(fileUpload());app.post('/upload', (req, res) =&gt; {    if (!req.files || Object.keys(req.files).length === 0) {        return res.status(400).send('No files were uploaded.');    }    let uploadedFile = req.files.file; // assuming the form field name is 'file'    // Read the content of the file    const fileContent = uploadedFile.data.toString();    // Process the file content    const updatedContent = processFile(fileContent);    // ...    res.send(updatedContent);});How to protect SQL Injection?      express-mongo-sanitize: Middleware to sanitize user input and prevent NoSQL injection.        xss-clean: Middleware to sanitize user input and prevent XSS attacks.  const express = require('express');const mongoSanitize = require('express-mongo-sanitize');const xss = require('xss-clean');const app = express();app.use(mongoSanitize());app.use(xss());///app.listen(3000);graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        mongoSanitize[\"express-mongo-sanitize üõ°Ô∏è\"]        xssClean[\"xss-clean üßπ\"]        handler[\"fa:fa-wrench Handler\"]        response[\"üîµ Response\"]    end    request --&gt; mongoSanitize    mongoSanitize --&gt; xssClean    xssClean --&gt; handler    handler --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style mongoSanitize fill:#ccf,stroke:#f66,stroke-width:2px    style xssClean fill:#ff9,stroke:#333,stroke-width:2px    style handler fill:#9cf,stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2pxHow to implement rate limiting?  express-rate-limit: Middleware to limit the number of requests from an IP address.const express = require('express');const rateLimit = require('express-rate-limit');const app = express();const authLimiter = rateLimit({    windowMs: 15 * 60 * 1000, // 15 minutes    max: 100, // limit each IP to 100 requests per windowMs    message: 'Too many requests from this IP, please try again after 15 minutes'});app.use('/auth', authLimiter);app.post('/auth/login', (req, res) =&gt; {    // Handle login logic});app.listen(3000);graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        authRoute[\"/auth/login\"]        authLimiter[\"authLimiter (Rate Limiter) üö¶\"]        handler[\"fa:fa-wrench Handler\"]        response[\"üîµ Response\"]    end    request --&gt; authRoute    authRoute --&gt; authLimiter    authLimiter --&gt; handler    handler --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style authRoute fill:#ccf,stroke:#f66,stroke-width:2px    style authLimiter fill:#ff9,stroke:#333,stroke-width:2px    style handler fill:#9cf,stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2pxHow to handle versioning in APIs?  Route Prefixing: Use a common prefix for all routes of a specific version.const express = require('express');const app = express();const v1Router = express.Router();const v2Router = express.Router();v1Router.get('/users', (req, res) =&gt; {    res.send('Users v1');});v2Router.get('/users', (req, res) =&gt; {    res.send('Users v2');});app.use('/v1', v1Router);app.use('/v2', v2Router);app.listen(3000);graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        v1Route[\"/v1/users\"]        v2Route[\"/v2/users\"]        v1Handler[\"fa:fa-wrench Users v1 Handler\"]        v2Handler[\"fa:fa-wrench Users v2 Handler\"]        response[\"üîµ Response\"]    end    request --&gt; v1Route    v1Route --&gt; v1Handler    v1Handler --&gt; response    request --&gt; v2Route    v2Route --&gt; v2Handler    v2Handler --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style v1Route fill:#ccf,stroke:#f66,stroke-width:2px    style v2Route fill:#ff9,stroke:#333,stroke-width:2px    style v1Handler fill:#9cf,stroke:#333,stroke-width:2px    style v2Handler fill:#9cf,stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2pxHow to handle CORS ?  cors: Middleware to enable Cross-Origin Resource Sharing (CORS) in Express.By default, browsers restrict cross-origin HTTP requests for security reasons, meaning a request from a different origin (like a frontend running on http://localhost:3000 trying to access an API on http://localhost:5000) would be blocked.With app.use(cors()), you‚Äôre allowing the server to accept requests from different origins, making it suitable for a frontend application hosted on a different domain than your API.const express = require('express');const cors = require('cors');const app = express();// Configuration object based on environmentconst corsOptions = {  origin: function (origin, callback) {    // List of allowed origins based on environment    const allowedOrigins = {      production: ['https://mywebside.com'],      development: ['https://dev.myside.com'],      local: ['http://localhost:3000']    };    // Get current environment from NODE_ENV (defaults to 'development')    const environment = process.env.NODE_ENV || 'development';    const whitelist = allowedOrigins[environment];    // Allow requests with no origin (like mobile apps, curl, postman)    if (!origin) {      return callback(null, true);    }    if (whitelist.indexOf(origin) !== -1) {      callback(null, true);    } else {      callback(new Error('Not allowed by CORS'));    }  },  credentials: true, // Allow credentials (cookies, authorization headers, etc)  methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],  allowedHeaders: ['Content-Type', 'Authorization']};// Apply CORS middlewareapp.use(cors(corsOptions));// Example routeapp.get('/', (req, res) =&gt; {  res.json({ message: 'Hello World!' });});const PORT = process.env.PORT || 3000;app.listen(PORT, () =&gt; {  console.log(`Server running in ${process.env.NODE_ENV || 'development'} mode on port ${PORT}`);});graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        corsRoute[\"/users\"]        cors[\"cors üåê\"]        handler[\"fa:fa-wrench Users Handler\"]        response[\"üîµ Response\"]    end    request --&gt; corsRoute    corsRoute --&gt; cors    cors --&gt; handler    handler --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style corsRoute fill:#ccf,stroke:#f66,stroke-width:2px    style cors fill:#ff9,stroke:#333,stroke-width:2px    style handler fill:#9cf,stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2pxHow to document APIs?  Swagger/OpenAPI: Use tools like Swagger UI or OpenAPI to document your APIs.const express = require('express');const swaggerUi = require('swagger-ui-express');const swaggerDocument = require('./swagger.json');const app = express();app.use('/api-docs', swaggerUi.serve, swaggerUi.setup(swaggerDocument));app.listen(3000);graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        swaggerRoute[\"/api-docs\"]        swaggerUi[\"swagger-ui-express üìÑ\"]        swaggerDocument[\"swagger.json üìÑ\"]        response[\"üîµ Response\"]    end    request --&gt; swaggerRoute    swaggerRoute --&gt; swaggerUi    swaggerUi --&gt; swaggerDocument    swaggerDocument --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style swaggerRoute fill:#ccf,stroke:#f66,stroke-width:2px    style swaggerUi fill:#ff9,stroke:#333,stroke-width:2px    style swaggerDocument fill:#9cf,stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2pxHow to manage environment variables ?  dotenv: Package to load environment variables from a .env file.require('dotenv').config();const express = require('express');const app = express();const PORT = process.env.PORT || 3000;app.listen(PORT, () =&gt; {    console.log(`Server is running on port ${PORT}`);});graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        dotenv[\"dotenv üìÑ\"]        handler[\"fa:fa-wrench Handler \\n process.env\"]    end      style dotenv fill:#ccf,stroke:#f66,stroke-width:2px    style handler fill:#9cf,stroke:#333,stroke-width:2pxHow to nest routers?  Router Nesting: Mount routers within other routers to create a nested routing structure.const express = require('express');const app = express();const userRouter = express.Router();const profileRouter = express.Router();userRouter.use('/profile', profileRouter);profileRouter.get('/', (req, res) =&gt; {    res.send('Profile');});app.use('/users', userRouter);app.listen(3000);graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        userRoute[\"/users\"]        profileRoute[\"/profile\"]        profileHandler[\"fa:fa-wrench Profile Handler\"]        response[\"üîµ Response\"]    end    request --&gt; userRoute    userRoute --&gt; profileRoute    profileRoute --&gt; profileHandler    profileHandler --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style userRoute fill:#ccf,stroke:#f66,stroke-width:2px    style profileRoute fill:#ff9,stroke:#333,stroke-width:2px    style profileHandler fill:#9cf,stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-widthHow to compress responses in Express.js?  compression: Middleware to compress responses using gzip or deflate.const express = require('express');const compression = require('compression');const app = express();app.use(compression());app.get('/users', (req, res) =&gt; {    res.send('Users');});app.listen(3000);graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        compression[\"compression üóúÔ∏è\"]        handler[\"fa:fa-wrench Handler\"]        response[\"üîµ Response\"]    end    request --&gt; compression    compression --&gt; handler    handler --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style compression fill:#ccf,stroke:#f66,stroke-width:2px    style handler fill:#9cf,stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2pxHow to validate request data ?  express-validator: Middleware to validate and sanitize request data.const express = require('express');const { body, validationResult } = require('express-validator');const app = express();app.post('/users',     body('email').isEmail(),    body('password').isLength({ min: 6 }),    (req, res) =&gt; {        const errors = validationResult(req);        if (!errors.isEmpty()) {            return res.status(400).json({ errors: errors.array() });        }        res.send('User created');    });app.listen(3000);graph TD    subgraph expressApp[\"fa:fa-server Express.js Application\"]        request[\"üî¥ Request\"]        validationRoute[\"/users\"]        validationMiddleware[\"express-validator üõ°Ô∏è\"]        handler[\"fa:fa-wrench Handler\"]        response[\"üîµ Response\"]    end    request --&gt; validationRoute    validationRoute --&gt; validationMiddleware    validationMiddleware --&gt; handler    handler --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style validationRoute fill:#ccf,stroke:#f66,stroke-width:2px    style validationMiddleware fill:#ff9,stroke:#333,stroke-width:2px    style handler fill:#9cf,stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2pxHow do you implement rate limiting in Express.js?  express-rate-limit: Middleware to limit the number of requests from an IP address.const express = require('express');const rateLimit = require('express-rate-limit');const app = express();const authLimiter = rateLimit({    windowMs: 15 * 60 * 1000, // 15 minutes    max: 100, // limit each IP to 100 requests per windowMs    message: 'Too many requests from this IP, please try again after 15 minutes'}); app.use('/auth', authLimiter);app.post('/auth/login', (req, res) =&gt; {    // Handle login logic});app.listen(3000);What‚Äôs the recommended way to handle async errors in Express route handlers?Using async/await with express-async-handler is a common pattern to handle async errors in Express route handlers. This library wraps route handlers with a try-catch block and forwards errors to the error-handling middleware.const express = require('express');const asyncHandler = require('express-async-handler');const app = express();app.get('/route', asyncHandler(async (req, res) =&gt; {    // Simulate an async operation    const result = await someAsyncFunction();    res.send(result);}));// Error-handling middlewareapp.use((err, req, res, next) =&gt; {    res.status(500).json({ message: err.message });});function someAsyncFunction() {    return new Promise((resolve, reject) =&gt; {        setTimeout(() =&gt; reject(new Error(\"Something went wrong!\")), 1000);    });}app.listen(3000, () =&gt; {    console.log('Server is running on port 3000');});How to implement session management in Express.js?  express-session: Middleware to manage sessions in Express.const express = require('express');const session = require('express-session');const app = express();app.use(session ({    secret    resave: false,    saveUninitialized: true,    cookie: { secure: false }}));app.get('/', (req, res) =&gt; {    if (req.session.views) {        req.session.views++;    } else {        req.session.views = 1;    }    res.send(`Views: ${req.session.views}`);});app.listen(3000);How to Redirect in Express.js?  res.redirect(): Method to redirect to a different URL.const express = require('express');const app = express();app.get('/old', (req, res) =&gt; {    res.redirect('/new');});app.get('/new', (req, res) =&gt; {    res.send('New route');});app.listen(3000);Keywords To Remembergraph     subgraph expressApp[\"fa:fa-server Express.js Application\"]            request[\"üî¥ \"]            middleware1[\"üö™\"]            middleware2[\"üö™\"]            router[\"fa:fa-sitemap\"]            routerMiddleware[\"üö™\"]            handler[\"fa:fa-wrench \"]            errorMiddleware[\"üö®\"]            response[\"üîµ \"]    end    request --&gt; middleware1 --&gt; middleware2 --&gt; router     router --&gt; routerMiddleware --&gt; handler     handler --&gt; errorMiddleware --&gt; response    style request fill:#f9f,stroke:#333,stroke-width:2px    style middleware1 fill:#ccf,stroke:#f66,stroke-width:2px    style middleware2 fill:#ff9,stroke:#333,stroke-width:2px    style router fill:#ccf,stroke:#f66,stroke-width:2px    style routerMiddleware fill:#add8e6,stroke:#333,stroke-width:2px    style handler fill:#9cf,stroke:#333,stroke-width:2px    style errorMiddleware stroke:#333,stroke-width:2px    style response fill:#9cf,stroke:#333,stroke-width:2px    classDef requestClass fill:#f9f,stroke:#333,stroke-width:2px;    classDef middlewareClass fill:#ccf,stroke:#f66,stroke-width:2px;    classDef routerClass fill:#ccf,stroke:#f66,stroke-width:2px;    classDef routerMiddlewareClass fill:#add8e6,stroke:#333,stroke-width:2px;    classDef handlerClass fill:#9cf,stroke:#333,stroke-width:2px;    classDef errorClass fill:#f61,stroke:#333,stroke-width:2px;    classDef responseClass fill:#9cf,stroke:#333,stroke-width:2px;"
  },
  
  {
    "title": "MERN - Express.js Fundamentals",
    "url": "/posts/expressjs-fundamental-processed/",
    "categories": "mern, expressjs",
    "tags": "expressjs, js, nodejs, mern",
    "date": "2024-09-19 07:00:00 +0700",
    





    
    "snippet": "FundamentalsExpress.js is a popular web application framework for Node.js that simplifies building web applications and APIs.      Middleware: Functions that access the request and response objects...",
    "content": "FundamentalsExpress.js is a popular web application framework for Node.js that simplifies building web applications and APIs.      Middleware: Functions that access the request and response objects to modify, add data, or trigger other functions.        Router: A mini-app that only deals with routing. It can have its middleware and routing logic.        Handler: A function that handles a specific route or endpoint.        Error Middleware: Middleware functions that have an extra parameter for error handling.  MiddlewareA request-response cycle in Express.js involves a series of middleware functions that execute sequentially. Each middleware can modify the request and response objects, end the request-response cycle, or call the next middleware in the stack.Middleware Functions: Execute sequentially, each modifying the request/response objects or ending the request-response cycle. Examples: Logging, authentication, parsing data  const express = require('express');  const app = express();  app.use((req, res, next) =&gt; { // ( üî¥, üîµ, üö™)    console.log('Middleware 1');    next();  });  app.use((req, res, next) =&gt; {    console.log('Middleware 2');    res.send('Hello, Middleware Flow!');  });  app.listen(3000);Below is an example of how middleware functions involve in the request-response cycle.Routingconst express = require('express');const router = express.Router();// Import Route Modules (Assume these contain route handlers)const authRoute = require('./auth.route'); const userRoute = require('./user.route');const clientRoute = require('./client.route');// Mount Routes on the Routerrouter.use('/auth', authRoute);     // Authentication routes (e.g., login, signup)router.use('/user', userRoute);     // User management routesrouter.use('/client', clientRoute); // Client-related routes// ... other routes (omitted for brevity)// 404 Not Found Handlerrouter.use((req, res, next) =&gt; {  // ... (Logic for handling 404 errors)});module.exports = router;// ... In your main app.js file:const app = express();// ... (Other middleware like body-parser, cors, etc.)// Mount the Routerapp.use('/api', router); // Prefix all routes with '/api'// ... (Error handling middleware)app.listen(3000, () =&gt; {  console.log('Server is running on port 3000');});  Modular Routes: Each route module (authRoute, userRoute, clientRoute) is responsible for a specific set of endpoints, promoting organization and maintainability.      Router Middleware: You can add middleware functions directly to the router using router.use(). These middleware will only apply to the routes defined within this router.    404 Handler: The router.use() at the end acts as a catch-all route to handle requests that don‚Äôt match any defined routes. It would typically send a ‚ÄúNot Found‚Äù (404) response.Pamaeters and Query Strings  Route Parameters: Extracted from the URL path using a colon followed by the parameter name (e.g., /users/:id).  Query Strings: Extracted from the URL query string using the req.query object (e.g., /users?name=John).// Route Parametersapp.get('/users/:id', (req, res) =&gt; {  const { id } = req.params;  res.send(`User ID: ${id}`);});// Query Stringsapp.get('/users', (req, res) =&gt; {  const { name } = req.query;  res.send(`User Name: ${name}`);});Priority of Routes  Exact Match: Routes with exact matches take precedence over dynamic routes.  Dynamic Routes: Routes with dynamic parameters (e.g., /users/:id) are matched next.  Wildcard Routes: Routes with wildcards (e.g., /users/*) are matched last.app.get('/users', (req, res) =&gt; {    res.send('All users');});app.get('/users/new', (req, res) =&gt; {    res.send('New user form');});app.get('/users/:id', (req, res) =&gt; {    res.send(`User ID: ${req.params.id}`);});app.get('/users/*', (req, res) =&gt; {    res.send('Wildcard route');});Route Composition  Route Composition: Use express.Router() to create modular, mountable route handlers. This allows you to define routes in separate files and mount them at any path.// routes/auth.route.jsconst express = require('express');const router = express.Router();router.post('/login', (req, res) =&gt; {  // Login logic});router.post('/signup', (req, res) =&gt; {  // Signup logic});module.exports = router;// routes/user.route.jsconst express = require('express');const router = express.Router();router.get('/', (req, res) =&gt; {  // Get all users});router.get('/:id', (req, res) =&gt; {  // Get user by ID});module.exports = router;// app.jsconst express = require('express');const app = express();const authRoute = require('./routes/auth.route');const userRoute = require('./routes/user.route');app.use('/auth', authRoute);app.use('/users', userRoute);app.listen(3000);Views  Templates: Use template engines like Pug, EJS, or Handlebars to create dynamic HTML.    const express = require('express');const app = express();app.set('view engine', 'pug');app.get('/', (req, res) =&gt; {  res.render('index', { title: 'Express', message: 'Hello there!' });});app.listen(3000);        Rendering: Generates and returns HTML based on the templates and data provided.    // views/index.pughtml  head    title= title  body    h1= message      Static FilesStatic files are assets that don‚Äôt change dynamically, such as images, CSS stylesheets, and client-side JavaScript files. express.static() is a middleware function, meaning it intercepts requests before they reach your route handlers.  const express = require('express');  const app = express();  app.use(express.static(path.join(__dirname, 'public')));   app.listen(3000);Directory: Specify the directory from which to serve static assets.Use Stream to download files  in public directory.const fs = require('fs');app.get('/download-csv', (req, res) =&gt; {  const filePath = path.join(__dirname, 'public', 'path/to/your/file.csv');  // Check if file exists  if (!fs.existsSync(filePath)) {    return res.status(404).send('File not found');  }  // Set headers for download  res.setHeader('Content-Disposition', 'attachment; filename=file.csv');  res.setHeader('Content-Type', 'text/csv');  // Pipe the file to the response  const fileStream = fs.createReadStream(filePath);  fileStream.pipe(res);});LoggingLog errors to the console or a file for debugging and monitoring. Datadog, Sentry, or other services can be used for more advanced error logging.function logErrors(err, req, res, next) {  console.error(err.stack); // Log to console in development  // You can replace this with logging to a file or external service  next(err); }Modern applications need standardized logging to monitor and debug issues effectively. Winston and Morgan are popular logging libraries for Node.js applications. Key requirements for logging include:  Structured Format: Log messages should be in a structured format (e.g., JSON) for easy parsing and analysis.  Log Levels: Different log levels (e.g., info, warn, error) help categorize log messages based on severity.‚Äô  Timestamps: Include timestamps in log messages to track when events occurred.  Context: Additional metadata.  Monitoring: Centralized logging solutions (e.g., Datadog, Sentry) for monitoring and alerting.// src/config/logger.jsconst winston = require('winston');const path = require('path');const logger = winston.createLogger({  level: process.env.LOG_LEVEL || 'info',  format: winston.format.combine(    winston.format.timestamp(),    winston.format.json()  ),  transports: [    // Console transport    new winston.transports.Console({      format: winston.format.combine(        winston.format.colorize(),        winston.format.simple()      )    }),    // Error file transport    new winston.transports.File({      filename: path.join(__dirname, '../../logs/error.log'),      level: 'error'    }),    // Combined file transport    new winston.transports.File({      filename: path.join(__dirname, '../../logs/combined.log')    })  ]});module.exports = logger;Log Outputs// Example usage in your application:try {  // Some operation that might fail  throw new Error('Database connection failed');} catch (error) {  // Error level (most serious issues)  logger.error('Database connection error', {    error: error.message,    stack: error.stack,    database: 'postgres'  });  /* Output:  {    \"level\": \"error\",    \"message\": \"Database connection error\",    \"timestamp\": \"2024-11-08T10:30:45.123Z\",    \"error\": \"Database connection failed\",    \"stack\": \"Error: Database connection failed\\n    at ...\",    \"database\": \"postgres\"  }  */  // Warning level (potential issues)  logger.warn('High API latency detected', {    latency: 1500,    endpoint: '/api/users'  });  /* Output:  {    \"level\": \"warn\",    \"message\": \"High API latency detected\",    \"timestamp\": \"2024-11-08T10:30:45.123Z\",    \"latency\": 1500,    \"endpoint\": \"/api/users\"  }  */  // Info level (normal operations)  logger.info('API request received', {    method: 'GET',    path: '/api/users'  });  /* Output:  {    \"level\": \"info\",    \"message\": \"API request received\",    \"timestamp\": \"2024-11-08T10:30:45.123Z\",    \"method\": \"GET\",    \"path\": \"/api/users\"  }  */}Error HandlingLogging ErrorsFor small to medium applications, it‚Äôs better to centralize logging in middleware for consistency and easier maintenance. Here‚Äôs why:// ‚ùå First Version - Don't mix logging and throwingconst logger = require('../config/logger');class UserService {  async getUser(id) {    const user = await User.findById(id);    if (!user.isActive) {      // Bad practice: Logging before throwing      logger.error('Inactive user attempted access', {          userId: user.id,        status: user.status      });      throw new Error('Inactive user access denied');    }    return user;  }}// ‚úÖ Second Version - Better approach with centralized logging// errors/AppError.jsclass AppError extends Error {  constructor(message, code) {    super(message);    this.code = code;  }}class InactiveUserError extends AppError {  constructor(userId, status) {    super('Inactive user access denied', 'INACTIVE_USER');    this.userId = userId;    this.status = status;  }}// services/userService.jsclass UserService {  async getUser(id) {    const user = await User.findById(id);    if (!user.isActive) {      throw new InactiveUserError(user.id, user.status);    }    return user;  }}// middleware/errorMiddleware.jsconst logger = require('../config/logger');function logErrors(err, req, res, next) {  logger.error('Error occurred', {    // Error details    error: {      message: err.message,      stack: process.env.NODE_ENV === 'development' ? err.stack : undefined,      code: err.code || 'UNKNOWN_ERROR',      // Include any custom properties from custom error classes      ...(err instanceof InactiveUserError ? {        userId: err.userId,        status: err.status      } : {})    },    // Request context    request: {      path: req.path,      method: req.method,      params: req.params,      query: req.query,      ip: req.ip    },    // User context (if available)    user: req.user ? {      id: req.user.id,      email: req.user.email    } : 'anonymous',    // Timestamp and environment    timestamp: new Date().toISOString(),    env: process.env.NODE_ENV  });  next(err);}// Example of how to use in Express app:app.use(logErrors);Benefits of centralized logging:  Single place to modify logging logic  Consistent log format  Easier to maintain  Cleaner service code  Separation of concernsThe only exception might be audit logging or business-specific logging that needs to be in the service layer.Handling different types of errorsTo handle different types of errors, you can create custom error classes that extend the built-in Error class. This approach allows you to categorize errors based on their type and provide a consistent error structure.// src/utils/AppError.jsclass AppError extends Error {  constructor(message, statusCode, code) {    super(message);    this.statusCode = statusCode;    this.code = code;  }}class DatabaseError extends AppError {  constructor(message = 'Database error occurred') {    super(message, 500, 'DATABASE_ERROR');  }}class ValidationError extends AppError {  constructor(message = 'Validation failed') {    super(message, 400, 'VALIDATION_ERROR');  }}class NotFoundError extends AppError {  constructor(message = 'Resource not found') {    super(message, 404, 'NOT_FOUND');  }}class AuthError extends AppError {  constructor(message = 'Authentication failed') {    super(message, 401, 'AUTH_ERROR');  }}module.exports = {  AppError,  DatabaseError,  ValidationError,  NotFoundError,  AuthError};// Usage in your service// userService.jsconst { DatabaseError, NotFoundError } = require('../utils/AppError');class UserService {  async getUser(id) {    try {      const user = await User.findById(id);      if (!user) {        throw new NotFoundError('User not found');      }      return user;    } catch (error) {      if (error.name === 'MongoError') {        throw new DatabaseError('Database query failed');      }      throw error;    }  }}// Then in your error middleware// middleware/errorMiddleware.jsconst logger = require('../config/logger');function logErrors(err, req, res, next) {  logger.error('Error occurred', {    error: {      message: err.message,      code: err.code,      statusCode: err.statusCode,      stack: process.env.NODE_ENV === 'development' ? err.stack : undefined    },    request: {      path: req.path,      method: req.method    },    user: req.user?.id || 'anonymous'  });  next(err);}function errorHandler(err, req, res, next) {  const statusCode = err.statusCode || 500;  res.status(statusCode).json({    status: 'error',    code: err.code || 'UNKNOWN_ERROR',    message: err.message  });}This approach:  Keeps error types organized  Provides consistent error structure  Makes error handling predictable  Easy to add new error types  Simple to use in servicesQ&amp;AHow does Express.js determine whether to call the next middleware or an error-handling middleware?It depends on how the next function is called:  next(): Without arguments, it proceeds to the next regular middleware.  next(err): With an error argument, it skips to the error-handling middleware.app.use((req, res, next) =&gt; {    next(); // Calls the next regular middleware});app.use((req, res, next) =&gt; {    next(new Error('Error occurred')); // Calls the error-handling middleware});app.use((err, req, res, next) =&gt; {    res.status(500).send('Something broke!');});What are the differences between req.query and req.params ?  req.query: Contains the query parameters in the URL (e.g., /users?name=John&amp;age=30).  req.params: Contains route parameters defined in the route path (e.g., /users/:id).app.get('/users', (req, res) =&gt; {    const { name, age } = req.query;    res.send(`Name: ${name}, Age: ${age}`);});app.get('/users/:id', (req, res) =&gt; {    const { id } = req.params;    res.send(`User ID: ${id}`);});How to parse the request body?  express.json(): Middleware to parse JSON bodies.  express.urlencoded(): Middleware to parse URL-encoded bodies.  express.text(): Middleware to parse text bodies.  express.raw(): Middleware to parse raw bodies.const express = require('express');const app = express();app.use(express.json()); // Parse JSON bodiesapp.use(express.urlencoded({ extended: true })); // Parse URL-encoded bodiesapp.post('/users', (req, res) =&gt; {    const { name, age } = req.body;    res.send(`Name: ${name}, Age: ${age}`);});app.listen(3000);Explain the order of router precedence ?  Exact Match: Routes with exact matches take precedence over dynamic routes.  Dynamic Routes: Routes with dynamic parameters (e.g., /users/:id) are matched next.  Wildcard Routes: Routes with wildcards (e.g., /users/*) are matched last.app.get('/users', (req, res) =&gt; {    res.send('All users');});app.get('/users/new', (req, res) =&gt; {    res.send('New user form');});app.get('/users/:id', (req, res) =&gt; {    res.send(`User ID: ${req.params.id}`);});app.get('/users/*', (req, res) =&gt; {    res.send('Wildcard route');});How to handle file uploads ?  express-fileupload: Middleware to handle file uploads.  req.files: Object containing uploaded files.const express = require('express');const fileUpload = require('express-fileupload');const app = express();app.use(fileUpload());app.post('/upload', (req, res) =&gt; {    if (!req.files || Object.keys(req.files).length === 0) {        return res.status(400).send('No files were uploaded.');    }    let uploadedFile = req.files.file; // assuming the form field name is 'file'    // Read the content of the file    const fileContent = uploadedFile.data.toString();    // Process the file content    const updatedContent = processFile(fileContent);    // ...    res.send(updatedContent);});How to protect SQL Injection?      express-mongo-sanitize: Middleware to sanitize user input and prevent NoSQL injection.        xss-clean: Middleware to sanitize user input and prevent XSS attacks.  const express = require('express');const mongoSanitize = require('express-mongo-sanitize');const xss = require('xss-clean');const app = express();app.use(mongoSanitize());app.use(xss());///app.listen(3000);How to implement rate limiting?  express-rate-limit: Middleware to limit the number of requests from an IP address.const express = require('express');const rateLimit = require('express-rate-limit');const app = express();const authLimiter = rateLimit({    windowMs: 15 * 60 * 1000, // 15 minutes    max: 100, // limit each IP to 100 requests per windowMs    message: 'Too many requests from this IP, please try again after 15 minutes'});app.use('/auth', authLimiter);app.post('/auth/login', (req, res) =&gt; {    // Handle login logic});app.listen(3000);How to handle versioning in APIs?  Route Prefixing: Use a common prefix for all routes of a specific version.const express = require('express');const app = express();const v1Router = express.Router();const v2Router = express.Router();v1Router.get('/users', (req, res) =&gt; {    res.send('Users v1');});v2Router.get('/users', (req, res) =&gt; {    res.send('Users v2');});app.use('/v1', v1Router);app.use('/v2', v2Router);app.listen(3000);How to handle CORS ?  cors: Middleware to enable Cross-Origin Resource Sharing (CORS) in Express.By default, browsers restrict cross-origin HTTP requests for security reasons, meaning a request from a different origin (like a frontend running on http://localhost:3000 trying to access an API on http://localhost:5000) would be blocked.With app.use(cors()), you‚Äôre allowing the server to accept requests from different origins, making it suitable for a frontend application hosted on a different domain than your API.const express = require('express');const cors = require('cors');const app = express();// Configuration object based on environmentconst corsOptions = {  origin: function (origin, callback) {    // List of allowed origins based on environment    const allowedOrigins = {      production: ['https://mywebside.com'],      development: ['https://dev.myside.com'],      local: ['http://localhost:3000']    };    // Get current environment from NODE_ENV (defaults to 'development')    const environment = process.env.NODE_ENV || 'development';    const whitelist = allowedOrigins[environment];    // Allow requests with no origin (like mobile apps, curl, postman)    if (!origin) {      return callback(null, true);    }    if (whitelist.indexOf(origin) !== -1) {      callback(null, true);    } else {      callback(new Error('Not allowed by CORS'));    }  },  credentials: true, // Allow credentials (cookies, authorization headers, etc)  methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],  allowedHeaders: ['Content-Type', 'Authorization']};// Apply CORS middlewareapp.use(cors(corsOptions));// Example routeapp.get('/', (req, res) =&gt; {  res.json({ message: 'Hello World!' });});const PORT = process.env.PORT || 3000;app.listen(PORT, () =&gt; {  console.log(`Server running in ${process.env.NODE_ENV || 'development'} mode on port ${PORT}`);});How to document APIs?  Swagger/OpenAPI: Use tools like Swagger UI or OpenAPI to document your APIs.const express = require('express');const swaggerUi = require('swagger-ui-express');const swaggerDocument = require('./swagger.json');const app = express();app.use('/api-docs', swaggerUi.serve, swaggerUi.setup(swaggerDocument));app.listen(3000);How to manage environment variables ?  dotenv: Package to load environment variables from a .env file.require('dotenv').config();const express = require('express');const app = express();const PORT = process.env.PORT || 3000;app.listen(PORT, () =&gt; {    console.log(`Server is running on port ${PORT}`);});How to nest routers?  Router Nesting: Mount routers within other routers to create a nested routing structure.const express = require('express');const app = express();const userRouter = express.Router();const profileRouter = express.Router();userRouter.use('/profile', profileRouter);profileRouter.get('/', (req, res) =&gt; {    res.send('Profile');});app.use('/users', userRouter);app.listen(3000);How to compress responses in Express.js?  compression: Middleware to compress responses using gzip or deflate.const express = require('express');const compression = require('compression');const app = express();app.use(compression());app.get('/users', (req, res) =&gt; {    res.send('Users');});app.listen(3000);How to validate request data ?  express-validator: Middleware to validate and sanitize request data.const express = require('express');const { body, validationResult } = require('express-validator');const app = express();app.post('/users',     body('email').isEmail(),    body('password').isLength({ min: 6 }),    (req, res) =&gt; {        const errors = validationResult(req);        if (!errors.isEmpty()) {            return res.status(400).json({ errors: errors.array() });        }        res.send('User created');    });app.listen(3000);Keywords To Remember"
  },
  
  {
    "title": "MERN - Node.js Fundamentals",
    "url": "/posts/nodejs-fundemental/",
    "categories": "mern, nodejs",
    "tags": "nodejs, nodejs",
    "date": "2024-09-18 07:00:00 +0700",
    





    
    "snippet": "BasicWhat is NodeJs ?Node.js is a JavaScript runtime built on Chrome‚Äôs V8 JavaScript engine. It uses an event-driven, non-blocking I/O model that makes it lightweight and efficient. Node.js is idea...",
    "content": "BasicWhat is NodeJs ?Node.js is a JavaScript runtime built on Chrome‚Äôs V8 JavaScript engine. It uses an event-driven, non-blocking I/O model that makes it lightweight and efficient. Node.js is ideal for building scalable network applications, as it can handle a large number of connections simultaneously.In Node.js, I/O (Input/Output) refers to the operations that involve reading and writing data to various sources such as files, networks, databases, and other streams. I/O operations are fundamental in any application that interacts with external resources. Node.js is designed to handle these operations efficiently using a non-blocking, asynchronous I/O model.const fs = require('fs');fs.readFile('file.txt', 'utf8', (err, data) =&gt; {  if (err) throw err;  console.log(data);});// The program can continue executing other tasks hereconsole.log('This will print before the file content');graph LRsubgraph Fundamentals[\"‚ö° Fundamentals\"]    runtime[\"üèÉ Runtime\"]    nonblocking[\"üö´ Non-Blocking I/O\"]    eventdriven[\"‚ö° Event-Driven\"]    eventloop[\"üîÅ Event Loop\"]endWhat is Event Driven ?Event-driven Node.js refers to the programming paradigm where the flow of the application is determined by events like user actions or system messages. Node.js uses an event loop to handle asynchronous operations efficiently, allowing non-blocking I/O operations. This approach enables building scalable, real-time applications that can handle multiple concurrent connections with optimal performance.graph TD    subgraph eventLoop[\"üîÑ Event Loop\"]    end    callStack[\"üìö Call Stack\"]        subgraph callbackQueue[\"fa:fa-lines-leaning Callback Queue\"]        callback1([üìû Callback 1])        callback2([üìû Callback 2])    end    subgraph webAPIs[\"üåê APIs\"]        setTimeout[\"‚è≥ setTimeout()\"]        fetch[\"üåê fetch()\"]        readFile[\"üìÇ fs.readFile()\"]    end    eventLoop --&gt; callStack    eventLoop --&gt; callbackQueue    webAPIs --&gt; callbackQueue        style eventLoop fill:#f9f,stroke:#333,stroke-width:2px    style callStack fill:#ccf,stroke:#f66,stroke-width:2px    style callbackQueue fill:#ff9,stroke:#333,stroke-width:2px    style webAPIs fill:#9cf,stroke:#333,stroke-width:2px      Event Loop: Continuously checks if the Call Stack  is empty.          If empty, it takes the next callback from the Callback Queue.      If not empty, it continues checking.            Call Stack: Executes JavaScript code, including callbacks. After execution, control returns to the Event Loop.        Callback Queue: Holds callbacks until they are ready to be executed. Receives callbacks from Web APIs.        Web APIs: Perform asynchronous operations (timers, network requests, etc.). When complete, they add their callbacks to the Callback Queue.  What are I/O operations in computing, and why are they significant ?I/O (Input/Output) operations refer to the communication between a computer program and the outside world, typically involving reading from or writing to a device or file. In Node.js, I/O operations are crucial as they deal with tasks like reading/writing files, network communications, and database interactions. Node.js uses a non-blocking, event-driven architecture for I/O operations, allowing it to handle many concurrent connections efficiently. This design makes Node.js particularly well-suited for building scalable network applications and web servers that can manage numerous simultaneous I/O operationsgraph TD    subgraph nodeJs[\"üì¶ Node.js\"]        style nodeJs fill:#e6fff2,stroke:#66cc99        application[\"üíª Application\"]        eventLoop[\"üîÑ Event Loop\"]    end    subgraph ioOperations[\"üîå I/O Operations\"]        style ioOperations fill:#fff0f5,stroke:#ff69b4        fileSystem[\"üìÅ File System\"]        network[\"üåê Network\"]        database[\"üóÑÔ∏è Database\"]    end    application --&gt;|\"Requests\"| ioOperations    ioOperations --&gt;|\"Responds\"| eventLoop    eventLoop --&gt;|\"Handles\"| applicationIf other languages also support non-blocking I/O, why is it a standout feature in Node.js?Non-blocking I/O is a core feature in Node.js, integrated from the ground up, ensuring that almost all I/O operations are asynchronous by default. Unlike other languages where non-blocking I/O is an optional feature, in Node.js, it‚Äôs the default behavior. This leads to simplified development and improved performance for I/O-bound applications. The rich ecosystem of libraries and built-in support for non-blocking I/O further distinguishes Node.js in this regard. For CPU-bound tasks Node.js is not the best choice.Is it true that 1 nodejs application is 1 process and 1 threadWhile a basic Node.js application does run in a single process with one main JavaScript execution thread, it‚Äôs not strictly limited to one thread overall, and you have options to utilize multiple processes or additional threads if needed.What are the main approaches to implementing asynchronicity in Node.jsgraph LRsubgraph Asynchronous[\"‚è± Asynchronous\"]    callback[\"üìû Callback\"]    promise[\"ü§ù Promise\"]    asyncawait[\"‚è≥ Async/Await\"]end  Callback: A function passed as an argument to be executed later (after an async operation completes)function fetchData(callback) {  setTimeout(() =&gt; {    callback('Data received');  }, 1000);}fetchData((data) =&gt; {  console.log(data);});  Promise: A cleaner way to handle asynchronous results and errors. Prevents callback hell.const fetchData = () =&gt; {  return new Promise((resolve, reject) =&gt; {    setTimeout(() =&gt; {      resolve('Data received');    }, 1000);  });};fetchData().then(data =&gt; console.log(data));  Async/Await:  Makes asynchronous code look synchronous.const fetchData = () =&gt; {return new Promise((resolve, reject) =&gt; {  setTimeout(() =&gt; {    resolve('Data received');    // Uncomment the following line to simulate an error:    // reject(new Error('Failed to fetch data'));  }, 1000);});};async function getData() {  try {    const data = await fetchData();    console.log(data);  } catch (error) {    console.error('Error:', error);  }}getData();What are the key differences between CommonJS and ES Modules ?The key differences between CommonJS and ES Modules are:  Syntax: CommonJS uses require() and module.exports, while ES Modules use import and export statements.  Loading: CommonJS modules load synchronously at runtime, whereas ES Modules load asynchronously and support static analysis.  Structure: ES Modules offer named exports and a default export, providing more flexibility than CommonJS‚Äôs single object export.  Browser compatibility: ES Modules are natively supported in modern browsers, making them ideal for frontend development without bundling.How does the console.log() function work ?When console.log() is called, it invokes the Console object in the Node.js runtime, which then calls the process object. The Process object writes to the stdout stream, which is buffered and eventually flushed to the operating system‚Äôs I/O operations. Finally, the OS handles the actual output, displaying it in the terminal, writing to a file, or allowing it to be captured by another process.graph TD    subgraph javascriptLayer[\"üíª JavaScript Layer\"]        style javascriptLayer fill:#e6f7ff,stroke:#4db8ff        consoleLogCall[\"üì¢ console.log() call\"]    end    subgraph nodejsRuntime[\"‚öôÔ∏è Node.js Runtime\"]        style nodejsRuntime fill:#fff0f5,stroke:#ff69b4        consoleObject[\"üñ•Ô∏è Console Object\"]        processObject[\"üîÑ Process Object\"]        stdoutStream[\"üì§ stdout Stream\"]        streamBuffer[\"üíæ Stream Buffer\"]    end    subgraph operatingSystem[\"üñ•Ô∏è Operating System\"]        style operatingSystem fill:#f0fff0,stroke:#90ee90        osIO[\"üîå OS I/O Operations\"]    end    subgraph output[\"üì∫ Output\"]        style output fill:#fff5e6,stroke:#ffa500        terminalOutput[\"üíª Terminal Output\"]        fileOutput[\"üìÑ File Output\"]        processCapture[\"üîç Process Capture\"]    end    consoleLogCall --&gt;|\"Invokes\"| consoleObject    consoleObject --&gt;|\"Calls\"| processObject    processObject --&gt;|\"Writes to\"| stdoutStream    stdoutStream --&gt;|\"Buffers in\"| streamBuffer    streamBuffer --&gt;|\"Flushes to\"| osIO    osIO --&gt;|\"Displays in\"| terminalOutput    osIO --&gt;|\"Writes to\"| fileOutput    osIO --&gt;|\"Captured by\"| processCaptureHow does Winston enhance logging capabilities ?Winston enhances Node.js logging by offering multiple transports, allowing simultaneous logging to console, files, and databases. It provides customizable logging levels and formatting, enabling better categorization and presentation of log messages.const winston = require('winston');const logger = winston.createLogger({  level: 'info',  format: winston.format.json(),  transports: [    new winston.transports.File({ filename: 'error.log', level: 'error' }),    new winston.transports.File({ filename: 'combined.log' }),  ],});if (process.env.NODE_ENV !== 'production') {  logger.add(new winston.transports.Console({    format: winston.format.simple(),  }));}// Usagelogger.log({  level: 'info',  message: 'Hello distributed log files!'});logger.info('Hello again distributed logs');How to build a web server in Node.js?Node.js provides a built-in http module that allows you to create web servers. You can use the http.createServer method to create a server that listens for incoming requests and sends responses. You can define request handlers to process incoming requests and generate responses. Here‚Äôs an example of how to build a simple web server in Node.js:const http = require('http');const server = http.createServer((req, res) =&gt; {  res.writeHead(200, { 'Content-Type': 'text/plain' });  res.end('Hello, World!');});server.listen(3000, () =&gt; {  console.log('Server running on port 3000');});graph LRsubgraph CoreModules[\"üì¶ Core Modules\"]    http[\"üåê http\"]endHow can you implement graceful shutdown in a Node.js application?Graceful shutdown in a Node.js application can be implemented by listening for process termination signals like SIGINT or SIGTERM. When these signals are received, the application should stop accepting new requests, finish processing ongoing operations, close database connections, and release other resources. Finally, the process can exit cleanly, ensuring that all data is saved and resources are properly released before shutdown.const server = require('http').createServer();const db = require('./db'); // Assume this is your database connectionfunction gracefulShutdown() {  console.log('Starting graceful shutdown');    server.close(() =&gt; {    console.log('HTTP server closed');        db.close(() =&gt; {      console.log('Database connection closed');      process.exit(0);    });  });  // Force shutdown after 30 seconds  setTimeout(() =&gt; {    console.error('Could not close connections in time, forcefully shutting down');    process.exit(1);  }, 30000);}process.on('SIGTERM', gracefulShutdown);process.on('SIGINT', gracefulShutdown);// Your server logic hereserver.listen(3000);How are buffers implemented and why use them instead of strings?Buffers are contiguous memory blocks for raw binary data, typically fixed-size and mutable. Buffers allow for faster I/O operations, especially when dealing with large amounts of data or binary information. Strings often use more memory due to character encoding (e.g., UTF-8, UTF-16).import fs from 'fs/promises';import crypto from 'crypto';async function advancedBufferExamples() {  try {    // 1. Reading file into buffer    const fileBuffer: Buffer = await fs.readFile('example.txt');    console.log('File content:', fileBuffer.toString());    // Output: File content: Hello, this is an example file.    // 2. Creating buffer from string    const str = 'Hello, Buffer!';    const strBuffer: Buffer = Buffer.from(str, 'utf-8');    console.log('String as buffer:', strBuffer);    // Output: String as buffer: &lt;Buffer 48 65 6c 6c 6f 2c 20 42 75 66 66 65 72 21&gt;    console.log('Buffer back to string:', strBuffer.toString());    // Output: Buffer back to string: Hello, Buffer!    // 3. Working with binary data    const binaryBuffer = Buffer.from([0x48, 0x65, 0x6c, 0x6c, 0x6f]);    console.log('Binary buffer as string:', binaryBuffer.toString());    // Output: Binary buffer as string: Hello    // 4. Buffer concatenation    const buffer1 = Buffer.from('Hello ');    const buffer2 = Buffer.from('World');    const combinedBuffer = Buffer.concat([buffer1, buffer2]);    console.log('Combined buffer:', combinedBuffer.toString());    // Output: Combined buffer: Hello World    // 5. Buffer slicing    const slicedBuffer = combinedBuffer.slice(0, 5);    console.log('Sliced buffer:', slicedBuffer.toString());    // Output: Sliced buffer: Hello    // 6. Buffer comparison    console.log('Buffers equal:', Buffer.compare(buffer1, buffer2) === 0);    // Output: Buffers equal: false    // 7. Writing integer to buffer    const intBuffer = Buffer.alloc(4);    intBuffer.writeInt32BE(123456789);    console.log('Int32 buffer:', intBuffer);    // Output: Int32 buffer: &lt;Buffer 07 5b cd 15&gt;    // 8. Reading integer from buffer    const readInt = intBuffer.readInt32BE(0);    console.log('Read int32:', readInt);    // Output: Read int32: 123456789    // 9. Base64 encoding/decoding    const base64Str = strBuffer.toString('base64');    console.log('Base64 encoded:', base64Str);    // Output: Base64 encoded: SGVsbG8sIEJ1ZmZlciE=    const decodedBuffer = Buffer.from(base64Str, 'base64');    console.log('Decoded buffer:', decodedBuffer.toString());    // Output: Decoded buffer: Hello, Buffer!    // 10. Using buffer with crypto    const hash = crypto.createHash('sha256');    hash.update(strBuffer);    console.log('SHA256 hash:', hash.digest('hex'));    // Output: SHA256 hash: 3d3748055cb22d3bbd36f50cb9b84b74807feb52d8b45c57a7a09c4a7ba9d0a4  } catch (error) {    console.error('Error in buffer operations:', error);  }}advancedBufferExamples();Building terminal aplicationHow can you implement command-line arguments parsing ?you can use the built-in process.argv array or a third-party library like yargsUse process.argvconst args = process.argv.slice(2);    // Parse the arguments  const parsedArgs = {};  for (let i = 0; i &lt; args.length; i += 2) {      const key = args[i].replace(/^--/, '');      const value = args[i + 1];      parsedArgs[key] = value;  }    console.log('Parsed arguments:', parsedArgs);Using yargsconst yargs = require('yargs/yargs');const { hideBin } = require('yargs/helpers');const argv = yargs(hideBin(process.argv))  .option('name', {    alias: 'n',    type: 'string',    description: 'Your name'  })  .option('age', {    alias: 'a',    type: 'number',    description: 'Your age'  })  .argv;console.log('Hello,', argv.name);console.log('You are', argv.age, 'years old');run this scriptnode index.js --name John --age 30What is process.stdin ?process.stdin is a readable stream in Node.js that represents the standard input.It allows you to read input from the command line or other input sources.process.stdin is an instance of a Readable Stream, providing methods to handle input.It‚Äôs commonly used for creating interactive console applications or processing piped data.You can listen for ‚Äòdata‚Äô events or use methods like .read() to access incoming data.process.stdin is particularly useful for creating interactive command-line interfaces (CLIs).const readline = require('readline');// Create an interface for reading from stdin and writing to stdoutconst rl = readline.createInterface({  input: process.stdin,  output: process.stdout});console.log('Welcome to the Simple Calculator!');console.log('Enter an expression (e.g., 2 + 3) or type \"exit\" to quit.');// Function to evaluate a simple mathematical expressionfunction evaluateExpression(expr) {  try {    return eval(expr);  } catch (error) {    return 'Invalid expression';  }}// Use readline to continuously prompt for inputfunction promptUser() {  rl.question('&gt; ', (input) =&gt; {    if (input.toLowerCase() === 'exit') {      console.log('Goodbye!');      rl.close();      return;    }    const result = evaluateExpression(input);    console.log(`Result: ${result}`);    promptUser(); // Prompt again for the next input  });}// Start the prompt looppromptUser();// Handle the close eventrl.on('close', () =&gt; {  process.exit(0);});graph     start[\"üöÄ Start\"]    prompt[\"‚ùì Prompt User\"]    process[\"‚öôÔ∏è Process Input\"]    evaluate[\"üßÆ Evaluate\"]    display[\"üìä Display Result\"]    checkExit[\"üö™ Exit?\"]    end1[\"üëã End\"]    start --&gt; prompt    prompt --&gt; process    process --&gt; evaluate    evaluate --&gt; display    display --&gt; checkExit    checkExit --&gt;|\"No\"| prompt    checkExit --&gt;|\"Yes\"| end1    classDef lightBlue fill:#e6f7ff,stroke:#1890ff;    classDef lightGreen fill:#f6ffed,stroke:#52c41a;    classDef lightRed fill:#fff1f0,stroke:#ff4d4f;    class start,end1 lightGreen;    class prompt,process,evaluate,display lightBlue;    class checkExit lightRed;How can you create progress bars to show long-running tasks in a terminal application?Example to show progress bard when download the Bitcoin whitepaper with cli-progressconst axios = require('axios');const fs = require('fs');const cliProgress = require('cli-progress');async function downloadFile(fileUrl, outputPath) {  const writer = fs.createWriteStream(outputPath);    const response = await axios({    method: 'get',    url: fileUrl,    responseType: 'stream'  });  const totalLength = response.headers['content-length'];  const progressBar = new cliProgress.SingleBar({    format: 'Downloading [{bar}] {percentage}% | ETA: {eta}s | {value}/{total} bytes',    barCompleteChar: '\\u2588',    barIncompleteChar: '\\u2591',    hideCursor: true  });  progressBar.start(totalLength, 0);  response.data.on('data', (chunk) =&gt; {    writer.write(chunk);    progressBar.increment(chunk.length);  });  response.data.on('end', () =&gt; {    writer.end();    progressBar.stop();    console.log('Download completed');  });  writer.on('finish', () =&gt; {    console.log('File saved as:', outputPath);  });  writer.on('error', (err) =&gt; {    console.error('Error writing file:', err);  });}// Bitcoin whitepaper URLconst fileUrl = 'https://bitcoin.org/bitcoin.pdf';const outputPath = './bitcoin_whitepaper.pdf';console.log('Starting download of Bitcoin whitepaper...');downloadFile(fileUrl, outputPath).catch(console.error);When you run this script, you‚Äôll see a progress barStarting download of Bitcoin whitepaper...Downloading [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% | ETA: 0s | 184292/184292 bytesDownload completedFile saved as: ./bitcoin_whitepaper.pdfWorking with filesHow can you read from and write to files ?Node.js provides file I/O operations through the built-in fs module. To read a file, you can use fs.readFile() for asynchronous reading or fs.readFileSync() for synchronous reading. For writing, use fs.writeFile() (asynchronous) or fs.writeFileSync() (synchronous). These methods allow you to interact with the file system, handling text and binary data. For larger files or streams of data, Node.js also offers more efficient streaming operations using fs.createReadStream() and fs.createWriteStream().const fs = require('fs');// Asynchronous readfs.readFile('input.txt', 'utf8', (err, data) =&gt; {  if (err) {    console.error('Error reading file:', err);    return;  }  console.log('Asynchronous read:', data);});// Synchronous readtry {  const data = fs.readFileSync('input.txt', 'utf8');  console.log('Synchronous read:', data);} catch (err) {  console.error('Error reading file:', err);}// Asynchronous writefs.writeFile('output.txt', 'Hello, Node.js!', (err) =&gt; {  if (err) {    console.error('Error writing file:', err);    return;  }  console.log('File written successfully');});// Synchronous writetry {  fs.writeFileSync('output-sync.txt', 'Hello, Node.js! (Sync)');  console.log('File written successfully (Sync)');} catch (err) {  console.error('Error writing file:', err);}// Reading and writing streams for larger filesconst readStream = fs.createReadStream('large-input.txt');const writeStream = fs.createWriteStream('large-output.txt');readStream.pipe(writeStream);readStream.on('end', () =&gt; {  console.log('Finished copying large file');});How can you monitor a file for changes ?You can monitor a file for changes in Node.js using the fs.watch() method, which watches for changes to a file or directory. You can specify the type of changes to watch for (e.g., ‚Äòchange‚Äô, ‚Äòrename‚Äô) and handle events accordingly.const fs = require('fs');function watchFile(filePath) {  let fileSize = fs.statSync(filePath).size;  fs.watch(filePath, (eventType) =&gt; {    if (eventType === 'change') {      const newSize = fs.statSync(filePath).size;      if (newSize &gt; fileSize) {        const stream = fs.createReadStream(filePath, {           start: fileSize,          encoding: 'utf8'        });        stream.on('data', (chunk) =&gt; {          console.log('New content:', chunk);        });        stream.on('end', () =&gt; {          fileSize = newSize;          console.log('Finished reading new content');        });      }    }  });  console.log(`Watching for changes on ${filePath}`);}// UsagewatchFile('./path/to/your/file.txt');How to determine the current directory and file location ?Node.js provides two global variables, __dirname and __filename, that allow you to access the current directory and file path, respectively. __dirname returns the absolute path of the current directory, while __filename returns the absolute path of the current file. You can use these variables to reference files or directories relative to the current location.Let‚Äôs assume we have the following project structure:my-node-project/‚îÇ‚îú‚îÄ‚îÄ src/‚îÇ   ‚îú‚îÄ‚îÄ index.js‚îÇ   ‚îî‚îÄ‚îÄ utils/‚îÇ       ‚îî‚îÄ‚îÄ config-reader.js‚îÇ‚îî‚îÄ‚îÄ config/    ‚îî‚îÄ‚îÄ app-config.jsonWe‚Äôre working in the config-reader.js file and want to access the app-config.json. Use path.join() to construct the path to our config file// File: src/utils/config-reader.jsconst path = require('path');const fs = require('fs');console.log('Current directory:', __dirname);// Output: Current directory: /path/to/my-node-project/src/utilsconsole.log('Current file:', __filename);// Output: Current file: /path/to/my-node-project/src/utils/config-reader.js// Construct the path to the config fileconst configFilePath = path.join(__dirname, '..', '..', 'config', 'app-config.json');console.log('Path to config file:', configFilePath);// Output: Path to config file: /path/to/my-node-project/config/app-config.json"
  },
  
  {
    "title": "JavaScript Array In 1 Page",
    "url": "/posts/javascript-array-in-1-page/",
    "categories": "javascript",
    "tags": "javascript",
    "date": "2024-09-17 07:00:00 +0700",
    





    
    "snippet": "This Article provides a basic guide to work with arrays in JavaScript, covering essential methods and techniques in a single page for quick reference.This guide breaks down each major method, from ...",
    "content": "This Article provides a basic guide to work with arrays in JavaScript, covering essential methods and techniques in a single page for quick reference.This guide breaks down each major method, from basic operations like push, pop, shift, and unshift to more complex functions such as map, filter, reduce, and flatMapI use Excalidraw, you can download it here.  JS Array In 1 Page (Excalidraw) - Google Drive"
  },
  
  {
    "title": "Atomic Isolation",
    "url": "/posts/Atomic-Isolation/",
    "categories": "",
    "tags": "",
    "date": "2024-08-12 00:00:00 +0700",
    





    
    "snippet": "Atomic Operation:  Single Document  Multiple Documents in the Same Collection  Multiple Documents in Different CollectionsInventory Managementasync function updateInventory(productId, quantity) {  ...",
    "content": "Atomic Operation:  Single Document  Multiple Documents in the Same Collection  Multiple Documents in Different CollectionsInventory Managementasync function updateInventory(productId, quantity) {  const session = await mongoose.startSession();  session.startTransaction();    try {    const product = await Product.findOne({ _id: productId }).session(session);    if (product.stock &lt; quantity) {      throw new Error('Insufficient stock');    }        await Product.updateOne(      { _id: productId },      { $inc: { stock: -quantity } }    ).session(session);        await Order.create([{ productId, quantity }], { session });        await session.commitTransaction();    console.log('Inventory updated and order created');  } catch (error) {    await session.abortTransaction();    console.error('Transaction aborted:', error);  } finally {    session.endSession();  }}Voting System  Atomically increment vote count and update voter‚Äôs status.const mongoose = require('mongoose');// Assume Candidate and Voter models are already definedasync function castVote(voterId, candidateId) {  const session = await mongoose.startSession();  session.startTransaction();  try {    // Check if the voter has already voted    const voter = await Voter.findOne({ userId: voterId }).session(session);    if (!voter || voter.hasVoted) {      throw new Error('Voter not eligible');    }    // Increment the candidate's vote count and update voter status atomically    const [candidateResult, voterResult] = await Promise.all([      Candidate.updateOne(        { _id: candidateId },        { $inc: { voteCount: 1 } }      ).session(session),            Voter.updateOne(        { userId: voterId },        { $set: { hasVoted: true, votedFor: candidateId } }      ).session(session)    ]);    if (candidateResult.modifiedCount !== 1 || voterResult.modifiedCount !== 1) {      throw new Error('Failed to update candidate or voter');    }    await session.commitTransaction();    return true;  } catch (error) {    await session.abortTransaction();    console.error('Voting failed:', error.message);    return false;  } finally {    session.endSession();  }}Auction System  Atomically update highest bid and notify previous highest bidder.const mongoose = require('mongoose');// Assume we have an Auction model defined// const Auction = mongoose.model('Auction', AuctionSchema);async function placeBid(auctionId, bidderId, bidAmount) {  const session = await mongoose.startSession();  session.startTransaction();  try {    // Find the auction and lock it for update    const auction = await Auction.findById(auctionId).session(session);        if (!auction || auction.endTime &lt; new Date()) {      throw new Error('Auction not found or has ended');    }    if (bidAmount &lt;= auction.currentHighestBid) {      throw new Error('Bid amount must be higher than current highest bid');    }    const previousHighestBidder = auction.currentHighestBidder;        // Update the auction with new highest bid    const updatedAuction = await Auction.findOneAndUpdate(      { _id: auctionId, currentHighestBid: { $lt: bidAmount } },      {         $set: {          currentHighestBid: bidAmount,          currentHighestBidder: bidderId        }      },      { new: true, session }    );    if (!updatedAuction) {      throw new Error('Bid update failed - someone may have outbid you');    }    // Prepare notification data (actual notification sending would be handled separately)    const notificationData = previousHighestBidder ? {      bidderId: previousHighestBidder,      message: `You've been outbid on auction ${auctionId}`    } : null;    await session.commitTransaction();        return {       success: true,       newHighestBid: bidAmount,      notificationData     };  } catch (error) {    await session.abortTransaction();    console.error('Bidding failed:', error.message);    return { success: false, error: error.message };  } finally {    session.endSession();  }}  Email Send and Update          Atomically send an email and update the email status in the database.        Game State Update          Atomically update player‚Äôs score, level, and inventory in a game.      Isolation Use Cases:Seat Reservation Systemasync function reserveSeat(flightId, seatNumber, userId) {  const session = await mongoose.startSession();  session.startTransaction();    try {    const seat = await Seat.findOne({       flightId,       seatNumber,       isReserved: false     }).session(session);        if (!seat) {      throw new Error('Seat not available');    }        await Seat.updateOne(      { _id: seat._id },      { $set: { isReserved: true, reservedBy: userId } }    ).session(session);        await Reservation.create([{       userId,       flightId,       seatNumber     }], { session });        await session.commitTransaction();    console.log('Seat reserved successfully');    return true;  } catch (error) {    await session.abortTransaction();    console.error('Reservation failed:', error);    return false;  } finally {    session.endSession();  }}  Stock Trading          Ensure isolation when multiple users are trying to buy/sell the same stock simultaneously.      Course Registration SystemThis system needs to ensure that students can‚Äôt register for courses with conflicting schedules or courses that are already at full capacity, even in a high-concurrency environment where many students might be registering simultaneously.const mongoose = require('mongoose');// Assume we have Course and Student models defined// const Course = mongoose.model('Course', CourseSchema);// const Student = mongoose.model('Student', StudentSchema);async function registerForCourse(studentId, courseId) {  const session = await mongoose.startSession();  session.startTransaction();  try {    // Lock the student and course documents for update    const student = await Student.findById(studentId).session(session);    const course = await Course.findById(courseId).session(session);    if (!student || !course) {      throw new Error('Student or course not found');    }    // Check course capacity    if (course.enrolledStudents.length &gt;= course.capacity) {      throw new Error('Course is already at full capacity');    }    // Check for schedule conflicts    const hasConflict = student.enrolledCourses.some(enrolledCourseId =&gt; {      const enrolledCourse = Course.findById(enrolledCourseId);      return doSchedulesConflict(course.schedule, enrolledCourse.schedule);    });    if (hasConflict) {      throw new Error('Schedule conflict detected');    }    // Update course and student    course.enrolledStudents.push(studentId);    student.enrolledCourses.push(courseId);    // Save changes    await Promise.all([      course.save({ session }),      student.save({ session })    ]);    await session.commitTransaction();    return { success: true, message: 'Registration successful' };  } catch (error) {    await session.abortTransaction();    console.error('Registration failed:', error.message);    return { success: false, error: error.message };  } finally {    session.endSession();  }}function doSchedulesConflict(schedule1, schedule2) {  // Simplified conflict check. In a real system, this would be more complex.  return schedule1.some(slot1 =&gt;     schedule2.some(slot2 =&gt;       slot1.day === slot2.day &amp;&amp;       ((slot1.startTime &lt;= slot2.startTime &amp;&amp; slot2.startTime &lt; slot1.endTime) ||       (slot2.startTime &lt;= slot1.startTime &amp;&amp; slot1.startTime &lt; slot2.endTime))    )  );}// Example usageasync function simulateConcurrentRegistrations() {  const registrations = [    registerForCourse('student1', 'course1'),    registerForCourse('student2', 'course1'),    registerForCourse('student1', 'course2')  ];  const results = await Promise.all(registrations);  console.log('Registration results:', results);}simulateConcurrentRegistrations().catch(console.error);Key isolation aspects:  Transaction use ensures atomic updates to both course and student documents.  Locking documents within the session prevents concurrent modifications.  Capacity and schedule conflict checks are performed within the transaction, ensuring consistency even with concurrent registrations.    Unique Order Number Generation  This system needs to generate unique, sequential order numbers in a high-concurrency environment, ensuring that no two orders receive the same number.const mongoose = require('mongoose');// Define a Counter model for keeping track of the latest order numberconst CounterSchema = new mongoose.Schema({  _id: String,  seq: Number});const Counter = mongoose.model('Counter', CounterSchema);async function generateUniqueOrderNumber() {  const session = await mongoose.startSession();  session.startTransaction();  try {    // Find and update the counter atomically    const counter = await Counter.findOneAndUpdate(      { _id: 'orderNumber' },      { $inc: { seq: 1 } },      { new: true, upsert: true, session }    );    // Generate the order number (e.g., ORD0001)    const orderNumber = `ORD${counter.seq.toString().padStart(4, '0')}`;    // Here you would typically create the actual order document    // For this example, we'll just simulate it    console.log(`Created order with number: ${orderNumber}`);    await session.commitTransaction();    return { success: true, orderNumber };  } catch (error) {    await session.abortTransaction();    console.error('Order number generation failed:', error.message);    return { success: false, error: error.message };  } finally {    session.endSession();  }}// Example usageasync function simulateConcurrentOrderCreation() {  const orderCreations = Array(10).fill().map(() =&gt; generateUniqueOrderNumber());  const results = await Promise.all(orderCreations);  console.log('Order creation results:', results);}simulateConcurrentOrderCreation().catch(console.error);Key isolation aspects:  Use of findOneAndUpdate with session ensures atomic increment of the sequence number.  The transaction ensures that the entire process of generating and using the order number is isolated.  Even with concurrent requests, each will get a unique, sequential number due to the atomic nature of the update operation.Both these examples demonstrate important aspects of isolation in database operations:  For the Course Registration System, isolation ensures that course capacity and student schedules remain consistent even when multiple students are registering simultaneously.  For the Order Number Generation, isolation guarantees that each order receives a unique, sequential number without gaps or duplicates, even in high-concurrency situations.These implementations use MongoDB‚Äôs transactions and atomic operations to maintain data integrity and consistency in multi-user environments where race conditions could otherwise lead to data inconsistencies or conflicts.Combining Atomicity and Isolation:  Distributed Task Queueasync function dequeueAndProcessTask() {  const session = await mongoose.startSession();  session.startTransaction();    try {    // Atomically find and update the next available task    const task = await Task.findOneAndUpdate(      { status: 'pending' },      { $set: { status: 'processing', startedAt: new Date() } },      { sort: { priority: -1, createdAt: 1 }, new: true, session }    );        if (!task) {      await session.commitTransaction();      return null; // No tasks available    }        // Process the task (simulated)    const result = await simulateTaskProcessing(task);        // Update task status based on processing result    await Task.updateOne(      { _id: task._id },      {         $set: {           status: result.success ? 'completed' : 'failed',          result: result.data,          completedAt: new Date()        }      }    ).session(session);        await session.commitTransaction();    console.log('Task processed:', task._id);    return task;  } catch (error) {    await session.abortTransaction();    console.error('Task processing failed:', error);    return null;  } finally {    session.endSession();  }}async function simulateTaskProcessing(task) {  // Simulate task processing  await new Promise(resolve =&gt; setTimeout(resolve, 1000));  return { success: Math.random() &gt; 0.1, data: 'Processed result' };}  Real-time Collaborative Editing          Ensure consistency when multiple users are editing the same document simultaneously.        Distributed Locking System          Implement a distributed lock for coordinating access to shared resources across multiple servers.        Social Media Post Creation with Mentions          Atomically create a post, update mentioned users‚Äô notification counters, and add to relevant feeds.        E-commerce Checkout Process          Handle inventory checks, payment processing, and order creation in a single atomic operation while ensuring isolation from other concurrent checkouts.      "
  },
  
  {
    "title": "Nestjs",
    "url": "/posts/nestjs/",
    "categories": "",
    "tags": "",
    "date": "2024-08-11 00:00:00 +0700",
    





    
    "snippet": "Certainly! I‚Äôll provide you with more use cases for both atomic operations and isolation in database transactions. These examples will cover various scenarios beyond the balance transfer and unique...",
    "content": "Certainly! I‚Äôll provide you with more use cases for both atomic operations and isolation in database transactions. These examples will cover various scenarios beyond the balance transfer and unique username creation we‚Äôve already discussed.Atomic Operation Use Cases:  Inventory Managementasync function updateInventory(productId, quantity) {  const session = await mongoose.startSession();  session.startTransaction();    try {    const product = await Product.findOne({ _id: productId }).session(session);    if (product.stock &lt; quantity) {      throw new Error('Insufficient stock');    }        await Product.updateOne(      { _id: productId },      { $inc: { stock: -quantity } }    ).session(session);        await Order.create([{ productId, quantity }], { session });        await session.commitTransaction();    console.log('Inventory updated and order created');  } catch (error) {    await session.abortTransaction();    console.error('Transaction aborted:', error);  } finally {    session.endSession();  }}  Voting System          Atomically increment vote count and update voter‚Äôs status.        Auction System          Atomically update highest bid and notify previous highest bidder.        Email Send and Update          Atomically send an email and update the email status in the database.        Game State Update          Atomically update player‚Äôs score, level, and inventory in a game.      Isolation Use Cases:  Seat Reservation Systemasync function reserveSeat(flightId, seatNumber, userId) {  const session = await mongoose.startSession();  session.startTransaction();    try {    const seat = await Seat.findOne({       flightId,       seatNumber,       isReserved: false     }).session(session);        if (!seat) {      throw new Error('Seat not available');    }        await Seat.updateOne(      { _id: seat._id },      { $set: { isReserved: true, reservedBy: userId } }    ).session(session);        await Reservation.create([{       userId,       flightId,       seatNumber     }], { session });        await session.commitTransaction();    console.log('Seat reserved successfully');    return true;  } catch (error) {    await session.abortTransaction();    console.error('Reservation failed:', error);    return false;  } finally {    session.endSession();  }}  Stock Trading          Ensure isolation when multiple users are trying to buy/sell the same stock simultaneously.        Hotel Room Booking          Similar to seat reservation, but for hotel rooms.        Course Registration System          Ensure students can‚Äôt register for courses with conflicting schedules or full capacity.        Unique Order Number Generation          Generate unique, sequential order numbers in a high-concurrency environment.      More Complex Use Cases Combining Atomicity and Isolation:  Distributed Task Queueasync function dequeueAndProcessTask() {  const session = await mongoose.startSession();  session.startTransaction();    try {    // Atomically find and update the next available task    const task = await Task.findOneAndUpdate(      { status: 'pending' },      { $set: { status: 'processing', startedAt: new Date() } },      { sort: { priority: -1, createdAt: 1 }, new: true, session }    );        if (!task) {      await session.commitTransaction();      return null; // No tasks available    }        // Process the task (simulated)    const result = await simulateTaskProcessing(task);        // Update task status based on processing result    await Task.updateOne(      { _id: task._id },      {         $set: {           status: result.success ? 'completed' : 'failed',          result: result.data,          completedAt: new Date()        }      }    ).session(session);        await session.commitTransaction();    console.log('Task processed:', task._id);    return task;  } catch (error) {    await session.abortTransaction();    console.error('Task processing failed:', error);    return null;  } finally {    session.endSession();  }}async function simulateTaskProcessing(task) {  // Simulate task processing  await new Promise(resolve =&gt; setTimeout(resolve, 1000));  return { success: Math.random() &gt; 0.1, data: 'Processed result' };}  Real-time Collaborative Editing          Ensure consistency when multiple users are editing the same document simultaneously.        Distributed Locking System          Implement a distributed lock for coordinating access to shared resources across multiple servers.        Social Media Post Creation with Mentions          Atomically create a post, update mentioned users‚Äô notification counters, and add to relevant feeds.        E-commerce Checkout Process          Handle inventory checks, payment processing, and order creation in a single atomic operation while ensuring isolation from other concurrent checkouts.      These use cases demonstrate the importance of atomic operations and isolation in various real-world scenarios. They highlight how these database transaction properties are crucial for maintaining data consistency and integrity, especially in high-concurrency environments or when dealing with critical business operations.What is Pipe ?Pipe NestJS is a feature in the NestJS framework that allows you to transform or validate data before it reaches the route handler. It acts as a middleware that processes incoming data, enabling tasks like data conversion, sanitization, or validation. Pipes can be used to ensure data meets certain criteria before it‚Äôs processed by the application logic.How do you use pipes ?To use pipes for validation in NestJS, you can create a custom pipe class implementing PipeTransform, or use built-in pipes like ValidationPipe. Apply the pipe at the parameter, method, or controller level using the @UsePipes() decorator. The pipe will then validate incoming data against defined rules before it reaches the route handler  // src/users/pipes/at-least-one-field.pipe.tsimport { PipeTransform, Injectable, ArgumentMetadata, BadRequestException } from '@nestjs/common';import { CreateUserDto } from '../dto/create-user.dto';@Injectable()export class AtLeastOneFieldPipe implements PipeTransform {  transform(value: CreateUserDto, metadata: ArgumentMetadata) {    if (!(value.email || value.ssn || value.passportNumber)) {      throw new BadRequestException('At least one of email, ssn, or passportNumber must be provided');    }    return value;  }}What is BadRequestException ?BadRequestException is a built-in NestJS exception class that generates HTTP 400 responses for client-side errors. It‚Äôs commonly thrown when incoming data fails validation or doesn‚Äôt meet required parameters. In code, it‚Äôs used like this: throw new BadRequestException('Validation failed'); to inform clients of invalid requests.Great question. NestJS provides several built-in exceptions for different scenarios. Here‚Äôs a summary of some common ones:what other exception types are used ?  NotFoundException (404): Used when a requested resource is not found.  UnauthorizedException (401): Thrown when a user isn‚Äôt authenticated for an operation.  ForbiddenException (403): Used when a user is authenticated but lacks necessary permissions.  ConflictException (409): Indicates a conflict with the current state of a resource.  InternalServerErrorException (500): For unexpected server errors not handled by other exceptions.Each of these maps to a specific HTTP status code and is used to communicate different types of errors to the client."
  },
  
  {
    "title": " Redis Fundamentals",
    "url": "/posts/redis-fundamentals/",
    "categories": "mern, redis",
    "tags": "redis, database",
    "date": "2024-08-06 07:00:00 +0700",
    





    
    "snippet": "FundamentalsRedis (Remote Dictionary Server) is an open-source, in-memory data structure store that serves as a database, cache, and message broker. It‚Äôs known for its blazing-fast performance and ...",
    "content": "FundamentalsRedis (Remote Dictionary Server) is an open-source, in-memory data structure store that serves as a database, cache, and message broker. It‚Äôs known for its blazing-fast performance and versatility, making it a popular choice for various applications.graph LRsubgraph redisFundamentals[\"üóÑÔ∏è Redis Fundamentals\"]    dataStructures[\"üìä Data Structures\"]    persistence[\"üíæ Persistence\"]    transactions[\"üîÑ Transactions\"]    pubSub[\"üì° Pub/Sub\"]endstyle redisFundamentals fill:#e6f3ff,stroke:#333,stroke-width:2pxData StructuresRedis supports several data structures, each optimized for specific use cases:graph LRsubgraph dataStructures[\"üìä Data Structures\"]    strings[\"üî§ Strings\"]    lists[\"üìú Lists\"]    sets[\"üé≠ Sets\"]    sortedSets[\"üèÜ Sorted Sets\"]    hashes[\"üóùÔ∏è Hashes\"]endstrings --&gt; |\"store\"| simpleValues[\"Simple Values\"]lists --&gt; |\"maintain\"| orderedElements[\"Ordered Elements\"]sets --&gt; |\"hold\"| uniqueItems[\"Unique Items\"]sortedSets --&gt; |\"combine\"| uniqueScored[\"Unique Scored Items\"]hashes --&gt; |\"represent\"| fieldValuePairs[\"Field-Value Pairs\"]style dataStructures fill:#fff0f5,stroke:#333,stroke-width:2pxBefore using ioredis, initialize the client:const Redis = require('ioredis');const redis = new Redis();  Strings: Basic key-value pairs, ideal for caching simple values.    # Redis CLISET user:1:name \"John Doe\"GET user:1:name# ioredisawait redis.set('user:1:name', 'John Doe');const name = await redis.get('user:1:name');        Lists: Ordered collections of strings, perfect for queues or recent updates.    # Redis CLILPUSH latest_news \"Breaking: New Discovery\"LRANGE latest_news 0 -1# ioredisawait redis.lpush('latest_news', 'Breaking: New Discovery');const news = await redis.lrange('latest_news', 0, -1);        Sets: Unordered collections of unique strings, useful for tagging or unique visitor tracking.    # Redis CLISADD user:1:skills \"python\" \"javascript\" \"redis\"SMEMBERS user:1:skills# ioredisawait redis.sadd('user:1:skills', 'python', 'javascript', 'redis');const skills = await redis.smembers('user:1:skills');        Sorted Sets: Sets with a score for each member, great for leaderboards or priority queues.    # Redis CLIZADD leaderboard 100 \"player1\" 95 \"player2\" 120 \"player3\"ZRANGE leaderboard 0 -1 WITHSCORES# ioredisawait redis.zadd('leaderboard', 100, 'player1', 95, 'player2', 120, 'player3');const leaderboard = await redis.zrange('leaderboard', 0, -1, 'WITHSCORES');        Hashes: Maps between string fields and string values, representing objects.    # Redis CLIHSET user:1 name \"John Doe\" age \"30\" city \"New York\"HGETALL user:1# ioredisawait redis.hset('user:1', 'name', 'John Doe', 'age', '30', 'city', 'New York');const user = await redis.hgetall('user:1');      graph LR    subgraph redisCommands[\"üìÅ Redis Commands by Data Structure\"]        subgraph strings[\"üî§ Strings\"]            setCommand[\"fa:fa-file-code SET\"]            getCommand[\"fa:fa-search GET\"]        end                subgraph lists[\"üìú Lists\"]            lpushCommand[\"fa:fa-plus-circle LPUSH\"]            rpushCommand[\"fa:fa-plus-circle RPUSH\"]            lpopCommand[\"fa:fa-minus-circle LPOP\"]            rpopCommand[\"fa:fa-minus-circle RPOP\"]        end                subgraph sets[\"üé≠ Sets\"]            saddCommand[\"fa:fa-plus SADD\"]            sremCommand[\"fa:fa-minus SREM\"]            smembersCommand[\"fa:fa-list SMEMBERS\"]            sismemberCommand[\"fa:fa-question SISMEMBER\"]        end                subgraph sortedSets[\"üèÜ Sorted Sets\"]            zaddCommand[\"fa:fa-plus ZADD\"]            zremCommand[\"fa:fa-minus ZREM\"]            zrangeCommand[\"fa:fa-sort-amount-asc ZRANGE\"]            zrankCommand[\"fa:fa-signal ZRANK\"]        end                subgraph hashes[\"üóùÔ∏è Hashes\"]            hsetCommand[\"fa:fa-file-code HSET\"]            hgetCommand[\"fa:fa-search HGET\"]        end                subgraph general[\"üîß General\"]            delCommand[\"fa:fa-trash DEL\"]        end    end    style redisCommands fill:#e0f7fa,stroke:#0288d1,stroke-width:2px    style strings fill:#e8f5e9,stroke:#4caf50,stroke-width:2px    style lists fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px    style sets fill:#ffe0b2,stroke:#ff9800,stroke-width:2px    style sortedSets fill:#fff3e0,stroke:#ff9800,stroke-width:2px    style hashes fill:#f1f8e9,stroke:#8bc34a,stroke-width:2px    style general fill:#ffebee,stroke:#e91e63,stroke-width:2pxTransactionsRedis Transactions allow executing a group of commands atomically. They involve commands like MULTI, EXEC, DISCARD, and WATCH. Transactions guarantee that commands are executed in sequence, ensuring isolation from other client requests. If EXEC is called, all queued commands are executed. If not, none are executed.Transactions also support optimistic locking using WATCH, which monitors keys for changes. If any watched key is modified before EXEC, the transaction aborts.graph LR    subgraph transactionFlow[\"Redis Transaction Flow\"]        watch[\"WATCH key(s)\"] --&gt;|\"Optional\"| startTransaction        startTransaction[\"MULTI\"] --&gt; queueOperations        subgraph queueOperations[\"Queue Operations\"]            operation1[\"Operation 1\"] --&gt; operation2[\"Operation 2\"]            operation2 --&gt; operation3[\"... Operation N\"]        end        queueOperations --&gt;|\"Execute\"| executeTransaction[\"EXEC\"]        queueOperations --&gt;|\"Cancel\"| discardTransaction[\"DISCARD\"]        executeTransaction --&gt;|\"Success\"| logSuccess[\"Transaction Complete\"]        executeTransaction --&gt;|\"Error\"| logError[\"Error Logged\"]        executeTransaction --&gt;|\"WATCH triggered\"| watchTriggered[\"Transaction Aborted\"]    end    classDef default stroke:#333,stroke-width:2px;    classDef watch fill:#e1bee7,stroke:#8e24aa,stroke-width:2px;    classDef multi fill:#bbdefb,stroke:#1976d2,stroke-width:2px;    classDef queue fill:#c8e6c9,stroke:#388e3c,stroke-width:2px;    classDef exec fill:#fff9c4,stroke:#fbc02d,stroke-width:2px;    classDef discard fill:#ffcdd2,stroke:#d32f2f,stroke-width:2px;    classDef success fill:#dcedc8,stroke:#689f38,stroke-width:2px;    classDef failure fill:#ffcdd2,stroke:#d32f2f,stroke-width:2px;    class watch watch;    class startTransaction multi;    class operation1,operation2,operation3 queue;    class executeTransaction exec;    class discardTransaction discard;    class logSuccess success;    class logError,watchTriggered failure;// Redis CLIWATCH mykeyval = GET mykey// ... perform some check or calculation with val ...MULTISET mykey newval// ... other commands ...EXEC// ioredisconst Redis = require('ioredis');const redis = new Redis();async function performTransactionWithOptimisticLock(key) {  try {    await redis.watch(key);        // Get the current value and perform some operation    const currentVal = await redis.get(key);    const newVal = someOperation(currentVal);    // Perform the transaction    const result = await redis      .multi()      .set(key, newVal)      // ... other commands ...      .exec();        if (result === null) {      console.log('Transaction aborted due to concurrent modification');      return false;    } else {      console.log('Transaction successful:', result);      return true;    }  } catch (error) {    console.error('Transaction error:', error);    return false;  }}function someOperation(value) {  // Perform some operation on the value  return value; // Placeholder for actual operation}// UsageperformTransactionWithOptimisticLock('mykey')  .then(success =&gt; console.log('Transaction success:', success))  .catch(error =&gt; console.error('Error:', error))  .finally(() =&gt; redis.quit());Redis transactions don‚Äôt support partial execution or rollbacks. In the scenario of 10 operations, it‚Äôs not possible for only 4 to succeed. Either all operations in the transaction are executed, or none are. This is a key feature of Redis transactions - they are atomicIf a watched key is changed between MULTI and EXEC, no error is thrown. Instead, the transaction is quietly aborted, and EXEC returns a null result (in ioredis) or an empty array (in Redis CLI). It‚Äôs up to your code to check for this condition and handle it appropriately.Caching LayerRedis is an excellent choice for implementing a caching layer in Node.js applications due to its speed and ease of use. By caching frequently accessed data in Redis, you can significantly reduce the load on your database and improve response times for your users.const express = require('express');const Redis = require('ioredis');const app = express();// Initialize Redis clientconst redis = new Redis();// Middleware to check cacheasync function cache(req, res, next) {  const { id } = req.params;    try {    // Check if the data is in Redis    const cachedData = await redis.get(`user:${id}`);        if (cachedData) {      // If found, serve from cache      return res.json(JSON.parse(cachedData));    }        // If not found, proceed to the next middleware    next();  } catch (err) {    console.error('Error checking cache:', err);    next();  }}// Route to get user dataapp.get('/user/:id', cache, async (req, res) =&gt; {  const { id } = req.params;    try {    // Simulate a database query    const user = await getUserFromDatabase(id);        // Cache the result in Redis    await redis.set(`user:${id}`, JSON.stringify(user), 'EX', 3600); // Cache for 1 hour        res.json(user);  } catch (err) {    console.error('Error fetching user:', err);    res.status(500).send('Server error');  }});// Simulated database query functionasync function getUserFromDatabase(id) {  // Placeholder for actual database query}// Start the serverapp.listen(3000, () =&gt; {  console.log('Server running on port 3000');});Key Concepts  Cache Key: The key used to store and retrieve data from Redis. In the example above, user:${id} is used as the key.  Expiration (TTL): The time-to-live (TTL) for cached data, set using the EX option in Redis. This ensures that stale data is automatically removed after a certain period.  Cache Miss: When data is not found in the cache, requiring a query to the database.  Cache Hit: When data is found in the cache, allowing the request to be served without querying the database.Caching Strategies  Lazy Caching (Cache-aside): The application checks the cache first, and if the data isn‚Äôt found, it queries the database and caches the result. This is the approach demonstrated above.  Write-through Caching: Data is written to the cache at the same time it is written to the database, ensuring that the cache is always up-to-date.  Read-through Caching: The application always queries the cache first. If the data is not found, the cache itself fetches the data from the database and updates the cache.Lazy Caching (Cache-aside)flowchart TD    subgraph lazyCaching[\"üê¢ Lazy Caching (Cache-aside)\"]        style lazyCaching fill:#e6f7ff,stroke:#69c0ff                app[\"üì± Application\"]        cache[\"üíæ Redis Cache\"]        db[\"üóÑÔ∏è Database\"]                app --&gt; |\"1. Check cache\"| cache        cache --&gt; |\"2. Cache miss\"| app        app --&gt; |\"3. Query database\"| db        db --&gt; |\"4. Return data\"| app        app --&gt; |\"5. Update cache\"| cache    endLazy caching checks the cache first and only queries the database on a cache miss.app.get('/user/:id', async (req, res) =&gt; {  const { id } = req.params;  const cacheKey = `user:${id}`;  try {    let userData = await redis.get(cacheKey);    if (!userData) {      const result = await pool.query('SELECT * FROM users WHERE id = $1', [id]);      userData = result.rows[0];      if (userData) {        await redis.setex(cacheKey, 3600, JSON.stringify(userData));      }    } else {      userData = JSON.parse(userData);    }    res.json(userData || { error: 'User not found' });  } catch (error) {    console.error('Error:', error);    res.status(500).json({ error: 'Internal server error' });  }});Pros:  Reduced database load  Faster response times for cached data  Only caches data that is actually requestedCons:  Initial requests for uncached data are slower  Potential for stale data if not properly invalidated  Complexity in managing cache consistencyWrite-through Cachingflowchart TD    subgraph writeThroughCaching[\"‚úçÔ∏è Write-through Caching\"]        style writeThroughCaching fill:#f6ffed,stroke:#95de64                app[\"üì± Application\"]        cache[\"üíæ Redis Cache\"]        db[\"üóÑÔ∏è Database\"]                app --&gt; |\"1. Write data\"| cache        app --&gt; |\"1. Write data\"| db        cache --&gt; |\"2. Acknowledge\"| app        db --&gt; |\"2. Acknowledge\"| app    endWrite-through caching updates both the cache and the database simultaneously.app.post('/user', async (req, res) =&gt; {  const { userData } = req.body;  const userId = generateUniqueId();  const cacheKey = `user:${userId}`;  try {    await Promise.all([      redis.setex(cacheKey, 3600, JSON.stringify(userData)),      pool.query('INSERT INTO users (id, data) VALUES ($1, $2)', [userId, userData])    ]);    res.json({ message: 'User created successfully', userId });  } catch (error) {    console.error('Error:', error);    res.status(500).json({ error: 'Internal server error' });  }});Pros:  Cache is always up-to-date with the database  Simplified read operationsCons:  Increased write latency  Higher resource usage for writes  May cache infrequently accessed dataRead-through Cachingflowchart TD    subgraph readThroughCaching[\"üìñ Read-through Caching\"]        style readThroughCaching fill:#fff7e6,stroke:#ffd591                app[\"üì± Application\"]        cache[\"üíæ Redis Cache\"]        db[\"üóÑÔ∏è Database\"]                app --&gt; |\"1. Request data\"| cache        cache --&gt; |\"2. Cache miss\"| db        db --&gt; |\"3. Return data\"| cache        cache --&gt; |\"4. Update cache\"| cache        cache --&gt; |\"5. Return data\"| app    endRead-through caching handles cache misses transparently, with the cache itself responsible for loading data from the database.class ReadThroughCache {  constructor(redis, db) {    this.redis = redis;    this.db = db;  }  async get(key, fetchFn) {    let data = await this.redis.get(key);    if (!data) {      data = await fetchFn();      if (data) {        await this.redis.setex(key, 3600, JSON.stringify(data));      }    } else {      data = JSON.parse(data);    }    return data;  }}const cache = new ReadThroughCache(redis, pool);app.get('/user/:id', async (req, res) =&gt; {  const { id } = req.params;  const cacheKey = `user:${id}`;  try {    const userData = await cache.get(cacheKey, async () =&gt; {      const result = await pool.query('SELECT * FROM users WHERE id = $1', [id]);      return result.rows[0];    });    res.json(userData || { error: 'User not found' });  } catch (error) {    console.error('Error:', error);    res.status(500).json({ error: 'Internal server error' });  }});Pros:  Simplified application logic  Consistent caching behavior across the application  Reduced chance of stale data compared to lazy loadingCons:  Increased complexity in cache implementation  Potential for increased latency on cache misses  May cache infrequently accessed dataPub/SubRedis Pub/Sub (Publish/Subscribe) is a messaging paradigm where senders (publishers) send messages to channels without knowledge of which receivers (subscribers) will receive them. Subscribers express interest in one or more channels and only receive messages from the channels they‚Äôre subscribed to.graph LR    subgraph pubSub[\"üì° Redis Pub/Sub System\"]        publisher[\"üë§ Publisher\"]        subscriber1[\"üë• Subscriber 1\"]        subscriber2[\"üë• Subscriber 2\"]        channel1[\"üì¢ Channel 1\"]        channel2[\"üì¢ Channel 2\"]    end    publisher --&gt;|\"Publish\"| channel1    publisher --&gt;|\"Publish\"| channel2    channel1 --&gt;|\"Broadcast\"| subscriber1    channel1 --&gt;|\"Broadcast\"| subscriber2    channel2 --&gt;|\"Broadcast\"| subscriber1    style pubSub fill:#f5f5f5,stroke:#333,stroke-width:2px    style publisher fill:#bbdefb,stroke:#1976d2,stroke-width:2px    style subscriber1 fill:#c8e6c9,stroke:#388e3c,stroke-width:2px    style subscriber2 fill:#c8e6c9,stroke:#388e3c,stroke-width:2px    style channel1 fill:#ffe0b2,stroke:#f57c00,stroke-width:2px    style channel2 fill:#ffe0b2,stroke:#f57c00,stroke-width:2px  Channels: Named destinations for messages. They don‚Äôt need to be created explicitly.  Publishers: Clients that send messages to channels.  Subscribers: Clients that listen for messages on specific channels.  Messages: The data sent from publishers to subscribers via channels.Implementation with ioredis:  Setting up connections:    const publisher = new Redis();const subscriber = new Redis();        We create separate Redis connections for publishing and subscribing to avoid blocking issues.    Subscribing to channels:    subscriber.subscribe('news_channel', 'tech_channel', (err, count) =&gt; {  // Callback after subscription});        A client can subscribe to multiple channels at once.    Handling incoming messages:    subscriber.on('message', (channel, message) =&gt; {  console.log(`Received ${message} from ${channel}`);});        This event listener is triggered for every message received on subscribed channels.    Publishing messages:    publisher.publish(channel, message, (err, count) =&gt; {  // Callback after publishing});        This sends a message to a specified channel. The callback receives the count of subscribers who received the message.    Unsubscribing and cleanup:    subscriber.unsubscribe();subscriber.quit();publisher.quit();        It‚Äôs important to properly close connections when they‚Äôre no longer needed.  Use Cases:  Real-time notifications: Sending updates to multiple clients simultaneously.  Chat applications: Broadcasting messages to chat rooms or users.  Live feeds: Distributing live data (e.g., stock prices, sports scores).  Distributed system events: Communicating events across different parts of a distributed system.Considerations:  Messages are fire-and-forget. If a subscriber is offline, it won‚Äôt receive messages sent while it was down.  Redis Pub/Sub doesn‚Äôt persist messages. For message queuing with persistence, consider Redis Streams.  Subscribers that are subscribed to channels are in ‚Äúsubscriber mode‚Äù and can only use a subset of Redis commands.BullMQBullMQ is a modern, high-performance job queue for Node.js applications, built on top of Redis. It allows you to manage background jobs and tasks efficiently, leveraging Redis‚Äôs speed and reliability.Key ConceptsBullMQ is designed to be scalable and robust, providing several key features:  Queues: Manage jobs and tasks by organizing them into queues.  Workers: Processes jobs from a queue, can be run concurrently.  Jobs: The tasks you want to process, with support for retries, delays, and prioritization.  Events: BullMQ emits events at different stages of a job‚Äôs lifecycle, such as when a job is completed, failed, or delayed.Basic Usage   const { Queue, Worker, QueueScheduler } = require('bullmq');   const IORedis = require('ioredis');   // Redis connection   const connection = new IORedis();   // Create a new queue   const myQueue = new Queue('myQueue', { connection });   // Add a job to the queue   myQueue.add('myJob', { foo: 'bar' });   // Schedule worker to process jobs   const worker = new Worker('myQueue', async job =&gt; {     console.log(`Processing job: ${job.id}`);     console.log(`Job data: ${JSON.stringify(job.data)}`);     // Job processing logic here   }, { connection });   worker.on('completed', job =&gt; {     console.log(`Job completed: ${job.id}`);   });   worker.on('failed', (job, err) =&gt; {     console.log(`Job failed: ${job.id} - ${err.message}`);   });graph LR    subgraph bullMQFlow[\"üöÄ BullMQ Flow\"]        queue[\"üìã Queue\"] --&gt; |\"Add Job\"| job[\"üìù Job\"]        job --&gt; |\"Schedule\"| worker[\"üë∑ Worker\"]        worker --&gt; |\"Process\"| completed[\"‚úÖ Job Completed\"]        worker --&gt; |\"Handle Error\"| failed[\"‚ùå Job Failed\"]        worker --&gt; |\"Retry\"| retry[\"üîÑ Retry Job\"]    end    style bullMQFlow fill:#fff3e0,stroke:#f57c00,stroke-width:2px    style queue fill:#bbdefb,stroke:#1976d2,stroke-width:2px    style job fill:#e1bee7,stroke:#8e24aa,stroke-width:2px    style worker fill:#c8e6c9,stroke:#388e3c,stroke-width:2px    style completed fill:#dcedc8,stroke:#689f38,stroke-width:2px    style failed fill:#ffcdd2,stroke:#d32f2f,stroke-width:2px    style retry fill:#ffe0b2,stroke:#ff9800,stroke-width:2pxUse CasesBullMQ is ideal for:  Background Job Processing: Handle tasks like image processing, sending emails, or performing heavy computations in the background.  Task Scheduling: Run jobs at specific times or after delays.  Rate-Limited Jobs: Control the rate of job execution to prevent overwhelming your system.LeaderboardsSorted Sets in Redis are a powerful data structure for implementing leaderboards, rankings, and scoring systems. By using the ZADD, ZINCRBY, and ZRANGE commands, you can efficiently manage player scores, ranks, and top scorers, with operations typically running in O(logN) time complexity, making them highly efficient for large datasets.const Redis = require('ioredis');const redis = new Redis();const LEADERBOARD_KEY = 'game:leaderboard';class Leaderboard {  // Increment player's score  static async incrementScore(playerId, increment) {    try {      const newScore = await redis.zincrby(LEADERBOARD_KEY, increment, playerId);      return { playerId, newScore: parseFloat(newScore) };    } catch (error) {      console.error('Error incrementing score:', error);      throw error;    }  }  // Get top players  static async getTopPlayers(count) {    try {      const topPlayers = await redis.zrevrange(LEADERBOARD_KEY, 0, count - 1, 'WITHSCORES');      return topPlayers.reduce((acc, cur, i) =&gt; {        if (i % 2 === 0) {          acc.push({ playerId: cur, score: parseFloat(topPlayers[i + 1]) });        }        return acc;      }, []);    } catch (error) {      console.error('Error fetching top players:', error);      throw error;    }  }  // Get player's rank and score  static async getPlayerRank(playerId) {    try {      const [rank, score] = await Promise.all([        redis.zrevrank(LEADERBOARD_KEY, playerId),        redis.zscore(LEADERBOARD_KEY, playerId)      ]);            if (rank !== null &amp;&amp; score !== null) {        return { playerId, rank: rank + 1, score: parseFloat(score) };      } else {        return null;      }    } catch (error) {      console.error('Error fetching player rank:', error);      throw error;    }  }}// Example usageasync function runExample() {  try {    // Increment scores for some players    await Leaderboard.incrementScore('player1', 50);    await Leaderboard.incrementScore('player2', 30);    await Leaderboard.incrementScore('player3', 70);    await Leaderboard.incrementScore('player1', 20);    // Get top 3 players    const topPlayers = await Leaderboard.getTopPlayers(3);    console.log('Top 3 players:', topPlayers);    // Get rank for player1    const player1Rank = await Leaderboard.getPlayerRank('player1');    console.log('Player 1 rank:', player1Rank);  } catch (error) {    console.error('Error in example:', error);  } finally {    redis.quit();  }}runExample();  Efficient Score Updates: ZINCRBY increments scores atomically.  Fast Retrieval of Top Players: ZREVRANGE quickly fetches top scores.  Player Ranking: ZREVRANK provides player rankings efficiently.  Automatic Sorting: Redis keeps the set sorted, eliminating manual sorting.Sample ApplicationA sample application demonstrating the use of Redis for managing a leaderboard can be found at https://github.com/kelvin-bz/redis-leaderboard.Q&amp;AHow does Redis achieve its high performance?Redis achieves its high performance through several key factors:  In-memory storage: All data is stored in RAM, allowing for extremely fast read and write operations.  Single-threaded architecture: Eliminates the need for context switching and locking mechanisms.  Asynchronous operations: Non-blocking I/O operations allow Redis to handle multiple clients efficiently.  Optimized data structures: Redis uses custom, highly optimized data structures.What are the limitations of Redis?While Redis is powerful, it has some limitations:  Memory Constraint: Dataset size limited by available RAM, potentially costly for large datasets.  Limited Query Capabilities: Lacks complex querying of relational databases; no native full-text search.  Eventual Consistency: In distributed setups, may have data inconsistencies between master and replicas.  Single-Threaded: Can‚Äôt fully utilize multi-core CPUs for a single instance.  Persistence Tradeoffs: Balancing between performance and data durability in persistence settings.Can Redis be used as a primary database?While Redis is highly performant and supports a variety of data structures, it is not typically used as a primary database due to its in-memory nature, which limits data size to the available RAM. However, for specific use cases like caching, session management, or real-time analytics, Redis can serve as the primary datastore.How does Redis handle data replication?Redis supports data replication through a master-slave replication model. The master instance serves as the primary node for read and write operations, while one or more slave instances replicate data from the master. Replication can be synchronous or asynchronous, with slaves able to handle read operations to distribute the load."
  },
  
  {
    "title": "Dsa Fundamental",
    "url": "/posts/dsa-fundamental/",
    "categories": "",
    "tags": "",
    "date": "2024-08-02 00:00:00 +0700",
    





    
    "snippet": "Linked ListLinked List is a linear data structure. It is a collection of nodes where each node contains a data field and a reference(link) to the next node in the sequence.flowchart LR    subgraph ...",
    "content": "Linked ListLinked List is a linear data structure. It is a collection of nodes where each node contains a data field and a reference(link) to the next node in the sequence.flowchart LR    subgraph linkedList[\"üîó Linked List\"]        style linkedList fill:#e6f7ff,stroke:#91d5ff        subgraph            head[\"üëâ Head\"]        tail[\"üëà Tail\"]        length[\"üìè Length\"]        end        subgraph nodes[\"üì¶ Nodes\"]            style nodes fill:#f6ffed,stroke:#b7eb8f            node1[\"üì¶  Node 1\"]            node2[\"üì¶  Node 2\"]            node3[\"üì¶  Node 3\"]        end        subgraph nod[\"Node\"]            style nodes fill:#f6ffed,stroke:#b7eb8f            data[\"üìÑ data\"]            next[\"‚û°Ô∏è next\"]        end        node1 --&gt;|\"next\"| node2        node2 --&gt;|\"next\"| node3        tail --&gt;|\"points to\"| node3            end    class head,tail,length,node1,node2,node3 nodeStyle    classDef nodeStyle fill:#fff,stroke:#1890ff,stroke-width:2px,color:#000To implement a linked list, we need two classes: Node and LinkedList.class Node {  constructor(data) {    this.data = data;    this.next = null;  }}class LinkedList {  constructor() {    this.head = null;    this.tail = null;    this.length = 0;  }  /// Operations}OperationsAdd firstAdd a new node to the beginning of the linked list.flowchart    subgraph linkedList[\"üîó Linked List After Prepend\"]        style linkedList fill:#e6f7ff,stroke:#91d5ff        subgraph            head[\"üëâ Head\"]          tail[\"üëà Tail\"]        end                subgraph nodes[\"Nodes\"]            direction LR            style nodes fill:#f6ffed,stroke:#b7eb8f            newNode[\"üì¶ New Node\"] --&gt; node1[\"üì¶ Node 1\"] --&gt; node2[\"üì¶ Node 2\"] --&gt; node3[\"üì¶ Node 3\"]        end                head --&gt; newNode        tail --&gt; node3    end    classDef nodeStyle fill:#fff,stroke:#1890ff,stroke-width:2px,color:#000    class head,tail,node1,node2,node3 nodeStyle    class newNode fill:#fff0f6,stroke:#eb2f96,stroke-width:2px,color:#000 // add a node at first it to linklist  addFirst(element) {    const node = new Node(element)    // Check if its the first element    if (this.headNode === null) {      this.tailNode = node    }    // Adding node at the start of the list and increase the length    node.next = this.headNode    this.headNode = node    this.length++    return this.size()  }DeleteRemove a node from the linked list.flowchart LR    subgraph linkedList[\"üîó Linked List After Delete\"]        style linkedList fill:#e6f7ff,stroke:#91d5ff        subgraph          head[\"üëâ Head\"]        tail[\"üëà Tail\"]        currentNode[\"üîç Current Node\"]        end                 subgraph nodes[\"Nodes\"]            direction LR            style nodes fill:#f6ffed,stroke:#b7eb8f            node1[\"üì¶ Node 1\"] --&gt; node2[\"üì¶ Node 2\"]            node2 --&gt; |\"Skip deleted node\"| node4[\"üì¶ Node 4\"]        end                deletedNode[\"‚ùå Deleted Node 3\"]        class deletedNode fill:#fff0f6,stroke:#eb2f96,stroke-width:2px,color:#000,stroke-dasharray:                head --&gt; node1        tail --&gt; node4    end    classDef nodeStyle fill:#fff,stroke:#1890ff,stroke-width:2px,color:#000    class head,tail,node1,node2,node4,currentNode nodeStyledelete(value) {    if (!this.head) {      return null;    }    let deletedNode = null;    // If the head must be deleted then make next node that is different    // from the head to be a new head.    while (this.head &amp;&amp; this.compare.equal(this.head.value, value)) {      deletedNode = this.head;      this.head = this.head.next;    }    let currentNode = this.head;    if (currentNode !== null) {      // If next node must be deleted then make next node to be a next next one.      while (currentNode.next) {        if (this.compare.equal(currentNode.next.value, value)) {          deletedNode = currentNode.next;          currentNode.next = currentNode.next.next;        } else {          currentNode = currentNode.next;        }      }    }    // Check if tail must be deleted.    if (this.compare.equal(this.tail.value, value)) {      this.tail = currentNode;    }    return deletedNode;  }Implementation      https://github.com/TheAlgorithms/JavaScript/blob/master/Data-Structures/Linked-List/SinglyLinkedList.js        https://github.com/trekhleb/javascript-algorithms/blob/master/src/data-structures/linked-list/LinkedList.js  ComplexityTime Complexity| Access    | Search    | Insertion | Deletion  || :‚Äî‚Äî-: | :‚Äî‚Äî-: | :‚Äî‚Äî-: | :‚Äî‚Äî-: || O(n)      | O(n)      | O(1)      | O(n)      |Space ComplexityO(n)Q&amp;AIn which scenario linked list is better than an array in javascript?In most other cases, JavaScript‚Äôs arrays are likely to be your best choice due to their flexibility, built-in methods, and performance optimizations.      Frequent insertions/deletions at the beginning or middle of the list:          Linked List: O(1) time complexity      Array: O(n) time complexity (due to shifting elements)        Dynamic size requirements:          Linked List: Easily grows or shrinks without reallocation      Array: May require occasional reallocation and copying when growing beyond capacity        Memory allocation:          Linked List: Can utilize fragmented memory spaces      Array: Requires contiguous memory blocks        Implementation of certain data structures:          Linked lists are fundamental in implementing more complex data structures like stacks, queues, and certain types of trees and graphs.        Circular buffers:          Linked lists can easily create circular structures, which can be useful for certain algorithms or data representations.      Binary Search TreeBinary Search Tree is a node-based binary tree data structure which has the following properties:  The left subtree of a node contains only nodes with keys lesser than the node‚Äôs key.  The right subtree of a node contains only nodes with keys greater than the node‚Äôs key.  The left and right subtree each must also be a binary search tree.      50        30      20      40        70      60      80                    https://github.com/trekhleb/javascript-algorithms/tree/master/src/data-structures/tree/binary-search-tree#binary-search-treeflowchart TD    subgraph binarySearchTree[\"üå≥ Binary Search Tree\"]        style binarySearchTree fill:#e6f7ff,stroke:#91d5ff        subgraph  BinarySearchTree        root[\"üëë Root\"]        size[\"üìè Size\"]        end        subgraph nodes[\"üì¶ Nodes\"]            style nodes fill:#f6ffed,stroke:#b7eb8f            node1[\"üì¶ Node 1\"]            node2[\"üì¶ Node 2\"]            node3[\"üì¶ Node 3\"]            node4[\"üì¶ Node 4\"]            node5[\"üì¶ Node 5\"]        end        subgraph nod[\"Node\"]            style nod fill:#f6ffed,stroke:#b7eb8f            data[\"üìÑ value\"]            left[\"‚ÜôÔ∏è left\"]            right[\"‚ÜòÔ∏è right\"]        end        root --&gt;|\"points to\"| node1        node1 --&gt;|\"left\"| node2        node1 --&gt;|\"right\"| node3        node2 --&gt;|\"left\"| node4        node3 --&gt;|\"right\"| node5    end    class root,size,node1,node2,node3,node4,node5 nodeStyle    classDef nodeStyle fill:#fff,stroke:#1890ff,stroke-width:2px,color:#000Implementationclass Node {    constructor(value) {        this.value = value;        this.left = null;        this.right = null;    }}class BinarySearchTree {    constructor() {        this.root = null;    }    insert(value) {        const newNode = new Node(value);        if (this.root === null) {            this.root = newNode;            return this;        }        let current = this.root;        while (true) {            if (value === current.value) return undefined;            if (value &lt; current.value) {                if (current.left === null) {                    current.left = newNode;                    return this;                }                current = current.left;            } else {                if (current.right === null) {                    current.right = newNode;                    return this;                }                current = current.right;            }        }    }    search(value) {        if (!this.root) return false;        let current = this.root;        while (current) {            if (value === current.value) return true;            if (value &lt; current.value) {                current = current.left;            } else {                current = current.right;            }        }        return false;    }    inOrderTraversal(node = this.root, result = []) {        if (node !== null) {            this.inOrderTraversal(node.left, result);            result.push(node.value);            this.inOrderTraversal(node.right, result);        }        return result;    }}// Example usage:const bst = new BinarySearchTree();bst.insert(10);bst.insert(5);bst.insert(15);bst.insert(2);bst.insert(7);console.log(bst.search(7));  // trueconsole.log(bst.search(9));  // falseconsole.log(bst.inOrderTraversal());  // [2, 5, 7, 10, 15]ComplexityTime Complexity| Access    | Search    | Insertion | Deletion  || :‚Äî‚Äî-: | :‚Äî‚Äî-: | :‚Äî‚Äî-: | :‚Äî‚Äî-: || O(log n)  | O(log n)  | O(log n)  | O(log n)  |Space ComplexityO(n)Q&amp;AWhat is the difference between a binary tree and a binary search tree?  Binary Tree:          A binary tree is a tree data structure in which each node has at most two children, referred to as the left child and the right child.      There are no restrictions on the values of the nodes.      It is used to represent hierarchical relationships.      It is used to implement more complex data structures like binary search trees, heaps, and expression trees.        Binary Search Tree:          A binary search tree is a node-based binary tree data structure in which the left subtree of a node contains only nodes with keys lesser than the node‚Äôs key, and the right subtree contains only nodes with keys greater than the node‚Äôs key.      It is used to store data in a sorted order.      It is used to implement searching algorithms like binary search.      It is used to implement more complex data structures like AVL trees, red-black trees, and splay trees.      When should I use a binary search tree over an sorted array?  Use a BST for dynamic data with frequent insertions and deletions (O(log n) vs O(n) for arrays).  Choose BST for efficient memory allocation in scenarios with unpredictable data growth.  Prefer BST for implementing maps/sets and performing range queries efficiently.  Opt for BST when space efficiency is crucial, especially with sparse data sets.  Select a sorted array instead if you need fast random access, better cache performance, or simpler implementation.B-TreeQ&amp;AHow data is stored in a B-tree?graph TD;    Root[\"Diana | George\"]    Root --&gt; Left[\"Alice | Bob | Charlie\"]    Root --&gt; Middle[\"Edward | Fiona\"]    Root --&gt; Right[\"Hannah | Ian | Julia | Kevin | Laura\"]Binary Searchfunction binarySearch(arr: number[], target: number): number {    let left = 0;    let right = arr.length - 1;    while (left &lt;= right) {        const mid = Math.floor((left + right) / 2);        if (arr[mid] === target) {            return mid; // Element found, return the index        } else if (arr[mid] &lt; target) {            left = mid + 1; // Search in the right half        } else {            right = mid - 1; // Search in the left half        }    }    return -1; // Element not found}// Example usage:const sortedArray: number[] = [1, 3, 5, 7, 9, 11];const target: number = 7;const index: number = binarySearch(sortedArray, target);console.log(index); // Output: 3 (because sortedArray[3] is 7)HEAPDefinitionHeap is a specialized tree-based data structure that satisfies the heap property. It is used to implement priority queues.Min-Heap: In a Min-Heap, the key at the root must be minimum among all keys present in the heap.Max-Heap: In a Max-Heap, the key at the root must be maximum among all keys present in the heap.Implementationgraph TD    subgraph heapStructure[\"üå≥ Heap Structure\"]        style heapStructure fill:#e6f7ff,stroke:#4db8ff        root[\"üîù Root (Smallest Element)\"]        leftChild[\"üëà Left Child\"]        rightChild[\"üëâ Right Child\"]        grandChild1[\"üçÉ Grandchild 1\"]        grandChild2[\"üçÉ Grandchild 2\"]        grandChild3[\"üçÉ Grandchild 3\"]        grandChild4[\"üçÉ Grandchild 4\"]        root --&gt; |\"Left\"| leftChild        root --&gt; |\"Right\"| rightChild        leftChild --&gt; |\"Left\"| grandChild1        leftChild --&gt; |\"Right\"| grandChild2        rightChild --&gt; |\"Left\"| grandChild3        rightChild --&gt; |\"Right\"| grandChild4    end    subgraph heapOperations[\"‚öôÔ∏è Heap Operations\"]        style heapOperations fill:#fff0f5,stroke:#ff69b4        insert[\"üì• Insert\"]        extractMin[\"üì§ Extract Min\"]        bubbleUp[\"üîº Bubble Up\"]        bubbleDown[\"üîΩ Bubble Down\"]    end    subgraph heapProperties[\"üìä Heap Properties\"]        style heapProperties fill:#f0fff0,stroke:#90ee90        completeTree[\"üå≤ Complete Binary Tree\"]        heapProperty[\"üìè Heap Property: Parent ‚â§ Children\"]    end    insert --&gt; |\"Triggers\"| bubbleUp    extractMin --&gt; |\"Triggers\"| bubbleDown    bubbleUp --&gt; |\"Maintains\"| heapProperty    bubbleDown --&gt; |\"Maintains\"| heapProperty    heapStructure --&gt; |\"Follows\"| completeTreeclass MinHeap {    private heap: number[];    constructor() {        this.heap = [];    }    private getParentIndex(index: number): number {        return Math.floor((index - 1) / 2);    }    private getLeftChildIndex(index: number): number {        return 2 * index + 1;    }    private getRightChildIndex(index: number): number {        return 2 * index + 2;    }    private swap(index1: number, index2: number): void {        const temp = this.heap[index1];        this.heap[index1] = this.heap[index2];        this.heap[index2] = temp;    }    insert(value: number): void {        this.heap.push(value);        this.bubbleUp(this.heap.length - 1);    }    private bubbleUp(index: number): void {        while (index &gt; 0 &amp;&amp; this.heap[this.getParentIndex(index)] &gt; this.heap[index]) {            this.swap(index, this.getParentIndex(index));            index = this.getParentIndex(index);        }    }    extractMin(): number | undefined {        if (this.heap.length === 0) return undefined;        if (this.heap.length === 1) return this.heap.pop();        const min = this.heap[0];        this.heap[0] = this.heap.pop()!;        this.bubbleDown(0);        return min;    }    private bubbleDown(index: number): void {        while (true) {            const leftChildIndex = this.getLeftChildIndex(index);            const rightChildIndex = this.getRightChildIndex(index);            let smallestIndex = index;            if (leftChildIndex &lt; this.heap.length &amp;&amp; this.heap[leftChildIndex] &lt; this.heap[smallestIndex]) {                smallestIndex = leftChildIndex;            }            if (rightChildIndex &lt; this.heap.length &amp;&amp; this.heap[rightChildIndex] &lt; this.heap[smallestIndex]) {                smallestIndex = rightChildIndex;            }            if (smallestIndex === index) break;            this.swap(index, smallestIndex);            index = smallestIndex;        }    }    peek(): number | undefined {        return this.heap[0];    }    size(): number {        return this.heap.length;    }}This code implements a basic min-heap in TypeScript. Here‚Äôs a breakdown of the key components:  The MinHeap class uses an array to store the heap.  insert(value) adds a new element to the heap and maintains the heap property.  extractMin() removes and returns the minimum element (root) from the heap.  peek() returns the minimum element without removing it.  size() returns the number of elements in the heap.The main operations are:  bubbleUp(): Used after insertion to maintain the heap property.  bubbleDown(): Used after extracting the minimum to maintain the heap property.Bubble Up &amp;&amp; Bubble DownBubble Up  Purpose: Used when inserting a new element.  Process:          Compares the new element with its parent.      Swaps if necessary.      Continues until the heap property is restored.      Bubble Down  Purpose: Used after extracting the root element.  Process:          Replaces the root with the last element.      Compares with children.      Swaps with the smaller child (in a min-heap) if needed.      Continues until the heap property is restored.      graph TD    subgraph bubbleUp[\"üîº Bubble Up\"]        style bubbleUp fill:#e6f7ff,stroke:#4db8ff        insert[\"üì• Insert new element\"]        compareParent[\"üîç Compare with parent\"]        swapUp[\"üîÑ Swap if needed\"]        insert --&gt; compareParent        compareParent --&gt; |\"New &lt; Parent\"| swapUp        swapUp --&gt; compareParent        compareParent --&gt; |\"New &gt;= Parent\"| done1[\"‚úÖ Done\"]    end    subgraph bubbleDown[\"üîΩ Bubble Down\"]        style bubbleDown fill:#fff0f5,stroke:#ff69b4        extractRoot[\"üì§ Extract root\"]        replaceRoot[\"üîÑ Replace with last element\"]        compareChildren[\"üîç Compare with children\"]        swapDown[\"üîÑ Swap with smaller child\"]        extractRoot --&gt; replaceRoot        replaceRoot --&gt; compareChildren        compareChildren --&gt; |\"Parent &gt; Child\"| swapDown        swapDown --&gt; compareChildren        compareChildren --&gt; |\"Parent &lt;= Children\"| done2[\"‚úÖ Done\"]    endComplexityTime Complexity| Access    | Search    | Insertion | Deletion  || :‚Äî‚Äî-: | :‚Äî‚Äî-: | :‚Äî‚Äî-: | :‚Äî‚Äî-: || O(1)      | O(n)      | O(log n)  | O(log n)  |Space ComplexityO(n)Q&amp;AWhen should I use a heap over a sorted array?Faster insertions: O(log n) vs O(n).Min heaps are preferred for dynamic datasets requiring frequent insertions and extractions, while sorted arrays excel for static data with constant minimum access needs.Depth-First Search (DFS)DefinitionDepth-First Search (DFS) is a graph traversal algorithm that explores as far as possible along each branch before backtracking. It starts at a chosen node and explores as far as possible along each branch before backtracking.ImplementationDFS can be implemented using either recursion or an explicit stack.graph TD    subgraph dfsProcess[\"DFS Process\"]        style dfsProcess fill:#e6f7ff,stroke:#4db8ff        start[\"Start at root/chosen node\"]        explore[\"Explore unvisited neighbor\"]        backtrack[\"Backtrack when no unvisited neighbors\"]        start --&gt; explore        explore --&gt; explore        explore --&gt; backtrack        backtrack --&gt; explore    end    subgraph dfsTypes[\"DFS Types\"]        style dfsTypes fill:#fff0f5,stroke:#ff69b4        preOrder[\"Pre-order\"]        inOrder[\"In-order\"]        postOrder[\"Post-order\"]    end    subgraph useCases[\"Use Cases\"]        style useCases fill:#f0fff0,stroke:#90ee90        pathFinding[\"üó∫Ô∏è Path Finding\"]        topologicalSort[\"üìä Topological Sorting\"]        cycleDetection[\"üîÑ Cycle Detection\"]    end    dfsProcess --&gt; dfsTypes    dfsTypes --&gt; useCasesclass Graph {    private adjacencyList: Map&lt;number, number[]&gt;;    constructor() {        this.adjacencyList = new Map();    }    addVertex(vertex: number): void {        if (!this.adjacencyList.has(vertex)) {            this.adjacencyList.set(vertex, []);        }    }    addEdge(vertex1: number, vertex2: number): void {        this.adjacencyList.get(vertex1)?.push(vertex2);        this.adjacencyList.get(vertex2)?.push(vertex1);    }    dfs(start: number): number[] {        const result: number[] = [];        const visited: Set&lt;number&gt; = new Set();        const dfsRecursive = (vertex: number) =&gt; {            visited.add(vertex);            result.push(vertex);            this.adjacencyList.get(vertex)?.forEach(neighbor =&gt; {                if (!visited.has(neighbor)) {                    dfsRecursive(neighbor);                }            });        };        dfsRecursive(start);        return result;    }}// Usageconst graph = new Graph();graph.addVertex(0);graph.addVertex(1);graph.addVertex(2);graph.addVertex(3);graph.addEdge(0, 1);graph.addEdge(0, 2);graph.addEdge(1, 2);graph.addEdge(2, 3);console.log(graph.dfs(0)); // Output: [0, 1, 2, 3]graph LR    0((0)) --- 1((1))    0 --- 2((2))    1 --- 2    2 --- 3((3))    style 0 fill:#f9f,stroke:#333,stroke-width:4px    style 1 fill:#bbf,stroke:#333,stroke-width:2px    style 2 fill:#bbf,stroke:#333,stroke-width:2px    style 3 fill:#bbf,stroke:#333,stroke-width:2pxThis implementation uses recursion to perform DFS on a graph represented as an adjacency list.Key Characteristics  Depth-first: Explores as far as possible along each branch before backtracking.  Backtracking: When it reaches a dead-end, it backtracks to explore other paths.  Memory efficient: Typically requires less memory than breadth-first search.  May not find shortest path: Unlike BFS, DFS is not guaranteed to find the shortest path in an unweighted graph.Variants  Pre-order DFS: Process current node before children.  In-order DFS: Process left child, then current node, then right child (for binary trees).  Post-order DFS: Process children before current node.ComplexityTime ComplexityO(V + E), where V is the number of vertices and E is the number of edges in the graph.Space ComplexityO(V) in the worst case, where V is the number of vertices.Use Cases  Path Finding: Finding a path between two vertices.  Topological Sorting: Ordering tasks based on their dependencies.  Cycle Detection: Detecting cycles in a graph.  Maze Solving: Finding a way out of a maze.  Strongly Connected Components: Finding strongly connected components in a directed graph.Q&amp;AWhen should I use DFS instead of BFS?Use DFS when:  You need to search all the way to a leaf node (e.g., in game tree analysis).  Memory is a constraint (DFS generally uses less memory than BFS).  You‚Äôre solving problems like topological sorting or cycle detection.  The solution is known to be far from the root.BFS is preferred when you need to find the shortest path in an unweighted graph or when the solution is likely to be closer to the starting point.What are the main differences between recursive and iterative implementations of DFS?  Recursive:          Simpler and more intuitive to implement      Uses system stack, which may lead to stack overflow for very deep graphs      Naturally handles pre-order, in-order, and post-order traversals        Iterative:          More complex to implement but avoids potential stack overflow      Explicitly manages its own stack      Easier to pause and resume      Typically more efficient in terms of function call overhead      This DFS section provides a comprehensive overview, including its definition, implementation (with both a diagram and TypeScript code), key characteristics, variants, complexity analysis, use cases, and relevant Q&amp;A. The structure and content depth are similar to the previous sections on Heap and Priority Queue, ensuring consistency in the educational material.Breadth-First Search (BFS)DefinitionBreadth-First Search (BFS) is a graph traversal algorithm that explores all the vertices of a graph at the present depth prior to moving on to the vertices at the next depth level.ImplementationBFS is typically implemented using a queue.graph TD    subgraph bfsProcess[\"BFS Process\"]        style bfsProcess fill:#e6f7ff,stroke:#4db8ff        start[\"Start at root/chosen node\"]        exploreNeighbors[\"Explore all neighbors\"]        moveNextLevel[\"Move to next level\"]        start --&gt; exploreNeighbors        exploreNeighbors --&gt; moveNextLevel        moveNextLevel --&gt; exploreNeighbors    end    subgraph bfsCharacteristics[\"BFS Characteristics\"]        style bfsCharacteristics fill:#fff0f5,stroke:#ff69b4        shortestPath[\"Finds Shortest Path\"]        levelOrder[\"Level Order Traversal\"]    end    subgraph useCases[\"Use Cases\"]        style useCases fill:#f0fff0,stroke:#90ee90        shortestPathFinding[\"üó∫Ô∏è Shortest Path Finding\"]        webCrawling[\"üï∏Ô∏è Web Crawling\"]        socialNetworks[\"üë• Social Network Analysis\"]    end    bfsProcess --&gt; bfsCharacteristics    bfsCharacteristics --&gt; useCasesclass Graph {    private adjacencyList: Map&lt;number, number[]&gt;;    constructor() {        this.adjacencyList = new Map();    }    addVertex(vertex: number): void {        if (!this.adjacencyList.has(vertex)) {            this.adjacencyList.set(vertex, []);        }    }    addEdge(vertex1: number, vertex2: number): void {        this.adjacencyList.get(vertex1)?.push(vertex2);        this.adjacencyList.get(vertex2)?.push(vertex1);    }    bfs(start: number): number[] {        const result: number[] = [];        const visited: Set&lt;number&gt; = new Set();        const queue: number[] = [start];        visited.add(start);        while (queue.length &gt; 0) {            const vertex = queue.shift()!;            result.push(vertex);            this.adjacencyList.get(vertex)?.forEach(neighbor =&gt; {                if (!visited.has(neighbor)) {                    visited.add(neighbor);                    queue.push(neighbor);                }            });        }        return result;    }}// Usageconst graph = new Graph();graph.addVertex(0);graph.addVertex(1);graph.addVertex(2);graph.addVertex(3);graph.addEdge(0, 1);graph.addEdge(0, 2);graph.addEdge(1, 2);graph.addEdge(2, 3);console.log(graph.bfs(0)); // Output: [0, 1, 2, 3]Key Characteristics  Level-wise exploration: Explores all neighbors at the present depth before moving to the next level.  Shortest path: Guarantees the shortest path in unweighted graphs.  Memory intensive: Typically requires more memory than depth-first search.  Complete: Will find a solution if one exists, provided the graph is finite.ComplexityTime ComplexityO(V + E), where V is the number of vertices and E is the number of edges in the graph.Space ComplexityO(V), where V is the number of vertices.Use Cases  Shortest Path Finding: In unweighted graphs or networks.  Web Crawling: Exploring web pages level by level.  Social Network Analysis: Finding all friends within n connections.  GPS Navigation: Finding nearby places.  Puzzle Solving: Solving puzzles with the fewest moves.Q&amp;AWhen should I use BFS instead of DFS?Use BFS when:  You need to find the shortest path in an unweighted graph.  The solution is likely to be closer to the starting point.  You‚Äôre working with a graph that may have cycles (BFS avoids getting trapped in cycles).  You need to search level by level (e.g., finding all nodes at a distance k from the start).DFS is preferred when memory is a constraint or when you need to search all the way to a leaf node before backtracking.How does BFS differ from Dijkstra‚Äôs algorithm?  Graph type:          BFS works on unweighted graphs.      Dijkstra‚Äôs algorithm works on weighted graphs.        Shortest path:          BFS finds the shortest path in terms of the number of edges.      Dijkstra‚Äôs finds the shortest path in terms of the sum of edge weights.        Implementation:          BFS typically uses a simple queue.      Dijkstra‚Äôs uses a priority queue to always select the node with the smallest current distance.        Complexity:          BFS: O(V + E)      Dijkstra‚Äôs: O((V + E) log V) with a binary heap implementation.      This BFS section provides a comprehensive overview, including its definition, implementation (with both diagrams and TypeScript code), key characteristics, complexity analysis, use cases, and relevant Q&amp;A. The structure and content depth are similar to the previous sections, ensuring consistency in the educational material. The Mermaid diagram for the graph structure is included to visualize the example used in the code snippet.DFS vs BFS Comparisonconst graph = new Graph();graph.addVertex(0);graph.addVertex(1);graph.addVertex(2);graph.addVertex(3);graph.addVertex(4);graph.addVertex(5);graph.addEdge(0, 1);graph.addEdge(0, 2);graph.addEdge(1, 3);graph.addEdge(2, 4);graph.addEdge(1, 4);graph.addEdge(3, 5);graph.addEdge(4, 5);Graph Structuregraph TD    0((0)) --- 1((1))    0 --- 2((2))    1 --- 3((3))    1 --- 4((4))    2 --- 4    3 --- 5((5))    4 --- 5    style 0 fill:#f9f,stroke:#333,stroke-width:4px    style 1 fill:#bbf,stroke:#333,stroke-width:2px    style 2 fill:#bbf,stroke:#333,stroke-width:2px    style 3 fill:#bbf,stroke:#333,stroke-width:2px    style 4 fill:#bbf,stroke:#333,stroke-width:2px    style 5 fill:#bbf,stroke:#333,stroke-width:2pxDFS Traversal (starting from vertex 0)graph TD    0((0)) --&gt; 1((1))    1 --&gt; 3((3))    3 --&gt; 5((5))    1 --&gt; 4((4))    0 --&gt; 2((2))    style 0 fill:#f9f,stroke:#333,stroke-width:4px    style 1 fill:#ddf,stroke:#333,stroke-width:3px    style 2 fill:#fdd,stroke:#333,stroke-width:2px    style 3 fill:#dfd,stroke:#333,stroke-width:2px    style 4 fill:#fdf,stroke:#333,stroke-width:2px    style 5 fill:#dff,stroke:#333,stroke-width:2px    classDef order1 fill:#f9f,stroke:#333,stroke-width:4px    classDef order2 fill:#ddf,stroke:#333,stroke-width:3px    classDef order3 fill:#dfd,stroke:#333,stroke-width:3px    classDef order4 fill:#dff,stroke:#333,stroke-width:3px    classDef order5 fill:#fdf,stroke:#333,stroke-width:3px    classDef order6 fill:#fdd,stroke:#333,stroke-width:3px    class 0 order1    class 1 order2    class 3 order3    class 5 order4    class 4 order5    class 2 order6DFS Output: [0, 1, 3, 5, 4, 2]BFS Traversal (starting from vertex 0)graph TD    0((0)) --&gt; 1((1))    0 --&gt; 2((2))    1 --&gt; 3((3))    1 --&gt; 4((4))    3 --&gt; 5((5))    4 --&gt; 5    style 0 fill:#f9f,stroke:#333,stroke-width:4px    style 1 fill:#ddf,stroke:#333,stroke-width:3px    style 2 fill:#dfd,stroke:#333,stroke-width:3px    style 3 fill:#fdf,stroke:#333,stroke-width:2px    style 4 fill:#fdf,stroke:#333,stroke-width:2px    style 5 fill:#dff,stroke:#333,stroke-width:2px    classDef order1 fill:#f9f,stroke:#333,stroke-width:4px    classDef order2 fill:#ddf,stroke:#333,stroke-width:3px    classDef order3 fill:#dfd,stroke:#333,stroke-width:3px    classDef order4 fill:#fdf,stroke:#333,stroke-width:3px    classDef order5 fill:#dff,stroke:#333,stroke-width:3px    class 0 order1    class 1,2 order2    class 3,4 order3    class 5 order4BFS Output: [0, 1, 2, 3, 4, 5]Key Differences  Traversal Order:          DFS goes deep into a path before backtracking. It explores 0 ‚Üí 1 ‚Üí 3 ‚Üí 5 before backtracking to explore 4 and 2.      BFS explores all neighbors of a vertex before moving to the next level. It explores all neighbors of 0 (1 and 2) before moving to the next level (3 and 4).        Path to Node 5:          DFS reaches 5 through the path 0 ‚Üí 1 ‚Üí 3 ‚Üí 5.      BFS reaches 5 last, after exploring all nodes at shorter distances from 0.        Exploration Pattern:          DFS tends to move away from the start node quickly.      BFS expands outward from the start node in circles of increasing radius.        Completeness:          Both DFS and BFS will visit all nodes in this connected graph, but in different orders.      In an infinite graph, DFS might not find a node that exists (if it goes down an infinite path), while BFS will always find a node if it exists at a finite depth.      These differences highlight why BFS is preferred for finding shortest paths in unweighted graphs and for exploring nodes closer to the start, while DFS is often simpler to implement recursively and can be more memory-efficient for deep graphs.BacktrackingDefinitionBacktracking is an algorithmic technique that considers searching every possible combination in order to solve a computational problem. It builds candidates to the solution incrementally and abandons a candidate (‚Äúbacktracks‚Äù) as soon as it determines that the candidate cannot lead to a valid solution.Concept Visualizationgraph TD    A[Start] --&gt; B[Choose]    B --&gt; C[Constraint Check]    C --&gt;|Valid| D[Next Step]    C --&gt;|Invalid| E[Backtrack]    D --&gt; F{Solution Found?}    F --&gt;|Yes| G[Return Solution]    F --&gt;|No| B    E --&gt; B        style A fill:#f9f,stroke:#333,stroke-width:4px    style B fill:#bbf,stroke:#333,stroke-width:2px    style C fill:#bfb,stroke:#333,stroke-width:2px    style D fill:#fbf,stroke:#333,stroke-width:2px    style E fill:#fbb,stroke:#333,stroke-width:2px    style F fill:#bff,stroke:#333,stroke-width:2px    style G fill:#ff9,stroke:#333,stroke-width:2pxImplementationHere‚Äôs a general template for backtracking algorithms:function backtrack(candidate: PartialSolution): void {    if (isComplete(candidate)) {        saveSolution(candidate);        return;    }        for (const next of nextChoices(candidate)) {        if (isValid(next)) {            applyChoice(candidate, next);            backtrack(candidate);            undoChoice(candidate, next);        }    }}graph TD    A((Start)) --&gt; B((B))    A --&gt; C((C))    B --&gt; D((D))    B --&gt; E((E))    C --&gt; F((F))    C --&gt; G((G))    D --&gt; H((H))    D --&gt; I((I))    E --&gt; J((J))    E --&gt; K((K))    style A fill:#f9f,stroke:#333,stroke-width:4px    style B fill:#bfb,stroke:#333,stroke-width:2px    style C fill:#fbb,stroke:#333,stroke-width:2px    style D fill:#bfb,stroke:#333,stroke-width:2px    style E fill:#fbb,stroke:#333,stroke-width:2px    style F fill:#fbb,stroke:#333,stroke-width:2px    style G fill:#fbb,stroke:#333,stroke-width:2px    style H fill:#bff,stroke:#333,stroke-width:2px    style I fill:#fbb,stroke:#333,stroke-width:2px    style J fill:#fbb,stroke:#333,stroke-width:2px    style K fill:#fbb,stroke:#333,stroke-width:2px    A --&gt;|1| B    B --&gt;|2| D    D --&gt;|3| H    H --&gt;|4| D    D --&gt;|5| I    I --&gt;|6| D    D --&gt;|7| B    B --&gt;|8| E    E --&gt;|9| B    B --&gt;|10| A    A --&gt;|11| C    C --&gt;|12| F    F --&gt;|13| C    C --&gt;|14| G    G --&gt;|15| C    C --&gt;|16| A    classDef explored fill:#bfb,stroke:#333,stroke-width:2px;    classDef invalid fill:#fbb,stroke:#333,stroke-width:2px;    classDef solution fill:#bff,stroke:#333,stroke-width:2px;    class A,B,D explored;    class C,E,F,G,I,J,K invalid;    class H solution;Nodes:  Green nodes (A, B, D): Explored valid paths  Red nodes (C, E, F, G, I, J, K): Invalid or backtracked paths  Blue node (H): Solution foundEdges:  Numbered to show the order of explorationProcess:  Start at A  Explore path A -&gt; B -&gt; D -&gt; H (solution found)  Backtrack to D, try I (invalid)  Backtrack to B, try E (invalid)  Backtrack to A, try C (invalid)  Explore F and G from C (both invalid)  Backtrack to A, all paths exploredBacktracking Illustrated:  When reaching an invalid node (red), the algorithm backtracks to the previous valid node and tries a different path.  The process of going back up the tree (e.g., H -&gt; D -&gt; B -&gt; A) represents undoing choices and trying alternatives.Efficiency:  Notice how not all paths are fully explored. For example, after finding H is a solution, paths from I are not explored further.  Paths from E, F, and G are not explored at all because they‚Äôre determined to be invalid.Key Characteristics  Incremental Approach: Builds the solution step by step.  Constraint Satisfaction: Uses constraints to prune the search space.  Depth-First Exploration: Explores deeply into the solution space before backtracking.  Completeness: Guarantees to find a solution if one exists, given enough time and space.  Recursive Nature: Often implemented using recursion, though iterative versions are possible.Common Applications  Combinatorial Optimization Problems          N-Queens Problem      Sudoku Solver      Crossword Puzzle Solver        Graph Problems          Graph Coloring      Hamiltonian Path      Travelling Salesman Problem        Parsing and Pattern Matching          Regular Expression Matching        Game AI          Chess Game Trees      Tic-Tac-Toe AI      ComplexityThe time complexity of backtracking algorithms can vary widely depending on the specific problem and how effectively the constraints prune the search space. In the worst case, it can be exponential, O(b^d), where b is the branching factor and d is the maximum depth of the search tree.Pros and ConsPros:  Can solve complex problems with multiple constraints  Memory efficient compared to brute-force enumeration  Guaranteed to find all solutions if they existCons:  Can be slow for large problem sizes  May not be suitable for real-time applications due to potentially long running times  Implementation can be tricky and prone to bugsQ&amp;AQ: How does backtracking differ from brute-force search?A: While both backtracking and brute-force search explore all possibilities, backtracking is more efficient because it abandons partial solutions as soon as it determines they cannot lead to a valid solution. Brute-force, on the other hand, generates all possible solutions before checking their validity.Q: Can backtracking be used for optimization problems?A: Yes, backtracking can be adapted for optimization problems. Instead of stopping at the first valid solution, the algorithm continues searching and keeps track of the best solution found so far. This approach is often combined with branch and bound techniques for better efficiency.Q: How does backtracking relate to dynamic programming?A: Both are problem-solving techniques, but they approach problems differently:  Backtracking builds a solution incrementally and abandons paths that fail.  Dynamic programming breaks the problem into smaller subproblems and stores their solutions to avoid redundant computations.Backtracking is often used when the problem requires finding all solutions or when the problem doesn‚Äôt have optimal substructure, which is a key requirement for dynamic programming.Subset Sum ProblemProblem StatementGiven a set of positive integers and a target sum, find all subsets of the given set whose sum is equal to the target sum.Example  Input set: [3, 4, 5, 2]  Target sum: 9  Output: [[3, 4, 2], [4, 5]]Implementationfunction subsetSum(numbers: number[], target: number): number[][] {    const result: number[][] = [];        function backtrack(index: number, currentSum: number, currentSubset: number[]) {        if (currentSum === target) {            result.push([...currentSubset]);            return;        }                if (index &gt;= numbers.length || currentSum &gt; target) {            return;        }                // Include the current number        currentSubset.push(numbers[index]);        backtrack(index + 1, currentSum + numbers[index], currentSubset);                // Exclude the current number (backtrack)        currentSubset.pop();        backtrack(index + 1, currentSum, currentSubset);    }        backtrack(0, 0, []);    return result;}// Usageconst numbers = [3, 4, 5, 2];const target = 9;console.log(subsetSum(numbers, target));Visualization of the Processgraph TD    A{Include 3?} --&gt;|Yes| B[\"Sum: 3, [3]\"]    A --&gt;|No| C[\"Sum: 0, []\"]        B --&gt; D{Include 4?}    C --&gt; E{Include 4?}        D --&gt;|Yes| F[\"Sum: 7, [3,4]\"]    D --&gt;|No| G[\"Sum: 3, [3]\"]        E --&gt;|Yes| H[\"Sum: 4, [4]\"]    E --&gt;|No| I[\"Sum: 0, []\"]        F --&gt; J{Include 5?}    G --&gt; K{Include 5?}    H --&gt; L{Include 5?}    I --&gt; M{Include 5?}        J --&gt;|Yes| N[\"Sum: 12, [3,4,5]\"]    J --&gt;|No| O[\"Sum: 7, [3,4]\"]        O --&gt; P{Include 2?}    P --&gt;|Yes| Q[\"Sum: 9, [3,4,2]\"]    P --&gt;|No| R[\"Sum: 7, [3,4]\"]    style A fill:#f9f,stroke:#333,stroke-width:4px    style D fill:#f9f,stroke:#333,stroke-width:4px    style E fill:#f9f,stroke:#333,stroke-width:4px    style J fill:#f9f,stroke:#333,stroke-width:4px    style K fill:#f9f,stroke:#333,stroke-width:4px    style L fill:#f9f,stroke:#333,stroke-width:4px    style M fill:#f9f,stroke:#333,stroke-width:4px    style P fill:#f9f,stroke:#333,stroke-width:4px    style B fill:#bfb,stroke:#333,stroke-width:2px    style C fill:#bfb,stroke:#333,stroke-width:2px    style F fill:#bfb,stroke:#333,stroke-width:2px    style G fill:#bfb,stroke:#333,stroke-width:2px    style H fill:#bfb,stroke:#333,stroke-width:2px    style I fill:#bfb,stroke:#333,stroke-width:2px    style N fill:#bfb,stroke:#333,stroke-width:2px    style O fill:#bfb,stroke:#333,stroke-width:2px    style Q fill:#bff,stroke:#333,stroke-width:2px    style R fill:#bfb,stroke:#333,stroke-width:2px    classDef decision fill:#f9f,stroke:#333,stroke-width:4px;    classDef sum fill:#bfb,stroke:#333,stroke-width:2px;    classDef solution fill:#bff,stroke:#333,stroke-width:2px;    class A,D,E,J,K,L,M,P decision;    class B,C,F,G,H,I,N,O,R sum;    class Q solution;Explanation of the Backtracking Process  We start at the root (Start) and decide whether to include or exclude each number.  At each step, we have two choices: include the current number or exclude it.  We continue this process for each number in the set.  If at any point the sum exceeds the target (9 in this case), we stop exploring that branch (backtrack).  If we reach the target sum, we add the current subset to our results.  The blue nodes ([3,4,2] and [4,5]) represent the solutions we find.This process demonstrates key aspects of backtracking:  Making choices (include or exclude a number)  Exploring possibilities (moving down the tree)  Backtracking when a path is exhausted or invalid (moving back up to try other options)  Finding all possible solutions that meet the criteriaThe Subset Sum problem clearly shows how backtracking systematically explores all possibilities while efficiently pruning paths that can‚Äôt lead to a solution."
  },
  
  {
    "title": "Subset Pattern: Optimizing MongoDB Working Sets",
    "url": "/posts/subset-pattern/",
    "categories": "system design, database design",
    "tags": "system design, mongodb, database design, performance optimization",
    "date": "2024-07-27 07:00:00 +0700",
    





    
    "snippet": "Subset Pattern: Optimizing MongoDB Working SetsLarge documents with infrequently used data can cause working sets to exceed RAM, leading to performance issues.The Subset Pattern addresses this by s...",
    "content": "Subset Pattern: Optimizing MongoDB Working SetsLarge documents with infrequently used data can cause working sets to exceed RAM, leading to performance issues.The Subset Pattern addresses this by splitting frequently accessed data into a main collection and infrequently accessed data into a secondary collection.graph LR    subgraph dataAccess[\"üîÑ Data Access\"]        query[\"üîç Query\"]        mainData[\"üìä Main Data\"]        secondaryData[\"üìö Secondary Data\"]    end    query --&gt; |\"Frequent Access\"| mainData    query --&gt; |\"Occasional Access\"| secondaryDataSplit DataFor example, a product document may contain both product information and all reviews. By splitting the reviews into a separate collection, the main collection only contains the most recent reviews, reducing the working set size.graph LR    subgraph originalData[\"üìÑ Original Document\"]        productInfo[\"üëú Product Info\"]        allReviews[\"üìù All Reviews\"]    end    subgraph splitData[\"üîÄ Split Data\"]        subgraph mainCollection[\"üìä Main Collection\"]            productInfoSplit[\"üè∑Ô∏è Product Info\"]            recentReviews[\"üìù Recent Reviews\"]        end        subgraph secondaryCollection[\"üìö Secondary Collection\"]            oldReviews[\"üìú Old Reviews\"]        end    end    originalData --&gt; |\"Split\"| splitData    productInfo --&gt; productInfoSplit    allReviews --&gt; recentReviews    allReviews --&gt; oldReviews    style originalData fill:#eeeeee,stroke:#333,stroke-width:2px    style splitData fill:#eeeeee,stroke:#333,stroke-width:2px    style mainCollection fill:#dbf0fe,stroke:#333,stroke-width:2px    style secondaryCollection fill:#e6f3ff,stroke:#333,stroke-width:2px    style productInfo fill:#ffb3ba,stroke:#333,stroke-width:2px    style allReviews fill:#ffdfba,stroke:#333,stroke-width:2px    style productInfoSplit fill:#baffc9,stroke:#333,stroke-width:2px    style recentReviews fill:#bae1ff,stroke:#333,stroke-width:2px    style oldReviews fill:#ffffba,stroke:#333,stroke-width:2pxBefore{  \"_id\": ObjectId(\"507f1f77bcf86cd799439011\"),  \"name\": \"Super Widget\",  \"reviews\": [    // Potentially hundreds of reviews  ]}AfterProduct Collection:{  \"_id\": ObjectId(\"507f1f77bcf86cd799439011\"),  \"name\": \"Super Widget\",  \"reviews\": [     {       \"review_id\": 123,       \"review_text\": \"Great widget.\"       },      {        \"review_id\": 456,        \"review_text\": \"Awesome widget.\"     }  ]}Review Collection:[ {  \"review_id\": 786,  \"product_id\": ObjectId(\"507f1f77bcf86cd799439011\"),  \"review_text\": \"Amazing widget.\"},{  \"review_id\": 789,  \"product_id\": ObjectId(\"507f1f77bcf86cd799439011\"),  \"review_text\": \"Fantastic widget.\"}]Mongoose SchemaIf you‚Äôre using Mongoose with MongoDB, you can define separate schemas for the main collection and the secondary collection.const mongoose = require('mongoose');// Review Schemaconst reviewSchema = new mongoose.Schema({  author: String,  text: String,  rating: Number,  createdAt: { type: Date, default: Date.now }});// Product Schemaconst productSchema = new mongoose.Schema({  name: String,  description: String,  price: Number,  recentReviews: [reviewSchema]});// Separate Review Schemaconst separateReviewSchema = new mongoose.Schema({  productId: { type: mongoose.Schema.Types.ObjectId, ref: 'Product' },  author: String,  text: String,  rating: Number,  createdAt: { type: Date, default: Date.now }});const Product = mongoose.model('Product', productSchema);const SeparateReview = mongoose.model('SeparateReview', separateReviewSchema);Add and Get Reviews// Function to add a new reviewasync function addReview(productId, reviewData) {  const product = await Product.findById(productId);  if (!product) throw new Error('Product not found');  const newReview = new SeparateReview({    productId: product._id,    ...reviewData  });  await newReview.save();  product.recentReviews.push(reviewData);  if (product.recentReviews.length &gt; 10) {    product.recentReviews.shift();  }  await product.save();}// Function to get all reviews for a productasync function getAllReviews(productId) {  const recentReviews = await Product.findById(productId).select('recentReviews');  const oldReviews = await SeparateReview.find({ productId });  return [...recentReviews, ...oldReviews];}More Use Cases  Social Media Platforms:          User Posts: Maintain recent posts in user document, older posts in a separate collection.      Friends List: Store active friends in user document, inactive friends separately.        Content Streaming Platforms:          Video Comments: Keep recent comments with video data, older comments in a separate collection.      Watch History: Store recent history in user profile, older history in a separate collection.        IoT Systems:          Device Logs: Maintain recent logs with device data, archive older logs separately.      Sensor Data: Keep recent readings in device document, historical data in a separate collection.        Financial Systems          Transaction History: Store recent transactions in account document, older transactions separately.      Portfolio Performance: Keep current holdings and recent performance in main document, historical data separately.      Considerations  Data Consistency: Ensure consistency between main and secondary collections.  Query Complexity: Manage queries that span both collections.  Data Growth: Plan for data growth and archival strategies.SummaryThe Subset Pattern is a powerful technique for optimizing MongoDB performance by reducing working set size. It involves splitting large documents into frequently and infrequently accessed data, storing them in separate collections.Key benefits include:  Improved query performance  Reduced memory usage  Better scalability for large datasetsHowever, it comes with trade-offs such as increased complexity in data management and potential for additional queries.Rerferences  MongoDB: Subset Pattern"
  },
  
  {
    "title": "Computed Pattern - Efficiently Managing Data Computations",
    "url": "/posts/computed-pattern/",
    "categories": "system design, database design",
    "tags": "system design, mongodb, database design, performance optimization",
    "date": "2024-07-26 07:00:00 +0700",
    





    
    "snippet": "Computed PatternThe Computed Pattern involves precomputing values and storing the results to optimize performance and reduce CPU workload. This approach is particularly beneficial for applications ...",
    "content": "Computed PatternThe Computed Pattern involves precomputing values and storing the results to optimize performance and reduce CPU workload. This approach is particularly beneficial for applications with high read-to-write ratios, enhancing query speed and system scalability.graph LR    subgraph reviewData[\"üìä Review Data\"]        subgraph reviews[\" \"]         review1[\"‚≠ê Review 1: 5 stars\"]         review2[\"‚≠ê Review 2: 4 stars\"]         review3[\"‚≠ê Review 3: 3 stars\"]        end        style reviews stroke-width:0px    end    subgraph computationProcess[\"üßÆ Computation Process\"]        avgRating[\"fa:fa-calculator Calculate Average\"]        totalReviews[\"fa:fa-list-ol Count Reviews\"]    end    subgraph productData[\"üì¶ Product Data\"]        subgraph computed            storedAvg[\"fa:fa-star Avg Rating: 4\"]            storedCount[\"fa:fa-hashtag Review Count: 3\"]        end        style computed fill:#bff,stroke:#333,stroke-width:2px    end    reviewData --&gt;  computationProcess    avgRating --&gt; |\"Store\"| storedAvg    totalReviews --&gt; |\"Store\"| storedCountApplying the Computed PatternOriginal Data Structure:In an e-commerce platform, each product can receive multiple ratings from users. Storing and recalculating the average rating on-the-fly for each view can be resource-intensive.{  \"_id\": ObjectId(\"507f1f77bcf86cd799439011\"),  \"product_name\": \"Wireless Mouse\",  \"reviews\": [    {\"user\": \"user01\", \"rating\": 5, \"date\": ISODate(\"2024-08-01\")},    {\"user\": \"user02\", \"rating\": 4, \"date\": ISODate(\"2024-08-02\")},    {\"user\": \"user03\", \"rating\": 3, \"date\": ISODate(\"2024-08-03\")}  ]}Computed Data Structure:By applying the Computed Pattern, we precompute the average rating and total review count, storing them in the product document.graph TD    subgraph productDocument[\"üìÑ Product Document\"]        productInfo[\"fa:fa-info-circle Product Info\"]        computedData[\"fa:fa-cogs Computed Data\"]        reviewsList[\"fa:fa-list Reviews List\"]    end    productInfo --&gt; |\"contains\"| computedData    productInfo --&gt; |\"contains\"| reviewsList    computedData --&gt; avgRating[\"fa:fa-star Avg Rating: 4\"]    computedData --&gt; totalReviews[\"fa:fa-hashtag Total Reviews: 3\"]    reviewsList --&gt; review1[\"fa:fa-comment Review 1\"]    reviewsList --&gt; review2[\"fa:fa-comment Review 2\"]    reviewsList --&gt; review3[\"fa:fa-comment Review 3\"]   {  \"_id\": ObjectId(\"507f191e810c19729de860ea\"),  \"product_name\": \"Wireless Mouse\",  \"computed\": {    \"average_rating\": 4,    \"total_reviews\": 3,    \"last_updated\": ISODate(\"2024-08-04\")  },  \"reviews\": [    {\"user\": \"user01\", \"rating\": 5, \"date\": ISODate(\"2024-08-01\")},    {\"user\": \"user02\", \"rating\": 4, \"date\": ISODate(\"2024-08-02\")},    {\"user\": \"user03\", \"rating\": 3, \"date\": ISODate(\"2024-08-03\")}  ]}Benefits of the Computed Pattern  Optimized Performance: Precomputing values reduces the load on the CPU, leading to faster queries and better overall performance.  Scalability: Efficiently managing computation ensures the system can scale without significant performance degradation.  Flexibility: Aggregated data can be recomputed as needed without altering the source data.  Real-time Analytics: Enables quick access to important metrics without complex aggregation queries.Mongoose SchemaIf you‚Äôre using Mongoose with Node.js, you can define the schema as follows:const mongoose = require('mongoose');const { Schema } = mongoose;const reviewSchema = new Schema({  user: { type: String, required: true },  rating: { type: Number, required: true, min: 1, max: 5 },  date: { type: Date, default: Date.now }});const productSchema = new Schema({  product_name: {    type: String,    required: true,    index: true  },  computed: {    average_rating: {      type: Number,      default: 0,      min: 0,      max: 5    },    total_reviews: {      type: Number,      default: 0,      min: 0    },    last_updated: {      type: Date,      default: Date.now    }  },  reviews: [reviewSchema]});productSchema.index({ 'computed.average_rating': -1 });const Product = mongoose.model('Product', productSchema);module.exports = { Product };Computing Aggregated DataTo compute and update the average rating for a product:async function updateProductComputedData(productId) {  const product = await Product.findById(productId);  if (!product) {    throw new Error('Product not found');  }  const totalRating = product.reviews.reduce((sum, review) =&gt; sum + review.rating, 0);  const totalReviews = product.reviews.length;  product.computed = {    average_rating: totalReviews &gt; 0 ? totalRating / totalReviews : 0,    total_reviews: totalReviews,    last_updated: new Date()  };  await product.save();  return product;}// Usagetry {  const updatedProduct = await updateProductComputedData('507f191e810c19729de860ea');  console.log('Updated product:', updatedProduct);} catch (error) {  console.error('Error updating product:', error);}More Use CasesSocial Media AnalyticsIn social media platforms, precomputing engagement metrics can significantly improve performance.{  \"_id\": ObjectId(\"507f191e810c19729de860eb\"),  \"post_content\": \"Check out this amazing sunset!\",  \"author\": \"user123\",  \"created_at\": ISODate(\"2024-08-01T18:30:00Z\"),  \"computed\": {    \"likes_count\": 1542,    \"comments_count\": 89,    \"shares_count\": 256,    \"last_updated\": ISODate(\"2024-08-04T12:00:00Z\")  }}Content Recommendation SystemsFor content platforms, precomputing user preferences and content popularity can enhance recommendation speed.{  \"_id\": ObjectId(\"507f191e810c19729de860ed\"),  \"user_id\": \"user456\",  \"computed\": {    \"favorite_genres\": [\"Sci-Fi\", \"Thriller\", \"Documentary\"],    \"average_watch_time\": 65, // in minutes    \"content_completion_rate\": 0.78,    \"recommended_content_ids\": [\"movie123\", \"series456\", \"docu789\"],    \"last_updated\": ISODate(\"2024-08-04T20:15:00Z\")  }}Financial Portfolio AnalysisFor financial applications, precomputing portfolio performance metrics can provide quick insights.{  \"_id\": ObjectId(\"507f191e810c19729de860ee\"),  \"portfolio_id\": \"port789\",  \"user_id\": \"investor101\",  \"computed\": {    \"total_value\": 250000.00,    \"day_change_percentage\": 0.025,    \"year_to_date_return\": 0.11,    \"risk_score\": 7,    \"sector_diversity_score\": 0.8,    \"last_updated\": ISODate(\"2024-08-04T22:00:00Z\")  },  \"holdings\": [    {\"stock\": \"AAPL\", \"quantity\": 100, \"purchase_price\": 150.00},    {\"stock\": \"GOOGL\", \"quantity\": 50, \"purchase_price\": 2000.00},    // ... more holdings  ]}IoT Device MonitoringFor IoT applications, precomputing device health and performance metrics can aid in proactive maintenance.{  \"_id\": ObjectId(\"507f191e810c19729de860ef\"),  \"device_id\": \"smartlock001\",  \"type\": \"Smart Lock\",  \"computed\": {    \"battery_level\": 0.75,    \"avg_daily_operations\": 24,    \"days_since_last_maintenance\": 45,    \"firmware_up_to_date\": true,    \"failure_probability\": 0.02,    \"last_updated\": ISODate(\"2024-08-04T23:45:00Z\")  },  \"operation_log\": [    {\"timestamp\": ISODate(\"2024-08-04T23:30:12Z\"), \"action\": \"unlock\", \"user\": \"resident1\"},    {\"timestamp\": ISODate(\"2024-08-04T23:40:18Z\"), \"action\": \"lock\", \"user\": \"resident1\"},    // ... more logs  ]}Considerations  Application Logic: Ensure that your application logic accounts for the timing and frequency of computations. Consider using database triggers or scheduled jobs for updates.  Data Consistency: Maintain consistency between source data and computed results. Implement error handling and retry mechanisms for failed updates.SummaryThe Computed Pattern efficiently handles repeated data computations by precomputing values and storing the results. This approach optimizes performance and scalability, making it ideal for applications with high read-to-write ratios and the need for quick access to aggregated data.Keywords To Remembergraph LR    preCalculation[\"fa:fa-calculator Pre-Calculation\"] --&gt; computedPattern[\"üñ•Ô∏è  Computed Pattern\"]    timeSeriesData[\"fa:fa-clock Time Series Data\"] --&gt; bucketPattern[\"fa:fa-bucket Bucket Pattern\"]    overflow[\"fa:fa-exclamation-circle Overflow\"] --&gt; outlierPattern[\"fa:fa-exclamation-triangle Outlier Pattern\"]    keyValue[\"fa:fa-key Key-Value\"] --&gt; attributePattern[\"fa:fa-tags Attribute Pattern\"]References  MongoDB Computed Pattern"
  },
  
  {
    "title": "Outlier Pattern - Handling Viral Posts in Social Media Platforms",
    "url": "/posts/outlier-pattern/",
    "categories": "system design, database design",
    "tags": "system design, mongodb, database design",
    "date": "2024-07-25 07:00:00 +0700",
    





    
    "snippet": "Outlier PatternImagine you‚Äôre building a social media platform where users can create posts, and others can like, comment, or share those posts. In most cases, the number of interactions per post i...",
    "content": "Outlier PatternImagine you‚Äôre building a social media platform where users can create posts, and others can like, comment, or share those posts. In most cases, the number of interactions per post is manageable. However, viral posts can accumulate massive amounts of likes, comments, and shares, potentially exceeding the document size limit or impacting query performance.graph TD    subgraph originalData[\"üìÅ Original Data Structure\"]        post[\"üêà Post: Cute Cat Video\"]        likes1[\"üëç Likes: user00, user01, ..., user999\"]    end    post --&gt; likes1Applying the Outlier PatternOriginal Data Structure:Most posts have a reasonable number of interactions stored directly within the post document.{  \"_id\": ObjectId(\"507f1f77bcf86cd799439011\"),  \"content\": \"Check out my new blog post!\",  \"likes\": [\"user00\", \"user01\", \"user02\"],   \"comments\": [    { \"user\": \"user03\", \"text\": \"Great post!\" },    { \"user\": \"user04\", \"text\": \"Thanks for sharing!\" }  ],   \"shares\": [\"user05\"]}Outlier Post:When a post goes viral and accumulates a large number of interactions, you create an ‚Äúoverflow‚Äù document to store the excess data.graph TD    subgraph originalData[\"üìÅ Original Data Structure\"]        post[\"fa:fa-file Post: Cute Cat Video\"]        likes1[\"fa:fa-thumbs-up Likes: user00, user01, ..., user999\"]        hasExtras[\"‚ö†Ô∏è Has Overflow: true\"]        style post fill:#f9,stroke:#333,stroke-width:4px        style likes1 fill:#bbf,stroke:#333,stroke-width:2px        style hasExtras fill:#faa,stroke:#333,stroke-width:2px    end    post --&gt; |contains|likes1    post --&gt; |indicates overflow|hasExtras    subgraph overflowData[\"üìÅ Overflow Data\"]        postID[\"fa:fa-id-card Post ID: 507f191e810c19729de860ea\"]        likes2[\"fa:fa-thumbs-up Overflow Likes: user1000, user1001, ...\"]        style postID fill:#f9,stroke:#333,stroke-width:4px        style likes2 fill:#bbf,stroke:#333,stroke-width:2px    end    postID --&gt; |links to|likes2// Main post document{  \"_id\": ObjectId(\"507f191e810c19729de860ea\"),  \"content\": \"This cute cat video is going viral!\",  \"likes\": [\"user00\", \"user01\", ..., \"user999\"],  \"has_overflow\": true}// Overflow document{  \"_id\": ObjectId(\"507f191e810c19729de860eb\"),   \"post_id\": ObjectId(\"507f191e810c19729de860ea\"),   \"likes\": [\"user1000\", \"user1001\", ...],  \"comments\": [    { \"user\": \"user1002\", \"text\": \"Adorable!\" },    { \"user\": \"user1003\", \"text\": \"Made my day!\" }  ]}classDiagram     Post \"1\" --&gt; \"0..1\" Overflow    class Post{        ObjectId _id        String content        String[] likes        Object[] comments        String[] shares        Boolean has_overflow    }    class Overflow{        ObjectId _id        ObjectId post_id        String[] likes        Object[] comments        String[] shares    }Benefits of the Outlier Pattern  Optimized Performance: Most posts are stored efficiently, leading to faster queries and better overall performance.  Scalability: Viral posts won‚Äôt hinder the system‚Äôs scalability.  Flexibility: You can easily add overflow documents for other types of interactions (comments, shares) as needed.Mongoose SchemaHere‚Äôs an improved Mongoose schema for implementing the Outlier Pattern:const mongoose = require('mongoose');const { Schema } = mongoose;const commentSchema = new Schema({  user: { type: String, required: true },  text: { type: String, required: true },  createdAt: { type: Date, default: Date.now }});const overflowSchema = new Schema({  post_id: {    type: Schema.Types.ObjectId,    required: true,    ref: 'Post'  },  likes: [String],  comments: [commentSchema],  shares: [String]});const postSchema = new Schema({  content: {    type: String,    required: true  },  likes: [String],  comments: [commentSchema],  shares: [String],  has_overflow: {    type: Boolean,    default: false  }});const Post = mongoose.model('Post', postSchema);const Overflow = mongoose.model('Overflow', overflowSchema);module.exports = { Post, Overflow };Retrieving All LikesTo retrieve all likes for a post, including any overflow likes:async function getAllLikes(postId) {  const post = await Post.findById(postId);  if (!post) {    throw new Error('Post not found');  }  let allLikes = [...post.likes];  if (post.has_overflow) {    const overflow = await Overflow.findOne({ post_id: postId });    if (overflow &amp;&amp; overflow.likes) {      allLikes = allLikes.concat(overflow.likes);    }  }  return allLikes;}Adding a New LikeHere‚Äôs an improved version of the function to add a new like to a post:const LIKES_THRESHOLD = 1000;async function addLike(postId, userId) {  const session = await mongoose.startSession();  session.startTransaction();  try {    const post = await Post.findById(postId).session(session);    if (!post) {      throw new Error('Post not found');    }    if (post.likes.includes(userId) || (post.has_overflow &amp;&amp; await userLikedInOverflow(postId, userId, session))) {      throw new Error('User has already liked this post');    }    if (post.has_overflow) {      await addToOverflow(postId, 'likes', userId, session);    } else if (post.likes.length &gt;= LIKES_THRESHOLD) {      await createOverflowDocument(post, 'likes', userId, session);    } else {      post.likes.push(userId);      await post.save({ session });    }    await session.commitTransaction();    return post;  } catch (error) {    await session.abortTransaction();    throw error;  } finally {    session.endSession();  }}async function userLikedInOverflow(postId, userId, session) {  const overflow = await Overflow.findOne({ post_id: postId }).session(session);  return overflow &amp;&amp; overflow.likes.includes(userId);}async function addToOverflow(postId, field, value, session) {  await Overflow.updateOne(    { post_id: postId },    { $addToSet: { [field]: value } },    { session }  );}async function createOverflowDocument(post, field, value, session) {  post.has_overflow = true;  await Overflow.create([{    post_id: post._id,    [field]: [value]  }], { session });  await post.save({ session });}More Use Cases  Comments: Implement a similar approach for comments, creating overflow documents when the number of comments exceeds a certain threshold.  IOT sensor data: In IoT systems, most sensors might report data within expected ranges. However, during anomalies or critical events, certain sensors might generate an unusually high volume of data points. The Outlier Pattern can help manage these spikes without affecting the overall system performance.  Transaction History: In financial systems, most users might have a limited number of transactions. However, high-frequency traders or large corporations might generate a massive number of transactions. By separating these outliers into overflow documents, you can maintain optimal performance for the majority of users.Considerations  Application Logic: Your application must handle checking for overflow documents and retrieving additional data when needed.  Data Consistency: Ensure consistency between the main post document and its overflow documents, especially during concurrent operations.  Query Complexity: Retrieving complete data may require multiple queries, potentially impacting read performance.  Indexing Strategy: Carefully consider indexing on both main and overflow collections to optimize query performance.  Handling Extreme Outliers : We can create multiple levels of overflow documents, shard the overflow collection.SummaryThe Outlier Pattern efficiently handles viral posts or other data outliers by storing excess data in separate overflow documents. This approach ensures optimal performance for the majority of data while accommodating extreme outliers. By carefully managing the transition between main and overflow documents, you can maintain data consistency and query performance across the system.References  MongoDB Outlier Pattern"
  },
  
  {
    "title": "Attribute Pattern - Efficiently Querying Similar Fields",
    "url": "/posts/attribute-pattern/",
    "categories": "system design, database design",
    "tags": "system design, mongodb, database design",
    "date": "2024-07-23 07:00:00 +0700",
    





    
    "snippet": "The Attribute Pattern is especially useful in the following scenarios:  When dealing with large documents that have numerous similar fields, but a subset of these fields shares common characteristi...",
    "content": "The Attribute Pattern is especially useful in the following scenarios:  When dealing with large documents that have numerous similar fields, but a subset of these fields shares common characteristics, and we need to sort or query this subset.      When the fields required for sorting are present only in a small subset of documents.    When both of the above conditions apply to the documents.To optimize our search performance, we might need multiple indexes for all subsets. However, creating many indexes can degrade performance. The Attribute Pattern offers an efficient solution for these scenarios.graph LR  keyValPairs[\"fa:fa-list Key-Value Pairs\"]   rareFields[\"fa:fa-gem Rare Fields\"]  commonCharacteristics[\"fa:fa-cubes Common Characteristics\"]  sortQuery[\"fa:fa-search Sort/Query\"]  keyValPairs --&gt; sortQuery  rareFields --&gt; keyValPairs  commonCharacteristics --&gt; keyValPairsOriginal Data StructureLet‚Äôs consider a collection of products. Each document likely has fields such as product ID, name, base price, and various attributes such as color and storage options. If we want to search by attribute, the challenge is determining which attribute to use since products often have different variants.{  \"product_id\": 12345,  \"name\": \"Smartphone\",  \"base_price\": 699.99,  \"color_variant_red\": 749.99,  \"color_variant_blue\": 739.99,  \"color_variant_green\": 729.99,  \"storage_64gb\": 699.99,  \"storage_128gb\": 799.99,  \"storage_256gb\": 899.99}Searching for a specific attribute would require multiple indexes:{ \"color_variant_red\": 1 }{ \"color_variant_blue\": 1 }{ \"color_variant_green\": 1 }{ \"storage_64gb\": 1 }{ \"storage_128gb\": 1 }{ \"storage_256gb\": 1 }Improved Data Structure with the Attribute PatternBy using the Attribute Pattern, we move the subset of attributes into an array of key-value pairs:{  \"product_id\": 12345,  \"name\": \"Smartphone\",  \"base_price\": 699.99,  \"variants\": [    { \"type\": \"color\", \"value\": \"red\", \"price\": 749.99 },    { \"type\": \"color\", \"value\": \"blue\", \"price\": 739.99 },    { \"type\": \"color\", \"value\": \"green\", \"price\": 729.99 },    { \"type\": \"storage\", \"value\": \"64gb\", \"price\": 699.99 },    { \"type\": \"storage\", \"value\": \"128gb\", \"price\": 799.99 },    { \"type\": \"storage\", \"value\": \"256gb\", \"price\": 899.99 }  ]}Now, indexing becomes more manageable:{ \"variants.type\": 1, \"variants.value\": 1 }Handling Rare/Unpredictable FieldsIn cases where products have rare or unique attributes, such as a limited edition variant, the Attribute Pattern provides a flexible solution:{  \"product_id\": 12345,  \"name\": \"Smartphone\",  \"base_price\": 699.99,  \"limited_edition_gold_plated\": 1299.99}Improved Data Structure:{  \"product_id\": 12345,  \"name\": \"Smartphone\",  \"base_price\": 699.99,  \"variants\": [    { \"type\": \"limited_edition\", \"value\": \"gold_plated\", \"price\": 1299.99 }  ]}Mongoose SchemaIf you‚Äôre using Mongoose with Node.js, you can define the schema as follows:graph LR  variantSchema[\"fa:fa-list Variant Schema\"]   productSchema[\"fa:fa-cube Product Schema\"]  Product[\"fa:fa-database Product Model\"]  variantSchema --&gt; productSchema  productSchema --&gt; Productconst mongoose = require('mongoose');const { Schema } = mongoose;const variantSchema = new Schema({  type: {    type: String,    required: true,  },  value: {    type: String,    required: true,  },  price: {    type: Number,    required: true,  },});const productSchema = new Schema({  product_id: {    type: Number,    required: true,    index: true, // Indexing product_id for efficient queries  },  name: {    type: String,    required: true,  },  base_price: {    type: Number,    required: true,  },  variants: [variantSchema], // Array of variant sub-documents});const Product = mongoose.model('Product', productSchema);module.exports = Product;More Use Cases      User Profiles with Custom Fields:Social networks or professional platforms often allow users to add custom fields to their profiles. The Attribute Pattern can handle these efficiently:    {  \"user_id\": \"12345\",  \"name\": \"John Doe\",  \"email\": \"john@example.com\",  \"attributes\": [    { \"key\": \"skillset\", \"value\": \"JavaScript\" },    { \"key\": \"languages\", \"value\": \"English\" },    { \"key\": \"certifications\", \"value\": \"AWS Certified Developer\" }  ]}            Content Management Systems (CMS):CMS platforms often need to handle different types of content with varying metadata:    {  \"content_id\": \"67890\",  \"title\": \"10 Tips for Productive Remote Work\",  \"content_type\": \"article\",  \"metadata\": [    { \"key\": \"author\", \"value\": \"Jane Smith\" },    { \"key\": \"category\", \"value\": \"Productivity\" },    { \"key\": \"reading_time\", \"value\": \"5 minutes\" }  ]}            IoT Device Management:IoT devices can have various sensors and capabilities that can be represented using the Attribute Pattern:    {  \"device_id\": \"sensor-001\",  \"type\": \"environmental_sensor\",  \"location\": \"warehouse-a\",  \"sensors\": [    { \"type\": \"temperature\", \"unit\": \"celsius\", \"value\": 22.5 },    { \"type\": \"humidity\", \"unit\": \"percent\", \"value\": 45 },    { \"type\": \"air_quality\", \"unit\": \"aqi\", \"value\": 50 }  ]}      SummaryThe Attribute Pattern groups similar fields into key-value pairs, reducing the need for multiple indexes and simplifying queries. This approach improves performance and scalability, making it ideal for applications dealing with diverse and unpredictable data structures.References  MongoDB Attribute PatternKeywords To Remembergraph  keyValPairs[\"fa:fa-list Key-Value Pairs\"]  rareFields[\"fa:fa-gem Rare Fields\"] commonCharacteristics[\"fa:fa-cubes Common Characteristics\"] sortQuery[\"fa:fa-search Sort/Query\"]"
  },
  
  {
    "title": "Bucket Pattern - Time Series Data",
    "url": "/posts/bucket-pattern/",
    "categories": "system design, database design",
    "tags": "system design, mongodb, database design",
    "date": "2024-07-23 07:00:00 +0700",
    





    
    "snippet": "The Bucket PatternWhen dealing with time series data, storing each measurement in its own document can lead to issues with data and index size as the application scales. For instance, a system reco...",
    "content": "The Bucket PatternWhen dealing with time series data, storing each measurement in its own document can lead to issues with data and index size as the application scales. For instance, a system recording heart rate several times a day would generate a new document for each measurement. This method requires indexing employee_id and timestamp for each entry.By using the Bucket Pattern, data is grouped into documents based on time spans, reducing the number of documents and simplifying queries. This pattern stores measurements from a specific period in a single document, which includes aggregated information such as the sum of heart rates and the number of measurements.Before and After Applying the Bucket PatternOriginal Data Stream{  \"employee_id\": 67890,  \"timestamp\": \"2023-07-01T08:00:00.000Z\",  \"heart_rate\": 72},{  \"employee_id\": 67890,  \"timestamp\": \"2023-07-01T12:00:00.000Z\",  \"heart_rate\": 75},{  \"employee_id\": 67890,  \"timestamp\": \"2023-07-01T16:00:00.000Z\",  \"heart_rate\": 70}graph     subgraph originalData[\"fa:fa-database Original Data Stream\"]        dataEntry1[\"fa:fa-file ID: 67890, Timestamp: 2023-07-01T08:00:00.000Z, Heart Rate: 72\"]        dataEntry2[\"fa:fa-file ID: 67890, Timestamp: 2023-07-01T12:00:00.000Z, Heart Rate: 75\"]        dataEntry3[\"fa:fa-file ID: 67890, Timestamp: 2023-07-01T16:00:00.000Z, Heart Rate: 70\"]    end    originalData --&gt; dataEntry1    originalData --&gt; dataEntry2    originalData --&gt; dataEntry3Data Stream with Bucket PatternBucketing the data by day: storing all measurements for July 1, 2023, in a single document.{  \"employee_id\": 67890,  \"date\": \"2023-07-01\",  \"measurements\": [    {      \"timestamp\": \"2023-07-01T08:00:00.000Z\",      \"heart_rate\": 72    },    {      \"timestamp\": \"2023-07-01T12:00:00.000Z\",      \"heart_rate\": 75    },    {      \"timestamp\": \"2023-07-01T16:00:00.000Z\",      \"heart_rate\": 70    }  ],  \"transaction_count\": 3,  \"sum_heart_rate\": 217}graph TD    subgraph bucketedData[\"fa:fa-archive Bucketed Data Stream\"]        subgraph bucket1[\"ID: 67890, July 1, 2023\"]            bucketEntry1[\"fa:fa-file 08:00, Heart Rate: 72\"]            bucketEntry2[\"fa:fa-file 12:00, Heart Rate: 75\"]            bucketEntry3[\"fa:fa-file 16:00, Heart Rate: 70\"]        end        subgraph bucket2[\"ID: 67890, July 2, 2023\"]            bucketEntry4[\"fa:fa-file 08:00, Heart Rate: 71\"]            bucketEntry5[\"fa:fa-file 12:00, Heart Rate: 74\"]            bucketEntry6[\"fa:fa-file 16:00, Heart Rate: 73\"]        end        subgraph bucket3[\"ID: 12345, July 1, 2023\"]            bucketEntry7[\"fa:fa-file 08:00, Heart Rate: 65\"]            bucketEntry8[\"fa:fa-file 12:00, Heart Rate: 67\"]            bucketEntry9[\"fa:fa-file 16:00, Heart Rate: 66\"]        end    end    style bucket1 fill:#bee,stroke:#333,stroke-width:4px    style bucket2 fill:#bee,stroke:#333,stroke-width:4px    style bucket3 fill:#bee,stroke:#333,stroke-width:4pxBenefits of the Bucket Pattern  Index Size Savings: Reduces the number of documents and indexes required.  Query Simplification: Pre-aggregated data simplifies queries.  Efficient Data Storage: Moves old data to archives to reduce load.  Easy Aggregation: Quickly calculates metrics like average heart rate from aggregated data.By applying the Bucket Pattern, data for specific periods can be efficiently stored, queried, and archived, improving performance and scalability.Example: Calculating Average Heart RateWith pre-aggregated sum_heart_rate and transaction_count, the average heart rate can be easily calculated:const averageHeartRate = sum_heart_rate / transaction_count;By storing the sum and the count of the heart rate measurements, you can easily calculate the average heart rate for any desired time period. This approach allows you to perform additional calculations, such as summing heart rates across multiple buckets.Archiving DataOld buckets can be moved to an archive, reducing the active data load. For example, heart rate data from several years ago can be archived since it is less frequently accessed.graph     subgraph activeData[\"fa:fa-archive Active Data\"]        activeBucket1[\"ID: 67890, July 2023\"]        activeBucket2[\"ID: 67890, August 2023\"]        activeBucket3[\"ID: 12345, July 2023\"]    end    subgraph archiveData[\"fa:fa-archive Archive Data\"]        archiveBucket1[\"ID: 67890, 2018\"]        archiveBucket2[\"ID: 12345, 2018\"]    end    activeData --&gt; activeBucket1    activeData --&gt; activeBucket2    activeData --&gt; activeBucket3    archiveData --&gt; archiveBucket1    archiveData --&gt; archiveBucket2Mongoose SchemaIf you are using MongoDB with Mongoose, you can define a schema for the bucket pattern as follows:const mongoose = require('mongoose');const { Schema } = mongoose;const measurementSchema = new Schema({  timestamp: {    type: Date,    required: true,  },  heart_rate: {    type: Number,    required: true,  },});const bucketSchema = new Schema({  employee_id: {    type: Number,    required: true,    index: true, // Indexing employee_id for efficient queries  },  date: {    type: Date,    required: true,    index: true, // Indexing date for efficient queries  },  measurements: [measurementSchema], // Array of measurement sub-documents  transaction_count: {    type: Number,    default: 0,  },  sum_heart_rate: {    type: Number,    default: 0,  },});const Bucket = mongoose.model('Bucket', bucketSchema);module.exports = Bucket;More Use Cases  Financial Transactions: Storing daily transaction data in buckets for each account.  Sensor Readings: Grouping sensor readings by hour or day for analysis.  Website Traffic: Aggregating website traffic data by day or week for reporting.  Stock Prices: Storing stock price data in buckets for each trading day.  Weather Data: Grouping weather data by day or week for historical analysis.Considerations  Bucket Size: Determine the optimal bucket size based on the frequency of data collection and the desired query performance. For example, buckets could be hourly, daily, or weekly.  Data Types: While the example uses heart rate data, it would be helpful to mention that the bucket pattern can be applied to various types of time series data (e.g., financial transactions, sensor readings).  Time Series Databases: Consider using time series databases for optimized storage and querying of time series data.SummaryThe Bucket Pattern groups time series data into larger documents, reducing the number of documents and simplifying queries. This approach provides significant benefits in terms of performance, scalability, and efficient data storage, especially for applications dealing with large volumes of time series data.References  Mongodb Bucket PatternKeywords To Remembergraph  subgraph          aggregation[\"üìä Aggregation\"]        archive[\"üóÑÔ∏è Archive\"]        index[\"üîç Index\"]     end    subgraph          timeSeriesData[\"‚è≤Ô∏è Time Series Data\"]        bucket[\"ü™£ Bucket\"]    end    "
  },
  
  {
    "title": "Microservices, Modular Monoliths, or Distributed Monoliths",
    "url": "/posts/modular-monolith/",
    "categories": "system design",
    "tags": "system design",
    "date": "2024-07-13 07:00:00 +0700",
    





    
    "snippet": "The architecture landscape is varied, offering a multitude of options to developers. Among the most popular choices are microservices, modular monoliths, and distributed monoliths. Each architectur...",
    "content": "The architecture landscape is varied, offering a multitude of options to developers. Among the most popular choices are microservices, modular monoliths, and distributed monoliths. Each architecture has its own strengths and weaknesses, making it crucial to understand the differences between them to make an informed decision.graph LR  subgraph      microservices[\"fa:fa-cubes Microservices\"]    modularMonolith[\"fa:fa-layer-group Modular Monolith\"]    distributedMonolith[\"fa:fa-network-wired Distributed Monolith\"]   endModular MonolithsModular monoliths are a hybrid approach that combines the benefits of monolithic architecture with modular design. The application is built as a single unit but organized into distinct modules, allowing for better separation of concerns and maintainability.graph LR  modularMonolith[\"fa:fa-layer-group Modular Monolith\"] --&gt; moduleOne[\"üü¶ Module 1\"]  modularMonolith --&gt; moduleTwo[\"üü¶ Module 2\"]  modularMonolith --&gt; moduleThree[\"üü¶ Module 3\"]graph LR    subgraph deployment[\"Deployment\"]    eCommerceMonolith[\"fa:fa-box E-commerce Monolith\"]    end    subgraph dataStore[\"Data Store\"]    monolithDatabase[\"fa:fa-database Monolith Database\"]    end    subgraph modules[\"Modules\"]    productCatalogModule[\"fa:fa-folder Product Catalog Module\"]    orderModule[\"fa:fa-folder Order Module\"]    paymentModule[\"fa:fa-folder Payment Module\"]    userModule[\"fa:fa-folder User Module\"]    end    eCommerceMonolith --&gt; |read/write| monolithDatabase    productCatalogModule -.-&gt; eCommerceMonolith    orderModule -.-&gt; eCommerceMonolith    paymentModule -.-&gt; eCommerceMonolith    userModule -.-&gt; eCommerceMonolith    style eCommerceMonolith fill:#add8e6,stroke:#000    style monolithDatabase fill:#add8e6,stroke:#000    style productCatalogModule fill:#c7f0f2,stroke:#000    style orderModule fill:#fff2cc,stroke:#000    style paymentModule fill:#d9ead3,stroke:#000    style userModule fill:#f4cccc,stroke:#000    style deployment fill:#f0f0f0,stroke:#000    style dataStore fill:#f0f0f0,stroke:#000    style modules fill:#f0f0f0,stroke:#000  Modules are organized by domain or feature, with clear boundaries and interfaces.  Shared database for all modules, but with well-defined access patterns.  Changes to one module can be deployed independently, but the entire application is deployed as a single unit.  Communication between modules is direct (function calls, shared memory).Distributed MonolithsDistributed monoliths are a variation of monolithic architecture where the application is split into multiple components that communicate over a network. While they offer some benefits of distributed systems, they can still suffer from the drawbacks of monolithic architecture.graph LR  distributedMonolith[\"fa:fa-network-wired Distributed Monolith\"] --&gt; serviceA[\"fa:fa-server Service A\"]  distributedMonolith --&gt; serviceB[\"fa:fa-server Service B\"]  serviceA --&gt; serviceB  serviceB --&gt; serviceAgraph LR    subgraph deployment[\"Deployment\"]    productCatalogService[\"fa:fa-box Product Catalog Service\"]    orderService[\"fa:fa-box Order Service\"]    paymentService[\"fa:fa-box Payment Service\"]    userService[\"fa:fa-box User Service\"]    end    subgraph dataStore[\"Data Store\"]    sharedDatabase[\"fa:fa-database Shared Database\"]    end    productCatalogService --&gt;|read/write product data|sharedDatabase    orderService --&gt;|read/write order data|sharedDatabase    paymentService --&gt;|read/write payment data|sharedDatabase    userService --&gt;|read/write user data|sharedDatabase    orderService --&gt;|process payment|paymentService    orderService --&gt;|get user info|userService    orderService --&gt;|get product info|productCatalogService    style productCatalogService fill:#c7f0f2,stroke:#000    style orderService fill:#fff2cc,stroke:#000    style paymentService fill:#d9ead3,stroke:#000    style userService fill:#f4cccc,stroke:#000    style sharedDatabase fill:#d5a6bd,stroke:#000     style deployment fill:#dae3f3,stroke:#000    style dataStore fill:#dae3f3,stroke:#000  They share a single database (e.g., one big database for products, orders, users, and payments).  They communicate through synchronous calls (e.g., the Order Service directly calls the Payment Service to process a payment).  Changes to one service often require coordinated deployments of other services.MicroservicesMicroservices are a distributed architecture composed of small, independent services, each responsible for a specific function. These services communicate over a network, enabling flexibility, scalability, and resilience.graph LR  microservices[\"fa:fa-cubes Microservices\"] --&gt; serviceOne[\"fa:fa-server Service 1\"]  microservices --&gt; serviceTwo[\"fa:fa-server Service 2\"]  microservices --&gt; serviceThree[\"fa:fa-server Service 3\"]graph LR    subgraph external[\"External\"]        client[\"fa:fa-user Client\"]    end    subgraph deployment[\"Deployment\"]        apiGateway[\"fa:fa-door-open API Gateway\"]        productCatalogService[\"fa:fa-box Product Catalog Service\"]        orderService[\"fa:fa-box Order Service\"]        paymentService[\"fa:fa-box Payment Service\"]        userService[\"fa:fa-box User Service\"]    end    subgraph dataStore[\"Data Store\"]        productCatalogDB[\"fa:fa-database Product Catalog DB\"]        orderDB[\"fa:fa-database Order DB\"]        paymentDB[\"fa:fa-database Payment DB\"]        userDB[\"fa:fa-database User DB\"]    end    subgraph messageQueue[\"Message Queue\"]        messageBroker[\"fa:fa-exchange Message Broker\"]    end        client --&gt;|API requests|apiGateway    apiGateway --&gt;|route requests|productCatalogService    apiGateway --&gt;|route requests|orderService    apiGateway --&gt;|route requests|paymentService    apiGateway --&gt;|route requests|userService    productCatalogService --&gt;|read/write|productCatalogDB    orderService --&gt;|read/write|orderDB    paymentService --&gt;|read/write|paymentDB    userService --&gt;|read/write|userDB    orderService -.-&gt;|publish event|messageBroker    paymentService -.-&gt;|consume event|messageBroker    orderService -.-&gt;|get user info|userService    orderService -.-&gt;|get product info|productCatalogService    style apiGateway fill:#e6b8af,stroke:#000    style productCatalogService fill:#c7f0f2,stroke:#000    style orderService fill:#fff2cc,stroke:#000    style paymentService fill:#d9ead3,stroke:#000    style userService fill:#f4cccc,stroke:#000    style productCatalogDB fill:#c7f0f2,stroke:#000    style orderDB fill:#fff2cc,stroke:#000    style paymentDB fill:#d9ead3,stroke:#000    style userDB fill:#f4cccc,stroke:#000    style messageBroker fill:#c5cae9,stroke:#000    style deployment fill:#f0f0f0,stroke:#000    style dataStore fill:#f0f0f0,stroke:#000    style messageQueue fill:#f0f0f0,stroke:#000    style client fill:#b7b7b7,stroke:#000    style external fill:#f0f0f0,stroke:#000  Each service has its own dedicated database (or can share with related services, but with clear boundaries)  Asynchronous communication (message queues, event-driven) preferred, but can use synchronous when necessary with careful designScalabilityMicroservices excel in scalability. Individual services can be scaled up or down as demand dictates, without impacting the rest of the system. Modular monoliths, conversely, necessitate scaling the entire application. Distributed monoliths offer some scalability, but interdependencies can limit it.graph LR  microservices[\"fa:fa-cubes Microservices\"] --&gt; independentScaling[\"fa:fa-expand-arrows-alt Independent Scaling\"]  modularMonolith[\"fa:fa-layer-group Modular Monolith\"] --&gt; wholeAppScaling[\"fa:fa-expand Whole Application Scaling\"]  distributedMonolith[\"fa:fa-network-wired Distributed Monolith\"] --&gt; limitedScaling[\"fa:fa-expand Limited Scaling\"]Technology FlexibilityMicroservices embrace technological diversity. Each service can be built using the technology stack best suited for its specific function. In contrast, modular and distributed monoliths typically rely on a unified tech stack across the entire application.graph LR  microservices[\"fa:fa-cubes Microservices\"] --&gt; diverseTech[\"fa:fa-code Diverse Technologies\"]  modularMonolith[\"fa:fa-layer-group Modular Monolith\"] --&gt; singleTech[\"fa:fa-code Single Technology\"]  distributedMonolith[\"fa:fa-network-wired Distributed Monolith\"] --&gt; singleTechDeploymentMicroservices enable independent deployment of services, allowing for faster release cycles and reduced risk. Modular monoliths require deploying the entire application, while distributed monoliths face similar challenges due to shared codebases and dependencies.graph LR  microservices[\"fa:fa-cubes Microservices\"] --&gt; independentDeployment[\"fa:fa-rocket Independent Deployment\"]  modularMonolith[\"fa:fa-layer-group Modular Monolith\"] --&gt; wholeAppDeployment[\"fa:fa-rocket Whole Application Deployment\"]  distributedMonolith[\"fa:fa-network-wired Distributed Monolith\"] --&gt; wholeAppDeploymentTeam AutonomyMicroservices are often associated with autonomous teams, where each team is responsible for a specific service. Modular monoliths and distributed monoliths typically involve coordinated teams working on different modules or components.graph LR  microservices[\"fa:fa-cubes Microservices\"] --&gt; autonomousTeams[\"fa:fa-users-cog Autonomous Teams\"]  modularMonolith[\"fa:fa-layer-group Modular Monolith\"] --&gt; coordinatedTeams[\"fa:fa-users Coordinated Teams\"]  distributedMonolith[\"fa:fa-network-wired Distributed Monolith\"] --&gt; coordinatedTeamsComplexityMicroservices introduce the intricacies of distributed systems‚Äîcommunication overhead, data consistency challenges, and increased operational complexity. Modular monoliths, with their unified codebase, tend to be simpler to manage. Distributed monoliths, while offering the illusion of separation, often grapple with the complexities of microservices without reaping the full benefits.graph LR  microservices[\"fa:fa-cubes Microservices\"] --&gt; distributedComplexity[\"fa:fa-wrench Distributed Complexity\"]  modularMonolith[\"fa:fa-layer-group Modular Monolith\"] --&gt; monolithicSimplicity[\"fa:fa-tools Monolithic Simplicity\"]  distributedMonolith[\"fa:fa-network-wired Distributed Monolith\"] --&gt; hiddenComplexity[\"fa:fa-question-circle Hidden Complexity\"]Error Isolation &amp; ResilienceIn a microservices architecture, if one service fails, it doesn‚Äôt necessarily bring down the entire system. Other services can continue to function. Modular monoliths are more vulnerable; an error in one module can impact the whole application. Distributed monoliths share this vulnerability due to their tight coupling.graph LR  microservices[\"fa:fa-cubes Microservices\"] --&gt; errorIsolation[\"fa:fa-shield-alt Error Isolation\"]  modularMonolith[\"fa:fa-layer-group Modular Monolith\"] --&gt; sharedRisk[\"fa:fa-exclamation-triangle Shared Risk\"]  distributedMonolith[\"fa:fa-network-wired Distributed Monolith\"] --&gt; sharedRiskData ConsistencyMicroservices face challenges in maintaining data consistency across services due to distributed transactions and eventual consistency patterns. Modular monoliths and distributed monoliths, with shared databases, can enforce consistency more easily but may face challenges in scaling and maintaining clear boundaries.graph LR  microservices[\"fa:fa-cubes Microservices\"] --&gt; eventualConsistency[\"‚ùÑÔ∏è  Eventual Consistency\"]  modularMonolith[\"fa:fa-layer-group Modular Monolith\"] --&gt; strongConsistency[\"ü™® Strong Consistency\"]  distributedMonolith[\"fa:fa-network-wired Distributed Monolith\"] --&gt; strongConsistencyHow to Choose the Right ArchitectureThere is no definitive ‚Äúbest‚Äù choice among microservices, modular monoliths, and distributed monoliths. The optimal architecture depends on your project‚Äôs specific requirements, team expertise, scalability needs, and long-term goals.graph LR  yourProject[\"fa:fa-project-diagram Your Project\"] --&gt; microservices[\"fa:fa-cubes Microservices\"]  yourProject --&gt; modularMonolith[\"fa:fa-layer-group Modular Monolith\"]  yourProject --&gt; distributedMonolith[\"fa:fa-network-wired Distributed Monolith\"]  Consider the Human Factor: Architecture decisions impact not just the code but also the people who work with it. Consider how the chosen architecture will affect your team‚Äôs collaboration, productivity, and satisfaction.  Start Small: Experiment with a small project or service to test the architecture‚Äôs suitability before committing to a larger implementation.  Iterate and Improve: Continuously evaluate and refine your architecture based on feedback and evolving project needs.  Stay Agile: Be prepared to adapt and evolve your architecture as your project evolves. Agile methodologies can help you respond to changing requirements and refine your architecture iteratively.Keywords To Remembergraph   subgraph Characteristics[\"‚ú® \"]    teamAutonomy[\"fa:fa-users\"]    scalability[\"fa:fa-expand-arrows-alt\"]    flexibility[\"fa:fa-code\"]    deployment[\"fa:fa-rocket\"]    coupling[\"üîó\"]  end  subgraph      microservices[\"fa:fa-cubes\"]    async[\"‚òï\"]    eventual-consistency[\"‚ùÑÔ∏è\"]  end  subgraph      distributedMonolith[\"fa:fa-network-wired\"]    sync[\"üïô\"]    strong-consistency[\"ü™®\"]  end  subgraph  [\" \"]    modularMonolith[\"fa:fa-layer-group\"]    single[\"fa:fa-1\"]  end"
  },
  
  {
    "title": "Pesimistic Locking And Optimistic Locking Patterns",
    "url": "/posts/pesimistic-locking-and-optimistic-locking-patterns/",
    "categories": "system design",
    "tags": "system design",
    "date": "2024-07-13 07:00:00 +0700",
    





    
    "snippet": "High Traffic E-Commerce Site: Handling Concurrent Requestsgraph LR    subgraph HighTraffic[\"üõçÔ∏è High Traffic Event\"]        MultipleUsers[\"üë• Multiple Users\"]        SimultaneousRequests[\"üîÉ Simultane...",
    "content": "High Traffic E-Commerce Site: Handling Concurrent Requestsgraph LR    subgraph HighTraffic[\"üõçÔ∏è High Traffic Event\"]        MultipleUsers[\"üë• Multiple Users\"]        SimultaneousRequests[\"üîÉ Simultaneous Requests\"]    end    subgraph Issues[\"‚ö†Ô∏è Issues\"]        RaceCondition[\"üèÅ Race Condition\"]        InventoryInaccuracy[\"‚ùå Inventory Inaccuracy\"]    endDuring high-traffic events like Black Friday sales, a large number of customers may attempt to purchase the same product simultaneously, leading to a potential race condition where multiple users could ‚Äúbuy‚Äù the last few items, resulting in overselling and inventory inconsistencies.Problems  Race Condition: Multiple users try to access and modify the same data (inventory count) simultaneously, leading to incorrect results.  Inventory Inaccuracy:  If not handled properly, more items could be ‚Äúsold‚Äù than are actually available.Pessimistic LockingThis pattern assumes conflicts are likely and takes a cautious approach:graph subgraph LockingMechanism[\"üîí Locking Mechanism\"]    AcquireLock[\"fa:fa-lock Acquire Lock\"]    CheckInventory[\"fa:fa-search Check Inventory\"]    DecrementInventory[\"fa:fa-minus Decrement Inventory\"]    ReleaseLock[\"fa:fa-unlock Release Lock\"]    TransactionComplete[\"fa:fa-check Transaction Complete\"]    InsufficientStock[\"fa:fa-ban Insufficient Stock\"]    style LockingMechanism fill:#f0f8ff,stroke:#007bffend    UserRequest[\"fa:fa-user User Request\"]    InventoryDatabase[\"fa:fa-database Inventory Database\"]    RequestQueue[\"fa:fa-list Request Queue\"]    style UserRequest fill:#90ee90, stroke:#228b22    style InventoryDatabase fill:#f08080, stroke:#8b0000    style RequestQueue fill:#add8e6, stroke:#00008bUserRequest --&gt;AcquireLockAcquireLock --&gt; |\"Success\"| CheckInventoryAcquireLock --&gt; |\"Failure\"| RequestQueueCheckInventory --&gt; |\"Sufficient\"| DecrementInventoryCheckInventory --&gt; |\"Insufficient\"| InsufficientStockDecrementInventory --&gt; TransactionCompleteInsufficientStock --&gt; ReleaseLockTransactionComplete --&gt; ReleaseLockReleaseLock --&gt; InventoryDatabase  Lock Acquisition: Before updating inventory, the system acquires an exclusive lock on the product record. If Lock Acquisition fails (due to another process holding the lock), the request can be queued or retried after a delay.  Transaction:          Check available inventory.      If sufficient, decrement inventory and complete the purchase.      If insufficient, release the lock and inform the user the item is out of stock.        Lock Release: After the transaction (successful or not), the lock is released, allowing other processes to access the product record.Balance between performance and consistencyUsing Pessimistic Locking for high concurrent traffic scenarios like Black Friday sales ensures data consistency but can introduce performance overhead during normal traffic conditions. To balance between performance and consistency, we can apply a hybrid approach using both Pessimistic and Optimistic Locking strategies based on traffic conditionsImplement a mechanism to monitor traffic and dynamically switch between Optimistic and Pessimistic Locking based on predefined thresholds.graph     subgraph lockingStrategies[\"fa:fa-shield-alt Locking Strategies\"]        pessimisticLocking[\"fa:fa-lock Pessimistic Locking\"]        optimisticLocking[\"fa:fa-unlock-alt Optimistic Locking\"]    end    subgraph pessimisticLockingFlow[\"fa:fa-face-frown Pessimistic Locking Flow\"]        pessimisticLocking --&gt; acquireLock[\"fa:fa-handcuffs Acquire Lock\"]        acquireLock --&gt; readAndModifyData[\"fa:fa-edit Read/Modify Data\"]        readAndModifyData --&gt; releaseLock[\"fa:fa-key Release Lock\"]        releaseLock --&gt; commitChanges[\"fa:fa-check Commit Changes\"]    end    subgraph optimisticLockingFlow[\"fa:fa-face-smile Optimistic Locking Flow\"]        optimisticLocking --&gt; readAndModifyData2[\"fa:fa-eye Read/Modify Data\"] --&gt; checkForConflicts[\"üî• Check for Conflicts\"]        checkForConflicts --&gt; |\"No Conflict\"| commitChanges2        checkForConflicts --&gt; |\"Conflict\"| resolveConflict[\"fa:fa-tools Resolve Conflict\"]        resolveConflict --&gt; commitChanges2[\"fa:fa-check Commit Changes\"]    end    style lockingStrategies fill:#f9f,stroke:#333,stroke-width:2px    style pessimisticLockingFlow fill:#9ff,stroke:#333,stroke-width:2px    style optimisticLockingFlow fill:#ff9,stroke:#333,stroke-width:2pxOptimistic LockingEach record has a version number. When updating, the application checks if the version matches the one it initially read. If it does, the update proceeds. If not, it signals a conflict, and the application can retry the operation.async function purchaseProduct(productId, quantity) {  const client = await MongoClient.connect(\"mongodb://your_db_connection_string\");  const db = client.db(\"your_database_name\");  const productsCollection = db.collection(\"products\");  try {    // 1. Read product data and version    const product = await productsCollection.findOne({ _id: productId });    const initialVersion = product.__v;    // 2. Modify data in memory (simulated purchase)    if (product.quantity &gt;= quantity) {      product.quantity -= quantity;    } else {      throw new Error(\"Insufficient stock\");    }    // 3. Attempt update with version check    const result = await productsCollection.updateOne(      { _id: productId, __v: initialVersion }, // Filter with initial version      { $set: { quantity: product.quantity }, $inc: { __v: 1 } } // Update quantity and increment version    );    if (result.modifiedCount === 1) {      return \"Purchase successful!\";    } else {      // Conflict detected, retry or handle gracefully      throw new Error(\"Item unavailable due to high demand. Please try again.\");    }  } catch (error) {    console.error(\"Error during purchase:\", error);   // Retry logic can be added here  } }Pessimistic locking prevents conflicts by locking data before modification, guaranteeing consistency but potentially slowing performance. Optimistic locking assumes conflicts are rare, allowing concurrent updates and checking for conflicts later, offering better performance but requiring conflict resolution mechanisms. Choose pessimistic locking when conflicts are frequent and data integrity is critical, and optimistic locking when conflicts are less likely and speed is prioritized.Keywords To Remembergraph  subgraph      concurrency[\"üõçÔ∏è\"]    race[\"\"fa:fa-motorcycle\"\"] end subgraph    pessimistic[\"fa:fa-face-frown\"]  Lock-Acquisition[\"fa:fa-lock\"]  unlock[\"fa:fa-key\"] end   subgraph    optimistic[\"fa:fa-face-smile\"]  conflict[\"üî•\"]   version[\"fa:fa-tag\"] end"
  },
  
  {
    "title": "Blockchain for dummies",
    "url": "/posts/blockchain-for-dummies/",
    "categories": "blockchain",
    "tags": "blockchain",
    "date": "2024-07-06 07:00:00 +0700",
    





    
    "snippet": "In this article, we will cover the basics of blockchain technology in a simple and easy-to-understand way. Each section start with a simple diagram to explain the concept and then followed by a bri...",
    "content": "In this article, we will cover the basics of blockchain technology in a simple and easy-to-understand way. Each section start with a simple diagram to explain the concept and then followed by a brief explanation. The good part of diagram is help you to understand the concept in a visual way and of course more questions will come to your mind, find the answers by yourself and making your own research is the best way to learn and memorize the concept.Transaction - Blockgraph LR    subgraph Keys[\"fa:fa-key Keys\"]        style Keys fill:#C2E0C6,stroke:#5FB483        PrivateKey[\"fa:fa-lock Private Key (Sender)\"]        PublicKey[\"fa:fa-unlock Public Key (Sender/Recipient)\"]    end    PrivateKey --\"fa:fa-pen Signs\"--&gt; TransactionSignature    PublicKey --\"fa:fa-check Verifies\"--&gt; TransactionSignature    subgraph Transaction[\"fa:fa-file-alt Transaction\"]      style Transaction fill:#E1D5E7,stroke:#9673A6      TransactionSignature[\"fa:fa-signature Digital Signature\"]      subgraph TransactionData[\"fa:fa-database Transaction Data\"]        style TransactionData fill:#FFF2CC,stroke:#D6B656        TransactionDetails[\"fa:fa-info-circle Transaction Details&lt;br/&gt;(Sender, Recipient, Amount)\"]    end    end    subgraph Verification[\"fa:fa-shield-alt Verification\"]      style Verification fill:#D4EDDA,stroke:#4CAF50      BlockchainNetwork[\"fa:fa-network-wired Blockchain Network\"]    end    Transaction --\"fa:fa-check Verified by\"--&gt; BlockchainNetwork    subgraph Block[\"fa:fa-cube Block (Example)\"]        style Block fill:#F0F0F0,stroke:#888888        BlockHeader[\"fa:fa-header Block Header\"]        BlockData[\"fa:fa-database Block Data (Multiple Transactions)\"]    end    Transaction -.-&gt; BlockData    BlockHeader --&gt; BlockHash[\"fa:fa-fingerprint Block Hash\"]    BlockHeader --&gt; PreviousBlockHash[\"fa:fa-link Previous Block's Hash\"]    BlockHeader --&gt; Nonce[\"fa:fa-random Nonce\"]      Transaction: A digital record of value transfer between two parties (sender and recipient) with a specific amount. It‚Äôs signed with the sender‚Äôs private key and verified by the network using their public key. In the case of deploying a smart contract, the transaction does not have a recipient address. ethereum‚Äôs doc        Block: A collection of verified transactions grouped together, along with a header containing metadata (e.g., timestamp, previous block‚Äôs hash).  Miner - Node - Walletgraph LR    subgraph Network[\"fa:fa-network-wired Blockchain Network\"]        style Network fill:#F2F2F2,stroke:#666666        Transactions[\"fa:fa-exchange-alt Transactions\"]        style Transactions fill:#E1D5E7,stroke:#9673A6        Blocks[\"fa:fa-cube Blocks\"]        style Blocks fill:#F0F0F0,stroke:#888888    end    subgraph Participants[\"fa:fa-users Participants\"]        style Participants fill:#F0F0F0,stroke:#888888        Wallet[\"fa:fa-wallet Wallet (User)\"]        style Wallet fill:#FFF2CC,stroke:#D6B656        Node[\"fa:fa-server Node\"]        style Node fill:#C2E0C6,stroke:#5FB483        Miner[\"fa:fa-user-shield Miner\"]        style Miner fill:#F5DEB3,stroke:#D2B48C    end        subgraph Wallet      PublicKey[\"fa:fa-key Public Key (Sender/Recipient)\"]      PrivateKey[\"fa:fa-lock Private Key (Sender)\"]    end    Wallet -- \"fa:fa-paper-plane Creates/Receives\" --&gt; Transactions    Node -- \"fa:fa-check Validates/Stores\" --&gt; Transactions    Node -- \"fa:fa-check Validates/Stores\" --&gt; Blocks    Miner -- \"fa:fa-hammer Creates/Validates\" --&gt; Blocks    Transactions -.-&gt; Blocks    PrivateKey -- \"fa:fa-pen Signs\" --&gt; Transactions    PublicKey -- \"fa:fa-check Verifies\" --&gt; Transactions    Miner -- \"fa:fa-coins Receives Rewards From\" --&gt; Blocks      Miner: Miners solve complex puzzles to add new blocks to the blockchain, ensuring its security and earning rewards in the process.        Node: Nodes maintain copies of the blockchain, validate transactions, and relay information across the network.        Wallet:  Wallets store users‚Äô private keys (used for signing transactions) and public keys (used for receiving transactions), allowing them to interact with the blockchain.  Mining Poolgraph TD    subgraph miners[\"fa:fa-users Miners\"]        miner1[\"fa:fa-user Miner 1\"]        style miner1 fill:#FFFACC,stroke:#FFD700        miner2[\"fa:fa-user Miner 2\"]        style miner2 fill:#FFFACD,stroke:#FFD700        miner3[\"fa:fa-user Miner 3\"]        style miner3 fill:#FFFACD,stroke:#FFD700    end        miner1 --&gt; |\"Computing Power\"|poolServer    miner2 --&gt; |\"Computing Power\"|poolServer    miner3 --&gt; |\"Computing Power\"|poolServer    poolServer[\"fa:fa-database Pool Server\"]    style poolServer fill:#AFEEEE,stroke:#40E0D0,stroke-width:2px    poolServer --&gt; |\"Solution Found\"| blockchain[\"fa:fa-link Blockchain\"]    style blockchain fill:#98FB98,stroke:#32CD32,stroke-width:2px    blockchain --&gt; |\"Block Reward\"| poolServer    poolServer --&gt; |\"Proportional Rewards\"| minersA mining pool is a collaborative effort where multiple cryptocurrency miners combine their computational resources to increase the chances of successfully mining a block and earning rewards. The reward is distributed proportionally to the miners based on the number of shares they contributed.Immutability and Consensusgraph TD    block1[\"fa:fa-cube Block 1\"]    block2[\"fa:fa-cube Block 2\"]    block3[\"fa:fa-times-circle Block 3\"]:::invalid    block4[\"fa:fa-times-circle Block 4\"]:::invalid    block5[\"fa:fa-times-circle Block 5\"]:::invalid    block1 --&gt;|fa:fa-link Hash Link| block2    block2 --&gt;|fa:fa-link Hash Link| block3    block3 --&gt;|fa:fa-link Hash Link| block4    block4 --&gt;|fa:fa-link Hash Link| block5    block2 -- \"fa:fa-edit Data Change\" --&gt; invalidBlock2[\"fa:fa-ban Invalid Block 2\"]    invalidBlock2 -.-&gt; |\"fa:fa-times Invalidates\"| block3    invalidBlock2 -.-&gt; |\"fa:fa-times Invalidates\"| block4    invalidBlock2 -.-&gt; |\"fa:fa-times Invalidates\"| block5    classDef invalid fill:#f96Immutability:This means that once a block of data is added to the blockchain, it is extremely difficult to alter or remove. This immutability is crucial for maintaining trust and security in the network. Each block in a blockchain contains a unique cryptographic hash that is calculated based on the block‚Äôs contents and the hash of the previous block. If you change any data within a block, its hash changes, invalidating all subsequent blocks in the chain.graph LR    classDef blockchain fill:#ff9,stroke:#333,stroke-width:2px;        subgraph Blockchain[\"fa:fa-network-wired Blockchain\"]        style Blockchain fill:#ff9,stroke:#333,stroke-width:2px        Block1[\"fa:fa-cube Block 1\"]        Block2[\"fa:fa-cube Block 2\"]        Block3[\"fa:fa-cube Block 3\"]        Block1 --&gt; |\"fa:fa-link Hash Link\"| Block2        Block2 --&gt; |\"fa:fa-link Hash Link\"| Block3    end    Block2 -- \"fa:fa-edit Attempted Data Change\" --&gt; Disagreement[\"fa:fa-exclamation-triangle Disagreement with\\nNetwork Consensus\"]Consensus: Changes to the blockchain require the agreement (consensus) of the majority of the network‚Äôs nodes. To modify a historical block, you‚Äôd need to convince the majority of nodes to rewrite the entire chain from that point onwards, which is computationally infeasible and extremely unlikely.Different blockchains use various consensus mechanisms to agree on the validity of transactions and the state of the blockchain.Proof of Work (PoW): Used by Bitcoin, it requires solving complex puzzles to add blocks, making it difficult for malicious actors to alter history due to the immense computational power needed.Proof of Stake (PoS): Employed by Ethereum 2.0, it selects validators based on staked cryptocurrency, disincentivizing malicious actions as they risk losing their stake.P2P (Peer-to-Peer) NetworkA peer-to-peer (P2P) network is a type of network architecture where all participants (devices or computers) have equal status and capabilities.  Unlike traditional client-server models, there‚Äôs no central authority governing the network. Instead, each participant, known as a node, directly communicates and shares resources with other nodes.graph TD    classDef network fill:#f0f0f0,stroke:#333,stroke-width:2px;    subgraph P2PBlockchainNetwork[\"fa:fa-sitemap Peer-to-Peer Blockchain Network\"]        style P2PBlockchainNetwork fill:#f0f0f0,stroke:#333,stroke-width:2px        A[\"fa:fa-server Node 1\"]        B[\"fa:fa-server Node 2\"]        C[\"fa:fa-server Node 3\"]        A &lt;--\"fa:fa-share-alt Share Ledger Data &amp; Validate Transactions\"--&gt; B        A &lt;--&gt; C        B &lt;--&gt; C    endHashrategraph LR    HashRate[\"fa:fa-tachometer-alt BTC Hash Rate&lt;br/&gt;614.42 EH/s\"]    style HashRate fill:#F2F2F2,stroke:#666666    HashRate -- \"fa:fa-info Represents\" --&gt; TotalNetworkPower[\"fa:fa-cogs Total Network Computing Power\"]    style TotalNetworkPower fill:#FFDDCC,stroke:#E69966    TotalNetworkPower -- \"fa:fa-users Influenced by\" --&gt; NumberOfMiners[\"fa:fa-user-friends Number of Miners\"]    style NumberOfMiners fill:#FFF2CC,stroke:#D6B656    TotalNetworkPower -- \"fa:fa-microchip Influenced by\" --&gt; MiningHardware[\"fa:fa-tools Mining Hardware Efficiency\"]    style MiningHardware fill:#C2E0C6,stroke:#5FB483    HashRate -- \"fa:fa-tasks Determines\" --&gt; Difficulty[\"fa:fa-dumbbell Mining Difficulty\"]    style Difficulty fill:#E1D5E7,stroke:#9673A6    HashRate -- \"fa:fa-clock Affects\" --&gt; BlockTime[\"fa:fa-hourglass-half Average Block Time&lt;br/&gt;~10 minutes\"]    style BlockTime fill:#D5E8D4,stroke:#82B366        HashRate -- \"fa:fa-lock Indicates\" --&gt; Security[\"fa:fa-shield-alt Network Security\"]    style Security fill:#9673A6,stroke:#E1D5E7Bitcoin Hashrate measures the total computing power miners use to secure the network, currently at 614.42 EH/s. A higher hashrate enhances security by making it harder for bad actors to control the network. It also influences mining difficulty, ensuring consistent block creation times.ETH (proof of stake)graph LR    subgraph Network[\"fa:fa-network-wired Blockchain Network (Ethereum 2.0)\"]        style Network fill:#F2F2F2,stroke:#666666        Transactions[\"fa:fa-exchange-alt Transactions\"]        style Transactions fill:#E1D5E7,stroke:#9673A6        Blocks[\"fa:fa-cube Blocks\"]        style Blocks fill:#F0F0F0,stroke:#888888    end    subgraph Participants[\"fa:fa-users Participants\"]        style Participants fill:#F0F0F0,stroke:#888888        Wallet[\"fa:fa-wallet Wallet (User)\"]        style Wallet fill:#FFF2CC,stroke:#D6B656        Node[\"fa:fa-server Node\"]        style Node fill:#C2E0C6,stroke:#5FB483        Validator[\"fa:fa-user-shield Validator\"]        style Validator fill:#F5DEB3,stroke:#D2B48C    end        subgraph Wallet      PublicKey[\"fa:fa-key Public Key (Sender/Recipient)\"]      PrivateKey[\"fa:fa-lock Private Key (Sender)\"]    end    Wallet -- \"fa:fa-paper-plane Creates/Receives\" --&gt; Transactions    Node -- \"fa:fa-check Validates/Stores\" --&gt; Transactions    Node -- \"fa:fa-check Validates/Stores\" --&gt; Blocks    Validator -- \"fa:fa-gavel Proposes/Validates\" --&gt; Blocks    Transactions -.-&gt; Blocks    PrivateKey -- \"fa:fa-pen Signs\" --&gt; Transactions    PublicKey -- \"fa:fa-check Verifies\" --&gt; Transactions    Validator -- \"fa:fa-coins Receives Rewards From\" --&gt; Blocks    Validator --&gt; StakedETH[\"fa:fa-piggy-bank Staked ETH (32 ETH)\"]    POS (proof of stake): stake ETH to become validators. Validators are randomly selected to propose new blocks, with others attesting to their validity. Correct validators earn rewards, while incorrect ones are penalized. This replaces mining, making Ethereum more scalable and eco-friendly.Gas FeesGas in the Ethereum network is a unit that measures the computational work required to execute transactions or smart contracts. Each operation on the Ethereum Virtual Machine (EVM) requires a certain amount of gas, which must be paid for in Ether (ETH). Gas ensures that transactions and contract executions are fairly priced according to their resource consumption. Users specify a gas limit and a gas price for their transactions; if the gas limit is exceeded, the transaction fails but the gas used is still deducted. This mechanism prevents network abuse and incentivizes efficient code execution.graph TD    subgraph UserA[\"fa:fa-user Bob's Account\"]        style UserA fill:#9f9,stroke:#333,stroke-width:2px        BalanceA[\"fa:fa-money-bill-wave 1.0042 ETH\"]    end    style Blockchain fill:#ff9,stroke:#333,stroke-width:2px    Miner[\"fa:fa-gavel Miner/Validator\"]        subgraph UserB[\"fa:fa-user Alice's Account\"]        style UserB fill:#9f9,stroke:#333,stroke-width:2px        BalanceB[\"fa:fa-money-bill-wave 0 ETH\"]    end    UserA --\"fa:fa-arrow-right Sends 1 ETH + 0.0042 ETH Fee\"--&gt; Blockchain    Blockchain --\"fa:fa-arrow-right Transfers 1 ETH\"--&gt; UserB    Blockchain --\"fa:fa-coins Pays 0.00021 ETH Tip\"--&gt; Miner    Blockchain --\"fa:fa-fire Burns 0.00399 ETH\"--&gt; LostETH[\"fa:fa-money-bill-wave 0.00399 ETH\"]  Transaction Cost: Sending ETH has a base cost of 21,000 units of gas.  Gas Price: Bob sets a gas price of 200 gwei (190 gwei base fee + 10 gwei tip).  Total Fee: The total fee Bob pays is 21,000 gas * 200 gwei/gas = 4,200,000 gwei (or 0.0042 ETH).  Bob‚Äôs account is debited 1.0042 ETH (1 ETH for Alice + 0.0042 ETH fee).  Alice‚Äôs account is credited 1 ETH.0.00399 ETH is burned (removed from circulation).  The miner receives 0.00021 ETH as a tip.Unit Conversion            Unit      Symbol      Value in Wei (Smallest Unit)      Usage                  Wei      wei      1      Atomic unit of Ether              Kwei      Kwei      10^3      Rarely used              Mwei      Mwei      10^6      Rarely used              Gwei      Gwei      10^9      Gas prices, small transactions              Microether (Szabo)      szabo      10^12      Rarely used              Milliether (Finney)      finney      10^15      Small amounts of Ether              Ether      ETH      10^18      Standard unit for most purposes      Key Points:  Wei: The smallest denomination of Ether. All other units are multiples of Wei.  Gwei: The most commonly used unit after Ether, especially for gas prices and small transactions.  Ether: The standard unit for most transactions and interactions with Ethereum applications.Example Conversions:  1 ETH = 1,000,000,000 Gwei  1 Gwei = 1,000,000,000 weiGas Price and Gas Limitgraph LR  subgraph transaction[\"fa:fa-file-invoice-dollar Transaction\"]      style transaction fill:#9f9,stroke:#333,stroke-width:2px      data[\"fa:fa-database Transaction Data\"]      gasLimit[\"fa:fa-gas-pump Gas Limit (Maximum Allowed)\"]      gasPrice[\"fa:fa-dollar-sign Gas Price (Bid per Unit)\"]  end  subgraph network[\"fa:fa-link Network\"]      style network fill:#ff9,stroke:#333,stroke-width:2px      marketDemand[\"fa:fa-chart-line Market Demand\"]      networkCongestion[\"fa:fa-traffic-light Network Congestion\"]      minerPreference[\"fa:fa-gavel Miner Preference\"]  end  subgraph execution[\"fa:fa-cogs Execution\"]      style execution fill:#f9f,stroke:#333,stroke-width:2px      gasUsed[\"fa:fa-fire Gas Used (Actual Consumption)\"]      transactionFee[\"fa:fa-coins Transaction Fee (Total Cost)\"]  end  transaction --&gt; execution  gasLimit -.-&gt; gasUsed  gasPrice &amp; gasUsed --&gt; transactionFee  marketDemand --&gt; gasPrice  networkCongestion --&gt; gasPrice  minerPreference --&gt; gasPrice      Gas Price as a Bid: The maximum amount of gas (computational units) the sender is willing to spend on executing the transaction. It determines the transaction‚Äôs priority and is often influenced by market demand, network congestion, and miner preference.        Gas Limit: The maximum amount of gas (computational units) the sender is willing to spend on executing the transaction.        Gas Used: The actual amount of gas consumed during the transaction execution.        Transaction Fee: The total cost of executing the transaction, calculated as: Gas Used * Gas Price  Stablecoinsgraph LR    classDef cadetblue fill:#5F9EA0,stroke:#333,stroke-width:2px;    classDef mediumaquamarine fill:#66CDAA,stroke:#333,stroke-width:2px;    classDef lightgreen fill:#90EE90,stroke:#333,stroke-width:2px;    classDef lightpink fill:#FFB6C1,stroke:#333,stroke-width:2px;    subgraph Stablecoins[\"fa:fa-coins **Types of Stablecoins**\"]        FiatBacked[\"fa:fa-dollar-sign Fiat-Backed\"]:::cadetblue        CommodityBacked[\"fa:fa-gem Commodity-Backed\"]:::mediumaquamarine        CryptoBacked[\"fa:fa-b Crypto-Backed\"]:::lightgreen        Algorithmic[\"fa:fa-cogs Algorithmic\"]:::lightpink    end    FiatBacked --&gt; Tether[\"fa:fa-dollar-sign Tether (USDT)\"]:::cadetblue    FiatBacked --&gt; USDCoin[\"fa:fa-dollar-sign USD Coin (USDC)\"]:::cadetblue    CommodityBacked --&gt; TetherGold[\"fa:fa-gem Tether Gold (XAUT)\"]:::mediumaquamarine    CommodityBacked --&gt; PAXGold[\"fa:fa-gem PAX Gold (PAXG)\"]:::mediumaquamarine    CryptoBacked --&gt; Dai[\"fa:fa-b Dai (DAI)\"]:::lightgreen    Algorithmic --&gt; AMPL[\"fa:fa-cogs Ampleforth (AMPL)\"]:::lightpink    Algorithmic --&gt; UST[\"fa:fa-cogs TerraUSD (UST)\"]:::lightpinkStablecoins offer stability in the volatile crypto market, making them suitable for everyday transactions, remittances, and trading. They provide easy access to the crypto world without exposure to extreme price fluctuation. They are a bridge between traditional finance and the crypto world.Fiat-Backed Stablecoins: These stablecoins are backed 1:1 by fiat currencies like the US Dollar or Euro. The value of each coin is directly tied to the value of the fiat currency.Commodity-Backed Stablecoins: These stablecoins are backed by physical assets such as precious metals. The value is tied to the commodity, providing stability based on the physical asset‚Äôs value.Crypto-Backed Stablecoins: These stablecoins are backed by other cryptocurrencies. They are usually over-collateralized to account for the volatility of the underlying crypto assets.Algorithmic Stablecoins: These stablecoins use algorithms and smart contracts to manage the supply and stabilize the value without needing reserves. They rely on mechanisms to adjust the coin supply based on demand.Tether LimitedTether Limited is the company behind USDT, one of the most widely used stablecoins in the cryptocurrency market. USDT is designed to maintain a one-to-one peg with the U.S. dollar, providing stability and making it easier for users to trade digital assets without exposure to price volatility. By bridging traditional finance with the crypto world, Tether has significantly enhanced liquidity and accessibility in the digital asset space. Its stablecoin facilitates fast, low-cost transactions across exchanges and platforms globally. Tether‚Äôs innovation has played a key role in the growing adoption and mainstream acceptance of cryptocurrencies.Tether Limited offers several stablecoin products besides USDT, including EUR‚ÇÆ (Euro Tether), CNH‚ÇÆ (Offshore Chinese Yuan Tether), XAU‚ÇÆ (Tether Gold), GBP‚ÇÆ (British Pound Sterling Tether), and MXN‚ÇÆ (Mexican Peso Tether).USDTUSDT, commonly known as Tether, is a stablecoin cryptocurrency designed to maintain a one-to-one peg with the U.S. dollar. It offers the stability of traditional currency while harnessing the advantages of blockchain technology, enabling users to transact quickly and efficiently. USDT is widely used across various cryptocurrency exchanges, facilitating seamless trading without the volatility typically associated with digital assets.graph     classDef primary fill:#FFFFDE,stroke:#333,stroke-width:2px;      %% Light Yellow    classDef secondary fill:#DEFFEF,stroke:#333,stroke-width:2px;   %% Light Teal    classDef tertiary fill:#DEDEFF,stroke:#333,stroke-width:2px;    %% Light Lavender    classDef quaternary fill:#FFDEEF,stroke:#333,stroke-width:2px;  %% Light Pink    subgraph Tether[\"fa:fa-dollar-sign Tether (USDT)\"]    style Tether fill:#fff, stroke-width:0px;            subgraph Stability[\"fa:fa-balance-scale Stability and Trust\"]        Pegged[\"fa:fa-lock Pegged to the US Dollar\"]:::secondary        Accepted[\"fa:fa-check-circle Widespread Acceptance\"]:::secondary    end    subgraph Liquidity[\"fa:fa-water Liquidity and Volume\"]        HighLiquidity[\"fa:fa-tint High Liquidity\"]:::tertiary        Volume[\"fa:fa-chart-bar Trading Volume\"]:::tertiary    end    subgraph Transparency[\"fa:fa-eye Transparency and Regulation\"]        AuditedReserves[\"fa:fa-file-alt Audited Reserves\"]:::quaternary        Compliance[\"fa:fa-shield-alt Compliance\"]:::quaternary    end    subgraph Accessibility[\"fa:fa-users Accessibility and Utility\"]        EaseOfUse[\"fa:fa-user Ease of Use\"]:::tertiary        VersatileUtility[\"fa:fa-exchange-alt Versatile Utility\"]:::tertiary    end    subgraph Integration[\"fa:fa-network-wired Cross-Platform Integration\"]        MultiChain[\"fa:fa-link Multi-Chain Support\"]:::secondary        DeFi[\"fa:fa-coins Integration with DeFi\"]:::secondary    end    Stability    Liquidity    Transparency    Accessibility    Integration    Stability --&gt; Pegged    Stability --&gt; Accepted    Liquidity --&gt; HighLiquidity    Liquidity --&gt; Volume    Transparency --&gt; AuditedReserves    Transparency --&gt; Compliance    Accessibility --&gt; EaseOfUse    Accessibility --&gt; VersatileUtility    Integration --&gt; MultiChain    Integration --&gt; DeFi    end USDCUSDC, or USD Coin, is a stablecoin cryptocurrency designed to maintain a one-to-one peg with the U.S. dollar. Issued by Circle in collaboration with Coinbase, USDC combines the stability of fiat currency with the efficiency of blockchain technology for fast and secure transactions. It is widely supported across numerous cryptocurrency exchanges and platforms, enabling seamless trading without the volatility typically associated with digital assets. Known for its transparency, USDC‚Äôs reserves are regularly audited to ensure full backing.DAIDAI is a decentralized stablecoin running on the Ethereum blockchain, designed to maintain a one-to-one peg with the U.S. dollar. Unlike centralized stablecoins, DAI is generated through over-collateralized loans using cryptocurrencies as collateral via the MakerDAO protocol. This allows for a stable digital currency that is not reliant on any central authority, combining the benefits of price stability and decentralization. DAI is widely used in the decentralized finance (DeFi) ecosystem, facilitating various financial activities such as lending, borrowing, and trading without the volatility typically associated with cryptocurrencies.Restakinggraph LR    classDef network fill:#f0f0f0,stroke:#333,stroke-width:2px;    subgraph EthereumBlockchain[\"fa:fa-link Ethereum Blockchain (Beacon Chain)\"]        style EthereumBlockchain fill:#F2F2F2,stroke:#666666        Validator[\"fa:fa-user-shield Validator\"]        style Validator fill:#FFF2CC,stroke:#D6B656    end    subgraph StakingPlatform[\"fa:fa-coins Staking Platform (e.g., Lido)\"]        style StakingPlatform fill:#C2E0C6,stroke:#5FB483        StakingPool[\"fa:fa-piggy-bank Staking Pool\"]        style StakingPool fill:#E6F2FF,stroke:#B8D8F2        stETH[\"fa:fa-coins stETH (Liquid Staking Token)\"]        style stETH fill:#D5E8D4,stroke:#82B366    end    subgraph RestakingPlatform[\"fa:fa-handshake Restaking Platform (e.g., EigenLayer)\"]        style RestakingPlatform fill:#E1D5E7,stroke:#9673A6        RestakingContract[\"fa:fa-file-contract Restaking Contract\"]        style RestakingContract fill:#FFDDCC,stroke:#E69966        OtherProtocol[\"fa:fa-cogs Other Protocol/Service\"]        style OtherProtocol fill:#F08080,stroke:#DC143C    end    Validator -- \"fa:fa-arrow-right Stake 32 ETH\" --&gt; StakingPool    StakingPool -- \"fa:fa-arrow-right Issues\" --&gt; stETH    Validator -- \"fa:fa-arrow-left Receives\" --&gt; stETH    stETH -- \"fa:fa-arrow-right Restake\" --&gt; RestakingContract    RestakingContract -- \"fa:fa-shield-alt Secures\" --&gt; OtherProtocol    RestakingContract -- \"fa:fa-gift Provides\" --&gt; AdditionalRewards[\"fa:fa-coins Additional Rewards\"]    Validator -- \"fa:fa-coins Earns\" --&gt; StakingRewards[\"fa:fa-coins Staking Rewards (ETH)\"]Restaking : allows Ethereum validators to use their staked ETH to secure additional protocols, like EigenLayer, beyond the main Ethereum network. This increases security for multiple protocols and enables validators to earn additional rewards. Validators‚Äô staked ETH is verified and utilized across different applications, enhancing capital efficiency and network robustness.Smart Contractsgraph LR    subgraph SmartContract[\"fa:fa-file-contract Smart Contract\"]        style SmartContract fill:#FFF2CC,stroke:#D6B656        ContractCode[\"fa:fa-code Contract Code&lt;br/&gt;(Solidity, etc.)\"]        StateVariables[\"fa:fa-database State Variables\"]        Functions[\"fa:fa-cogs Functions\"]        Events[\"fa:fa-bullhorn Events\"]    end    subgraph Blockchain[\"fa:fa-network-wired Blockchain\"]        style Blockchain fill:#F2F2F2,stroke:#666666        BlockchainNetwork[\"fa:fa-link Blockchain Network\"]    end    subgraph Participants[\"fa:fa-users Participants\"]        style Participants fill:#F0F0F0,stroke:#888888        User1[\"fa:fa-user User 1\"]        style User1 fill:#C2E0C6,stroke:#5FB483        User2[\"fa:fa-user User 2\"]        style User2 fill:#F5DEB3,stroke:#D2B48C    end        subgraph Trigger[\"fa:fa-bolt Triggers\"]        style Trigger fill:#D4EDDA,stroke:#4CAF50        ExternalCall[\"fa:fa-external-link-alt External Call&lt;br/&gt;(User/Contract)\"]        style ExternalCall fill:#E6F2FF,stroke:#B8D8F2        TimeBased[\"fa:fa-clock Time-Based&lt;br/&gt;(e.g., Scheduled Payment)\"]        style TimeBased fill:#E6F2FF,stroke:#B8D8F2        EventBased[\"fa:fa-calendar Event-Based&lt;br/&gt;(e.g., Price Change)\"]        style EventBased fill:#E6F2FF,stroke:#B8D8F2    end    User1 -- \"fa:fa-hand-point-right Interacts With\" --&gt; SmartContract    User2 -- \"fa:fa-hand-point-right Interacts With\" --&gt; SmartContract    SmartContract -- \"fa:fa-link Deployed On\" --&gt; BlockchainNetwork    ContractCode -- \"fa:fa-pencil-alt Defines\" --&gt; StateVariables    ContractCode -- \"fa:fa-pencil-alt Defines\" --&gt; Functions    ContractCode -- \"fa:fa-pencil-alt Defines\" --&gt; Events    Functions -- \"fa:fa-sync-alt Updates\" --&gt; StateVariables    Events -- \"fa:fa-bell Emitted When State Changes\" --&gt; BlockchainNetwork    ExternalCall --&gt; Functions    TimeBased --&gt; Functions    EventBased --&gt; FunctionsSmart contract: A smart contract is a self-executing contract with the terms directly written into code on a blockchain. It automatically enforces and executes the terms of an agreement when predefined conditions are met. This eliminates the need for intermediaries, reducing costs and enhancing security. Smart contracts are commonly used in various applications, including finance, real estate, and supply chain management.Smart Contract Deploy Transactiongraph TD    subgraph transaction[\"fa:fa-file-invoice-dollar Transaction\"]        style transaction fill:#9f9,stroke:#333,stroke-width:2px        gasPrice[\"fa:fa-dollar-sign Gas Price\"]        gasLimit[\"fa:fa-gas-pump Gas Limit\"]        compiledBytecode[\"fa:fa-file-code Compiled Bytecode\"]        signature[\"fa:fa-signature Signature (from Private Key)\"]        noRecipient[\"fa:fa-ban no recipient\"]    end    subgraph blockchain[\"fa:fa-link Blockchain Network\"]        style blockchain fill:#ff9,stroke:#333,stroke-width:2px        minersValidators[\"fa:fa-gavel Miners/Validators\"]    end    transaction --&gt;|\"Verified &amp; Executed By\"&lt;i class='fas fa-arrow-right'&gt;&lt;/i&gt;| minersValidators  No Recipient Address: Unlike regular transactions that send funds to an address, deployment transactions don‚Äôt have a recipient. The transaction itself creates the new contract address.  Gas Costs: Deploying a smart contract consumes gas, and the cost depends on the complexity of your contract. Be mindful of the gas limit you set.  Immutability: Once deployed, a smart contract‚Äôs code is generally immutable (cannot be changed), so thorough testing is crucial before deployment.  Modifying a smart contract on a blockchain like Ethereum requires deploying a new transaction with the updated code. This is due to the fundamental immutability of blockchains: once data is written to the blockchain, it cannot be changed.Dapp (Decentralized Application)flowchart TB    classDef blockchain fill:#ff9,stroke:#333,stroke-width:2px;    classDef contract fill:#99f,stroke:#333,stroke-width:2px;    classDef external fill:#9f9,stroke:#333,stroke-width:2px;    SmartContract[\"fa:fa-file-contract Smart Contract: MyToken\"]:::contract    Blockchain[\"fa:fa-network-wired Blockchain\"]:::blockchain    TransactionLog[\"fa:fa-list Transaction Log (Blockchain)\"]:::blockchain    EventData[\"fa:fa-database Event Data (Blockchain)\"]:::blockchain    BalancesData[\"fa:fa-balance-scale Balances Data (Blockchain)\"]:::blockchain    dApp[\"fa:fa-desktop Decentralized Application\"]:::external    Web3[\"fa:fa-code Web3.js/Ethers.js\"]:::external    SmartContract --&gt;|fa:fa-upload Deploys to| Blockchain    SmartContract --&gt;|fa:fa-bullhorn Emits Event| TransactionLog    SmartContract --&gt;|fa:fa-exchange-alt Reads/Writes| BalancesData    Blockchain --&gt;|fa:fa-save Stores| TransactionLog    Blockchain --&gt;|fa:fa-save Stores| EventData    Blockchain --&gt;|fa:fa-save Stores| BalancesData    dApp --&gt;|fa:fa-hand-point-right Interacts with| Web3    Web3 --&gt;|fa:fa-database Reads Event Data| EventData    Web3 --&gt;|fa:fa-exchange-alt Reads/Writes| BalancesData    Web3 --&gt;|fa:fa-bell Listens for Events| TransactionLog    subgraph BlockchainSection[\" \"]        Blockchain        TransactionLog        EventData        BalancesData    endDecentralized applications (dApps) are software applications that run on a blockchain or peer-to-peer network of computers instead of a single computer. They are open-source, operate autonomously, and have their data stored immutably on a blockchain. dApps are often used to create financial services, social networks, games, and other applications that don‚Äôt require a central authority or intermediary.Examples of dApps : Uniswap, Compound, Aave, MakerDAO etc.NFT (Non-Fungible Token)graph LR    subgraph NFTSmartContract[\"fa:fa-file-contract NFT Smart Contract\"]        style NFTSmartContract fill:#FFF2CC,stroke:#D6B656        ContractCode[\"fa:fa-code Contract Code&lt;br/&gt;(Solidity, etc.)\"]        StateVariables[\"fa:fa-database State Variables&lt;br/&gt;(Token Metadata, Owner)\"]        Functions[\"fa:fa-cogs Functions&lt;br/&gt;(Mint, Transfer, Burn)\"]        Events[\"fa:fa-bullhorn Events&lt;br/&gt;(Transfer, Approval)\"]    end    subgraph EthereumBlockchain[\"fa:fa-link Ethereum Blockchain\"]        style EthereumBlockchain fill:#F2F2F2,stroke:#666666        BlockchainNetwork[\"fa:fa-network-wired Ethereum Network\"]    end    subgraph Participants[\"fa:fa-users Participants\"]        style Participants fill:#F0F0F0,stroke:#888888        Creator[\"fa:fa-user-plus Creator\"]        style Creator fill:#C2E0C6,stroke:#5FB483        Buyer[\"fa:fa-user Buyer\"]        style Buyer fill:#F5DEB3,stroke:#D2B48C        Seller[\"fa:fa-user-times Seller\"]        style Seller fill:#ADD8E6,stroke:#4682B4    end        subgraph Trigger[\"fa:fa-bolt Triggers\"]        style Trigger fill:#D4EDDA,stroke:#4CAF50        MintCall[\"fa:fa-arrow-circle-up Mint Call&lt;br/&gt;(Creator)\"]        style MintCall fill:#E6F2FF,stroke:#B8D8F2        TransferCall[\"fa:fa-exchange-alt Transfer Call&lt;br/&gt;(Owner)\"]        style TransferCall fill:#E6F2FF,stroke:#B8D8F2        BurnCall[\"fa:fa-trash Burn Call&lt;br/&gt;(Owner)\"]        style BurnCall fill:#E6F2FF,stroke:#B8D8F2    end    Creator -- \"fa:fa-hand-point-right Interacts With\" --&gt; NFTSmartContract    Buyer -- \"fa:fa-hand-point-right Interacts With\" --&gt; NFTSmartContract    Seller -- \"fa:fa-hand-point-right Interacts With\" --&gt; NFTSmartContract    NFTSmartContract -- \"fa:fa-link Deployed On\" --&gt; BlockchainNetwork    ContractCode -- \"fa:fa-pencil-alt Defines\" --&gt; StateVariables    ContractCode -- \"fa:fa-pencil-alt Defines\" --&gt; Functions    ContractCode -- \"fa:fa-pencil-alt Defines\" --&gt; Events    Functions -- \"fa:fa-sync-alt Updates\" --&gt; StateVariables    Events -- \"fa:fa-bell Emitted When State Changes\" --&gt; BlockchainNetwork    MintCall --&gt; Functions    TransferCall --&gt; Functions    BurnCall --&gt; FunctionsAn NFT (Non-Fungible Token) is a unique digital asset represented by a smart contract. The smart contract contains code that defines state variables like token metadata and ownership, and functions for minting, transferring, and burning NFTs. Creators can mint new NFTs, owners can transfer them, and events like transfers and approvals are logged on the blockchain. Participants such as creators, buyers, and sellers interact with the NFT smart contract to manage the lifecycle of NFTs, ensuring transparency and security in ownership.Lending and Borrowinggraph LR    subgraph DeFiSmartContract[\"fa:fa-file-contract DeFi Lending/Borrowing Smart Contract\"]        style DeFiSmartContract fill:#FFF2CC,stroke:#D6B656        ContractCode[\"fa:fa-code Contract Code&lt;br/&gt;(Solidity, etc.)\"]        StateVariables[\"fa:fa-database State Variables&lt;br/&gt;(Lender Balances, Borrower Debts)\"]        Functions[\"fa:fa-cogs Functions&lt;br/&gt;(Deposit, Withdraw, Borrow, Repay)\"]        Events[\"fa:fa-bullhorn Events&lt;br/&gt;(Deposit, Withdrawal, Borrow, Repay)\"]    end    subgraph EthereumBlockchain[\"fa:fa-link Ethereum Blockchain\"]        style EthereumBlockchain fill:#F2F2F2,stroke:#666666        BlockchainNetwork[\"fa:fa-network-wired Ethereum Network\"]    end    subgraph Participants[\"fa:fa-users Participants\"]        style Participants fill:#F0F0F0,stroke:#888888        Lender[\"fa:fa-user-tie Lender\"]        style Lender fill:#C2E0C6,stroke:#5FB483        Borrower[\"fa:fa-user Borrower\"]        style Borrower fill:#F5DEB3,stroke:#D2B48C    end        subgraph Trigger[\"fa:fa-bolt Triggers\"]        style Trigger fill:#D4EDDA,stroke:#4CAF50        DepositCall[\"fa:fa-arrow-circle-down Deposit Call&lt;br/&gt;(Lender)\"]        style DepositCall fill:#E6F2FF,stroke:#B8D8F2        WithdrawCall[\"fa:fa-arrow-circle-up Withdraw Call&lt;br/&gt;(Lender)\"]        style WithdrawCall fill:#E6F2FF,stroke:#B8D8F2        BorrowCall[\"fa:fa-hand-holding-usd Borrow Call&lt;br/&gt;(Borrower)\"]        style BorrowCall fill:#E6F2FF,stroke:#B8D8F2        RepayCall[\"fa:fa-handshake Repay Call&lt;br/&gt;(Borrower)\"]        style RepayCall fill:#E6F2FF,stroke:#B8D8F2    end    Lender -- \"fa:fa-hand-point-right Interacts With\" --&gt; DeFiSmartContract    Borrower -- \"fa:fa-hand-point-right Interacts With\" --&gt; DeFiSmartContract    DeFiSmartContract -- \"fa:fa-link Deployed On\" --&gt; BlockchainNetwork    ContractCode -- \"fa:fa-pencil-alt Defines\" --&gt; StateVariables    ContractCode -- \"fa:fa-pencil-alt Defines\" --&gt; Functions    ContractCode -- \"fa:fa-pencil-alt Defines\" --&gt; Events    Functions -- \"fa:fa-sync-alt Updates\" --&gt; StateVariables    Events -- \"fa:fa-bell Emitted When State Changes\" --&gt; BlockchainNetwork    DepositCall --&gt; Functions    WithdrawCall --&gt; Functions    BorrowCall --&gt; Functions    RepayCall --&gt; FunctionsLenders deposit funds into a smart contract to earn interest. Borrowers take out loans by using collateral and paying interest. Smart contracts manage balances, debts, and ensure transparent transactions. Lenders can withdraw their funds, and borrowers can repay loans through specific function calls. This system enhances accessibility and efficiency in financial services without intermediaries.Liquidity Poolgraph LR;subgraph LiquidityPool[\"fa:fa-water Liquidity Pool\"]    TokenA[\"fa:fa-coins Token A\"]:::skyblue    TokenB[\"fa:fa-coins Token B\"]:::limegreen    AMM[\"fa:fa-cogs Automated\\nMarket Maker\"]:::goldendUser1[\"fa:fa-user User 1\\n(Liquidity\\nProvider)\"]:::coral --&gt;|fa:fa-arrow-down Deposits\\nToken A &amp; B| LiquidityPoolUser2[\"fa:fa-user User 2\\n(Liquidity\\nProvider)\"]:::orchid --&gt;|fa:fa-arrow-down Deposits\\nToken A &amp; B| LiquidityPoolLiquidityPool --&gt;|fa:fa-exchange-alt Swaps\\nToken A &lt;--&gt; B| User3[\"fa:fa-user User 3\\n(Swapper)\"]:::hotpinkLiquidityPool --&gt;|fa:fa-coins Trading\\nFees| User1 &amp; User2AMM --&gt;|fa:fa-balance-scale Determines\\nPrice &amp; Executes\\nTrades| LiquidityPoolclassDef aqua fill:#00FFFF,stroke:#333,stroke-width:2px;classDef skyblue fill:#87CEEB,stroke:#333,stroke-width:2px;classDef limegreen fill:#32CD32,stroke:#333,stroke-width:2px;classDef coral fill:#FF7F50,stroke:#333,stroke-width:2px;classDef orchid fill:#DA70D6,stroke:#333,stroke-width:2px;classDef hotpink fill:#FF69B4,stroke:#333,stroke-width:2px;classDef gold fill:#FFD700,stroke:#333,stroke-width:2px;Liquidity Pool:  Core component of decentralized exchanges (DEX) like Uniswap.  A smart contract holding a pair of tokens (e.g., Token A and Token B).  The ratio of tokens in the pool determines their relative prices.Automated Market Maker (AMM):  Algorithm that governs the liquidity pool.  Determines token prices based on the ratio of assets in the pool using a constant product formula.  Automatically executes trades when users interact with the pool.Liquidity Providers (User 1 &amp; User 2):  Deposit equal value of two tokens into the pool.  Receive LP tokens representing their share of the pool.  Earn trading fees proportional to their share.  Face risk of impermanent loss if token prices diverge significantly.Swappers (User 3):  Trade one token for another directly within the pool.  Pay a small transaction fee, distributed to liquidity providers.ETH VM ( Virtual Machine)This is a virtual computer that runs on the Ethereum network. It‚Äôs responsible for executing smart contracts.graph LR    subgraph EVM[\"Ethereum Virtual Machine (EVM)\"]        style EVM fill:#F2F2F2,stroke:#666666        Bytecode[\"Bytecode&lt;br/&gt;(Smart Contract Instructions)\"]        style Bytecode fill:#FFF2CC,stroke:#D6B656        EVMRuntime[\"EVM Runtime Environment\"]        style EVMRuntime fill:#C2E0C6,stroke:#5FB483        Storage[\"Storage&lt;br/&gt;(Persistent Data)\"]        style Storage fill:#E1D5E7,stroke:#9673A6        Memory[\"Memory&lt;br/&gt;(Temporary Data)\"]        style Memory fill:#D5E8D4,stroke:#82B366        Stack[\"Stack&lt;br/&gt;(Computation)\"]        style Stack fill:#FFDDCC,stroke:#E69966    end    subgraph External[\"External\"]        Blockchain[\"Ethereum Blockchain\"]        style Blockchain fill:#F0F0F0,stroke:#888888    end    Bytecode -- \"Executes\" --&gt; EVMRuntime    EVMRuntime -- \"Reads/Writes\" --&gt; Storage    EVMRuntime -- \"Reads/Writes\" --&gt; Memory    EVMRuntime -- \"Uses\" --&gt; Stack    EVMRuntime -- \"Interacts With\" --&gt; Blockchain    Blockchain -- \"Stores\" --&gt; Bytecode    Blockchain -- \"Provides\" --&gt; Gas[\"Gas (Execution Fee)\"]    Gas --&gt; EVMRuntimeHard Fork vs Soft Forkgraph TD    subgraph ethereumBlockchain[\"fa:fa-link Ethereum Blockchain\"]        genesisBlock[\"fa:fa-cube Genesis Block\"] --&gt; |Initial State| block1[\"Block 1\"]         block1 --&gt; |Protocol Rules| block2[\"Block 2\"]        block2 --&gt; |Existing Rules| block3[\"Block 3\"]    end        subgraph softFork[\"fa:fa-code-fork Soft Fork (Backward Compatible)\"]        block3 --&gt; |New Rules, Old Nodes Valid| block4Soft[\"Block 4\"]        block4Soft --&gt; |Continued Compatibility| block5Soft[\"Block 5\"]        style softFork fill:#90EE90,stroke:#006400    end    subgraph hardFork[\"fa:fa-code-branch Hard Fork (Non-Compatible)\"]        block3 --&gt; |New Chain, New Rules| block4Hard[\"Block 4'\"]        block4Hard --&gt; |Separate Chain Continues| block5Hard[\"Block 5'\"]        style hardFork fill:#FFCCCB,stroke:#8B0000    end        block3 -.-&gt; block4Soft    block3 -.-&gt; block4Hard  Soft Fork:          Introduces backward-compatible changes to the protocol rules.      Older nodes can still validate new blocks, even if they don‚Äôt understand the new rules.      Both old and new nodes continue on the same chain.        Hard Fork:          Introduces non-backward-compatible changes.      Older nodes cannot validate new blocks created under the new rules.      This results in a permanent split into two separate chains: The original chain with old nodes following the old rules. A new chain with upgraded nodes following the new rules.      TVL (Total Value Locked)graph TD    subgraph DeFiProtocol[\"DeFi Protocol (e.g., Uniswap, Aave)\"]        style DeFiProtocol fill:#F2F2F2,stroke:#666666        LiquidityPools[\"Liquidity Pools\"]        style LiquidityPools fill:#FFF2CC,stroke:#D6B656        LendingPools[\"Lending Pools\"]        style LendingPools fill:#E1D5E7,stroke:#9673A6        StakingContracts[\"Staking Contracts\"]        style StakingContracts fill:#D5E8D4,stroke:#82B366    end    subgraph Assets[\"Assets (Locked in the Protocol)\"]        style Assets fill:#C2E0C6,stroke:#5FB483        Crypto1[\"Crypto Asset 1 (e.g., ETH)\"]        Crypto2[\"Crypto Asset 2 (e.g., USDC)\"]        Crypto3[\"Crypto Asset 3 (e.g., WBTC)\"]        LPTokens[\"LP Tokens (Liquidity Provider Tokens)\"]    end    subgraph TVL[\"Total Value Locked (TVL)\"]        style TVL fill:#FFDDCC,stroke:#E69966        TVLValue[\"USD Value of All&lt;br/&gt;Locked Assets\"]    end    Assets --&gt; LiquidityPools    Assets --&gt; LendingPools    Assets --&gt; StakingContracts    LiquidityPools --&gt; TVLValue    LendingPools --&gt; TVLValue    StakingContracts --&gt; TVLValue    TVLValue -- \"Indicates\" --&gt; ProtocolPopularity[\"Protocol Popularity\"]    TVLValue -- \"Reflects\" --&gt; UserTrust[\"User Trust &amp; Confidence\"]    TVLValue -- \"Influences\" --&gt; YieldRates[\"Yield Rates &amp; Incentives\"]Total/Locked/Circulating Supplygraph    subgraph TotalSupply[\"Total Supply\"]        style TotalSupply fill:#FFF2CC,stroke:#D6B656        AllCoins[\"All coins/tokens ever created\"]    end    subgraph CirculatingSupply[\"Circulating Supply\"]        style CirculatingSupply fill:#C2E0C6,stroke:#5FB483        AvailableCoins[\"Coins/tokens available&lt;br/&gt;for trading and use\"]            end        subgraph LockedSupply[\"Locked Supply\"]        style LockedSupply fill:#E1D5E7,stroke:#9673A6        TeamHoldings[\"Tokens held by the project team/investors\"]        Vesting[\"Tokens locked in vesting schedules\"]        Reserve[\"Tokens held in reserve funds\"]    end    TotalSupply --&gt; LockedSupply    TotalSupply --&gt; CirculatingSupply    subgraph Factors[\"Factors Affecting Circulating Supply\"]        style Factors fill:#D4EDDA,stroke:#4CAF50        NewIssuance[\"New Issuance (Mining/Minting)\"]        CoinBurn[\"Coin Burning\"]        MarketSentiment[\"Market Sentiment\"]    end    NewIssuance --&gt; CirculatingSupply    CoinBurn --&gt; CirculatingSupply    MarketSentiment --&gt; CirculatingSupply    CirculatingSupply -- \"Impacts\" --&gt; MarketCap[\"Market Capitalization&lt;br/&gt;(Price x Circulating Supply)\"]    CirculatingSupply -- \"Impacts\" --&gt; CoinPrice[\"Coin/Token Price\"]      Total Supply: The maximum number of coins or tokens that will ever exist for a particular cryptocurrency. This number is often fixed or predetermined.        Locked Supply:  The portion of the total supply that is not currently available for trading or use. This includes tokens held by the project team, investors, or in reserve funds, often subject to vesting schedules (gradual release over time).        Circulating Supply:  The number of coins or tokens that are currently available and actively traded on the market. This is the supply that directly impacts the market dynamics of the cryptocurrency.  TPS ( Transactions Per Second)graph     transactionsPerSecond[\"fa:fa-tachometer-alt Transactions Per Second (TPS)\"]    style transactionsPerSecond fill:#F2F2F2,stroke:#666666    subgraph factorsInfluencingTps[\"Factors Influencing TPS\"]        style factorsInfluencingTps fill:#D4EDDA,stroke:#4CAF50        blockSize[\"fa:fa-expand-arrows-alt Block Size\"]        style blockSize fill:#FFF9E6,stroke:#F9EBC8        blockTime[\"fa:fa-clock Block Time\"]        style blockTime fill:#FFF9E6,stroke:#F9EBC8        consensusMechanism[\"fa:fa-cogs Consensus Mechanism\"]        style consensusMechanism fill:#FFF9E6,stroke:#F9EBC8        transactionComplexity[\"fa:fa-search-dollar Transaction Complexity\"]        style transactionComplexity fill:#FFF9E6,stroke:#F9EBC8    end    subgraph benefitsHighTps[\"Benefits of High TPS\"]        style benefitsHighTps fill:#C2E0C6,stroke:#5FB483        scalability[\"fa:fa-chart-line Scalability\"]        lowFees[\"fa:fa-dollar-sign Low Fees\"]        fastTransactions[\"fa:fa-bolt Fast Transactions\"]        betterUserExperience[\"fa:fa-thumbs-up Better User Experience\"]    end    subgraph drawbacksHighTps[\"Drawbacks of High TPS\"]        style drawbacksHighTps fill:#F2DEDE,stroke:#D9534F        centralizationRisk[\"fa:fa-map-pin Centralization Risk\"]        potentialSecurityRisks[\"fa:fa-shield-alt Potential Security Risks\"]        higherCosts[\"fa:fa-money-bill-wave Higher Costs\"]        ddosAttackRisk[\"fa:fa-shield-virus DDoS Attack Risk\"]    end    subgraph examplesTpsBlockchains[\"Examples of TPS in Blockchains\"]        style examplesTpsBlockchains fill:#E1D5E7,stroke:#9673A6        bitcoin[\"fa:fa-b Bitcoin (BTC) ~4-7 TPS\"]        ethereum[\"fa:fa-e Ethereum (ETH) ~15-45 TPS\"]        solana[\"fa:fa-s Solana (SOL) ~50,000 TPS (theoretical)\"]    end    transactionsPerSecond --&gt; |\"Benefits\"|benefitsHighTps    transactionsPerSecond --&gt; |\"Drawbacks\"|drawbacksHighTps    blockSize --&gt; |\"Influences\"|transactionsPerSecond    blockTime --&gt; |\"Influences\"|transactionsPerSecond    consensusMechanism --&gt; |\"Influences\"|transactionsPerSecond    transactionComplexity --&gt; |\"Influences\"|transactionsPerSecond    transactionsPerSecond --&gt; |\"Examples\"|examplesTpsBlockchainsTPS: measures a blockchain‚Äôs transaction processing speed, influenced by block size, time, consensus, and complexity.High TPS offers scalability and fast transactions, but can risk security and decentralization.Layer 0, 1, 2graph     subgraph Layer0[\"fa:fa-network-wired Layer 0\"]        style Layer0 fill:#F0E68C,stroke:#BDB76B        Layer0Network1[\"fa:fa-cube Network 1\"]        Layer0Network2[\"fa:fa-cube Network 2\"]        Layer0Network1 --&gt; Layer0Network2    end    subgraph ParentChain[\"fa:fa-sitemap Parent Chain (L1)\"]        style ParentChain fill:#DCDCDC,stroke:#696969        ParentBlock1[\"fa:fa-cube Block 1\"]        ParentBlock2[\"fa:fa-cube Block 2\"]        ParentBlock3[\"fa:fa-cube Block 3\"]        ParentBlock1 --&gt; ParentBlock2 --&gt; ParentBlock3    end    subgraph ChildChain[\"fa:fa-link Child Chain (L2)\"]        style ChildChain fill:#95E1D3,stroke:#00868B        ChildBlock1[\"fa:fa-cube Block 1\"]        ChildBlock2[\"fa:fa-cube Block 2\"]        ChildBlock3[\"fa:fa-cube Block 3\"]        ChildBlock1 --&gt; ChildBlock2 --&gt; ChildBlock3    end    Layer0Network1 --&gt; |\"Infrastructure Support\"| ParentChain    Layer0Network2 --&gt; |\"Infrastructure Support\"| ParentChain    ChildBlock3 --&gt; |\"Transaction Data\"| Checkpoint[\"fa:fa-flag-checkered Checkpoint\"]    Checkpoint --&gt; |\"State Root\"| ParentBlock2  Layer 0 (L0): The foundation layer of blockchain ecosystems, providing the underlying infrastructure and protocols that enable interoperability between different blockchain networks. L0 solutions aim to solve scalability and communication issues across multiple chains. Examples include Polkadot, Cosmos, and Avalanche.  Layer 1 (L1) - Parent Chain: The base blockchain protocol, such as Bitcoin or Ethereum. This is the main blockchain that provides the fundamental security and consensus mechanisms. L1 chains are typically slower but offer the highest level of security and decentralization. They handle the final settlement of transactions and maintain the network‚Äôs core functionality.  Layer 2 (L2) - Child Chain: Scaling solutions built on top of Layer 1 to improve transaction speed and reduce costs. L2 solutions process most transactions off the main chain and periodically settle them on the L1 network. Examples include Lightning Network for Bitcoin and Optimistic Rollups for Ethereum. L2 solutions offer faster and cheaper transactions while leveraging the security of the underlying L1 network.Shardinggraph LR    subgraph beaconChain[\"fa:fa-sitemap Beacon Chain\"]        style beaconChain fill:#F2F2F2,stroke:#666666        beaconNode1[\"fa:fa-server Beacon Node 1\"]        beaconNode2[\"fa:fa-server Beacon Node 2\"]        beaconNode3[\"fa:fa-server Beacon Node 3\"]        beaconNode1 --&gt; |\"Connects to\"|beaconNode2        beaconNode2 --&gt; |\"Connects to\"|beaconNode3    end    subgraph shardChains[\"fa:fa-network-wired Shard Chains\"]        style shardChains fill:#C2E0C6,stroke:#5FB483        shardChain1[\"fa:fa-link Shard Chain 1\"]        shardChain2[\"fa:fa-link Shard Chain 2\"]        shardChain3[\"fa:fa-link Shard Chain 3\"]    end    beaconChain --&gt; |\"Links to\"|shardChain1    beaconChain --&gt; |\"Links to\"|shardChain2    beaconChain --&gt; |\"Links to\"|shardChain3    subgraph shardChain1[\"Shard Chain 1\"]        style shardChain1 fill:#E6F2FF,stroke:#B8D8F2        shardBlock1[\"fa:fa-cube Shard Block 1\"]        shardBlock2[\"fa:fa-cube Shard Block 2\"]        shardBlock1 --&gt; |\"Connects to\"|shardBlock2    end    subgraph shardChain2[\"Shard Chain 2\"]        style shardChain2 fill:#E6F2FF,stroke:#B8D8F2        shardBlock3[\"fa:fa-cube Shard Block 3\"]        shardBlock4[\"fa:fa-cube Shard Block 4\"]        shardBlock3 --&gt; |\"Connects to\"|shardBlock4    end    subgraph shardChain3[\"Shard Chain 3\"]        style shardChain3 fill:#E6F2FF,stroke:#B8D8F2        shardBlock5[\"fa:fa-cube Shard Block 5\"]        shardBlock6[\"fa:fa-cube Shard Block 6\"]        shardBlock5 --&gt; |\"Connects to\"|shardBlock6    end      Beacon Chain (Coordinator): This is the main chain that manages the entire sharded network. It assigns validators to different shard chains and ensures consensus across the network.        Shard Chains (Parallel Processing): These are smaller chains that run in parallel, each processing a portion of the network‚Äôs transactions and state. This parallel processing significantly increases the overall transaction throughput of the blockchain.        Shard Blocks: Blocks on individual shard chains contain transactions and data relevant to that specific shard.  Blockchain Trilelema (DSS)graph LR    subgraph BlockchainTrilemma[\"Blockchain Trilemma\"]        Decentralization[\"fa:fa-network-wired Decentralization\"]        Scalability[\"fa:fa-expand Scalability\"]        Security[\"fa:fa-lock Security\"]    end      subgraph Explanation[\"Explanation\"]        ScalabilityDef[\"Ability to handle\\nmany transactions\"]        SecurityDef[\"Protection against\\nattacks and failures\"]        DecentralizationDef[\"Distribution of\\ncontrol and power\"]    end    Scalability --- ScalabilityDef    Security --- SecurityDef    Decentralization --- DecentralizationDef    classDef primary fill:#FFFFDE,stroke:#333,stroke-width:2px;    classDef secondary fill:#DEFFEF,stroke:#333,stroke-width:2px;    classDef tertiary fill:#DEDEFF,stroke:#333,stroke-width:2px;    class BlockchainTrilemma primary;    class Explanation secondary;    class Scalability,Security,Decentralization tertiary;The blockchain trilemma is a concept coined by Vitalik Buterin that proposes a set of three main issues ‚Äî decentralization, security and scalability ‚Äî that developers encounter when building blockchains, forcing them to ultimately sacrifice one ‚Äúaspect‚Äù for as a trade-off to accommodate the other two.Spot TradingSpot trading is the buying and selling of assets for immediate delivery and payment. It involves the exchange of assets at the current market price, with transactions settled within a short period. Spot trading is commonly used in traditional financial markets and cryptocurrency exchanges.graph TD    userHasBTC[\"üìà User has 1 BTC\"]    buyBTC[\"üìà Buys 1 BTC at $10,000\"]    sellBTC[\"üìà Sells 1 BTC at $11,000\"]    profit[\"üíµ Profit: $1,000\"]    loss[\"üîª Loss: $1,000\"]    userHasBTC --&gt; |\"buys BTC\"|buyBTC    buyBTC --&gt; |\"BTC at $11,000\"|sellBTC    sellBTC --&gt; |\"profit\"|profit    sellBTC --&gt; |\"loss\"|lossFuture TradingFuture trading is a financial contract where parties agree to buy or sell an asset at a future date for a predetermined price. It allows investors to speculate on the price movement of an asset without owning it. Future trading is commonly used in commodities, currencies, and cryptocurrencies.graph TD    userHasBTC[\"üìà User has 1 BTC\"]    openLongFutures[\"üìÑ Opens long futures position with 10x leverage\"]    priceOutcome[\"üìÖ After 1 month\"]    profit[\"üíµ Profit: $10,000 if BTC at $11,000\"]    loss[\"üîª Loss: $10,000 if BTC at $9,000\"]    userHasBTC --&gt; |\"opens position\"|openLongFutures    openLongFutures --&gt; |\"1 month duration\"|priceOutcome    priceOutcome --&gt; |\"BTC at $11,000\"|profit    priceOutcome --&gt; |\"BTC at $9,000\"|lossShort SellingShort selling is a trading strategy where an investor borrows an asset and sells it at the current price, with the expectation that the price will fall. The investor then buys back the asset at a lower price, returns it to the lender, and profits from the price difference.graph TD    userHasBTC[\"üìà User has 1 BTC\"]    openShortPosition[\"üìÑ Opens short position with 10x leverage\"]    priceOutcome[\"üìÖ After shorting BTC\"]    profit[\"üíµ Profit: $10,000 if BTC at $9,000\"]    loss[\"üîª Loss: $10,000 if BTC at $11,000\"]    userHasBTC --&gt; |\"opens short\"|openShortPosition    openShortPosition --&gt; |\"price change\"|priceOutcome    priceOutcome --&gt; |\"BTC at $9,000\"|profit    priceOutcome --&gt; |\"BTC at $11,000\"|loss"
  },
  
  {
    "title": "AKIRA TORIYAMA SONG | Gone Too Soon | ANIMETRIX [DRAGON BALL Z]",
    "url": "/posts/akira-toriyama-song-gone-too-soon/",
    "categories": "music",
    "tags": "music, anime",
    "date": "2024-07-03 23:00:00 +0700",
    





    
    "snippet": "I have known Dragon Ball since I was a child. Back then, I dreamed of being a character who could help Goku in his battles. I still have this habit as an adult when I have trouble sleeping. The fig...",
    "content": "I have known Dragon Ball since I was a child. Back then, I dreamed of being a character who could help Goku in his battles. I still have this habit as an adult when I have trouble sleeping. The fight with Frieza, the first time I saw Goku turn Super Saiyan ‚Äì those moments are forever in my mind.Enjoy this touching tribute to Akira Toriyama, the creator of Dragon Ball. The song is called ‚ÄúGone Too Soon‚Äù by ANIMETRIX."
  },
  
  {
    "title": "Unlocking the Unconscious: Freud's Psychoanalytic Theory",
    "url": "/posts/unlocking-the-unconscious-freud-psychoanalytic-theory/",
    "categories": "psychology",
    "tags": "psychology",
    "date": "2024-07-02 23:00:00 +0700",
    





    
    "snippet": "Sigmund FreudSigmund Freu (/Ààz…™…°m änt fr…î…™d/), born in 1856 in Austria, was a neurologist and the founder of psychoanalysis. He revolutionized the understanding of the human mind and its workings, i...",
    "content": "Sigmund FreudSigmund Freu (/Ààz…™…°m änt fr…î…™d/), born in 1856 in Austria, was a neurologist and the founder of psychoanalysis. He revolutionized the understanding of the human mind and its workings, introducing groundbreaking concepts like the unconscious, the id, ego, and superego, and the importance of childhood experiences in shaping personality.graph TB    Freud[\"Sigmund Freud\"]    Psychoanalysis[\"Psychoanalysis\"]    Freud --&gt;|Founder of| Psychoanalysis    Freud --&gt;|Born in 1856 in Austria| FreudDetails[\"Neurologist and founder of psychoanalysis\"]    Freud --&gt;|Revolutionized understanding of the human mind| FreudConcepts[\"Introduced the unconscious, id, ego, and superego, \\nimportance of childhood experiences\"]    Psychoanalysis --&gt;|Therapeutic method to explore and interpret the unconscious| Techniques[\"Techniques like dream analysis and free association\"]    Techniques --&gt;|Used to bring unconscious conflicts to conscious awareness for resolution| Resolution[\"Resolution of conflicts\"]    style Freud fill:#FFD700,stroke:#000,stroke-width:2px    style Psychoanalysis fill:#FF6347,stroke:#000,stroke-width:2px    style FreudDetails fill:#FFD700,stroke:#000,stroke-width:2px    style FreudConcepts fill:#FFA500,stroke:#000,stroke-width:2px    style Techniques fill:#FF6347,stroke:#000,stroke-width:2px    style Resolution fill:#FF4500,stroke:#000,stroke-width:2pxMindgraph LR    subgraph Conscious[\"Conscious\"]        style Conscious fill:#FFF2CC,stroke:#D6B656        Thoughts[\"Thoughts\"]:::thoughtsColor        Perceptions[\"Perceptions\"]:::perceptionsColor    end    subgraph Preconscious[\"Preconscious\"]        style Preconscious fill:#C2E0C6,stroke:#5FB483        Memories[\"Memories\"]:::memoriesColor        StoredKnowledge[\"Stored Knowledge\"]:::storedKnowledgeColor    end    subgraph Unconscious[\"Unconscious\"]        style Unconscious fill:#E1D5E7,stroke:#9673A6        Fears[\"Fears\"]:::fearsColor        ViolentMotives[\"Violent Motives\"]:::violentMotivesColor        ImmoralUrges[\"Immoral Urges\"]:::immoralUrgesColor        SelfishNeeds[\"Selfish Needs\"]:::selfishNeedsColor        IrrationalWishes[\"Irrational Wishes\"]:::irrationalWishesColor        ShamefulExperiences[\"Shameful Experiences\"]:::shamefulExperiencesColor        UnacceptableSexualDesires[\"Unacceptable Sexual Desires\"]:::unacceptableSexualDesiresColor    end    Thoughts --&gt; Memories    Perceptions --&gt; Memories    Memories --&gt; Fears    Memories --&gt; ViolentMotives    Memories --&gt; ImmoralUrges    Memories --&gt; SelfishNeeds    Memories --&gt; IrrationalWishes    Memories --&gt; ShamefulExperiences    Memories --&gt; UnacceptableSexualDesires    style Thoughts fill:#FFD700,stroke:#333,stroke-width:2px    style Perceptions fill:#FFA500,stroke:#333,stroke-width:2px    style Memories fill:#98FB98,stroke:#333,stroke-width:2px    style StoredKnowledge fill:#20B2AA,stroke:#333,stroke-width:2px    style Fears fill:#FF6347,stroke:#333,stroke-width:2px    style ViolentMotives fill:#FF4500,stroke:#333,stroke-width:2px    style ImmoralUrges fill:#DB7093,stroke:#333,stroke-width:2px    style SelfishNeeds fill:#FF1493,stroke:#333,stroke-width:2px    style IrrationalWishes fill:#FF69B4,stroke:#333,stroke-width:2px    style ShamefulExperiences fill:#C71585,stroke:#333,stroke-width:2px    style UnacceptableSexualDesires fill:#8B008B,stroke:#333,stroke-width:2pxThe ConsciousThe conscious mind encompasses everything that we are aware of at any given moment. This includes our current thoughts, perceptions, and feelings. It is the aspect of our mental processing that we can think about and discuss rationally.  Thoughts: These are the ideas and considerations actively occupying our minds.  Perceptions: These are our interpretations of sensory information from our environment.The PreconsciousThe preconscious contains thoughts and feelings that are not currently in our conscious awareness but can be brought to consciousness easily. It serves as a bridge between the conscious and unconscious parts of the mind.  Memories: These are past experiences that can be recalled when needed.  Stored Knowledge: This includes learned information and skills that we can access when necessary.The UnconsciousThe unconscious mind is the largest and most enigmatic part of Freud‚Äôs model. It comprises thoughts, memories, and desires that are outside of conscious awareness but still influence our behavior and emotions. The contents of the unconscious are often disturbing or socially unacceptable, leading to their repression.  Fears: Deep-seated anxieties that influence our behavior.  Violent Motives: Aggressive impulses kept out of conscious awareness.  Immoral Urges: Desires that conflict with societal norms and personal ethics.  Selfish Needs: Strong desires for self-gratification that are socially unacceptable.  Irrational Wishes: Desires that do not align with rational thought or reality.  Shameful Experiences: Past events that cause feelings of shame.  Unacceptable Sexual Desires: Sexual urges that are considered taboo.Interaction Between the PartsFreud‚Äôs theory suggests that the different parts of the mind interact dynamically:  Conscious Thoughts and Perceptions can trigger memories in the preconscious.  Memories in the Preconscious can influence the unconscious, leading to the emergence of repressed fears, motives, and desires.Personality Structuregraph TD;    subgraph Id[\"Id\"]        style Id fill:#F5B7B1,stroke:#C0392B        BasicNeeds[\"Basic Needs\"]        Instincts[\"Instincts\"]        PleasurePrinciple[\"Pleasure Principle\"]    end    subgraph Ego[\"Ego\"]        style Ego fill:#AED6F1,stroke:#2980B9        RealityPrinciple[\"Reality Principle\"]        Balance[\"Balances Id and Superego\"]        Rationalizes[\"Rationalizes Instincts\"]        SelfPerception[\"Self-Perception\"]    end    subgraph Superego[\"Superego\"]        style Superego fill:#D7BDE2,stroke:#8E44AD        MoralPrinciples[\"Moral Principles\"]        Ethics[\"Ethics of Thoughts and Actions\"]        SocialAcceptance[\"Social Acceptance\"]        SenseOfGuilt[\"Sense of Guilt\"]        ExternalInfluences[\"External Influences\"]    end    Id --&gt;|Impulses| Ego    Superego --&gt;|Moral Standards| Ego    Ego --&gt;|Balances| Id    Ego --&gt;|Balances| Superego    Id --&gt; BasicNeeds    Id --&gt; Instincts    Id --&gt; PleasurePrinciple    Ego --&gt; RealityPrinciple    Ego --&gt; Balance    Ego --&gt; Rationalizes    Ego --&gt; SelfPerception    Superego --&gt; MoralPrinciples    Superego --&gt; Ethics    Superego --&gt; SocialAcceptance    Superego --&gt; SenseOfGuilt    Superego --&gt; ExternalInfluencesId: The most primitive part of the personality, driven by the pleasure principle. It seeks immediate gratification of basic needs and desires, such as hunger, thirst, and sexual urges. It is impulsive, irrational, and unconscious.Ego: The mediator between the Id and the Superego, operating on the reality principle. It seeks to satisfy the Id‚Äôs desires in socially acceptable ways. It is rational, logical, and conscious.Superego: The moral compass of the personality, driven by the morality principle. It represents internalized ideals and values learned from parents and society. It strives for perfection and adherence to social norms. It is both conscious and unconscious.OverallFreud‚Äôs work has had a profound and lasting impact on psychology. While his theories have their flaws and limitations, they opened up new avenues for understanding the human mind and the complexities of behavior. Many of his concepts have been refined and integrated into modern psychotherapy practices, while others have been discarded or revised.graph LR    FreudCriticisms[\"Criticisms of Freud's Theories\"]        FreudCriticisms --&gt; OveremphasisSexuality[\"Overemphasis on Sexuality\"]    FreudCriticisms --&gt; LackScientificRigor[\"Lack of Scientific Rigor\"]    FreudCriticisms --&gt; Unfalsifiability[\"Unfalsifiability\"]    FreudCriticisms --&gt; BiasSubjectivity[\"Bias and Subjectivity\"]    FreudCriticisms --&gt; LimitedSample[\"Limited Sample\"]    FreudCriticisms --&gt; NeglectChildhoodTrauma[\"Neglect of Childhood Trauma\"]    FreudCriticisms --&gt; GenderBias[\"Gender Bias\"]    FreudCriticisms --&gt; PseudoscientificMethods[\"Pseudoscientific Methods\"]        style FreudCriticisms fill:#FFD700,stroke:#333,stroke-width:2px    style OveremphasisSexuality fill:#FF6347,stroke:#333,stroke-width:2px    style LackScientificRigor fill:#87CEEB,stroke:#333,stroke-width:2px    style Unfalsifiability fill:#FFA500,stroke:#333,stroke-width:2px    style BiasSubjectivity fill:#98FB98,stroke:#333,stroke-width:2px    style LimitedSample fill:#DB7093,stroke:#333,stroke-width:2px    style NeglectChildhoodTrauma fill:#FF4500,stroke:#333,stroke-width:2px    style GenderBias fill:#8B008B,stroke:#333,stroke-width:2px    style PseudoscientificMethods fill:#C71585,stroke:#333,stroke-width:2pxIt‚Äôs important to approach Freud‚Äôs work with a critical eye, acknowledging both its contributions and its shortcomings. His ideas continue to spark debate and research, demonstrating their enduring relevance in the ongoing exploration of the human psyche."
  },
  
  {
    "title": "Hashing Passwords",
    "url": "/posts/password-hashing/",
    "categories": "cryptography",
    "tags": "security, cryptography",
    "date": "2024-06-30 07:00:00 +0700",
    





    
    "snippet": "Why Hash Passwords? üõ°Ô∏èPlaintext storage is dangerousgraph LRsubgraph storingPasswords[\"fa:fa-database Storing Passwords\"]    plainText[\"fa:fa-file-alt Password: 'password123'\"] --&gt;|\"Insecure: Ea...",
    "content": "Why Hash Passwords? üõ°Ô∏èPlaintext storage is dangerousgraph LRsubgraph storingPasswords[\"fa:fa-database Storing Passwords\"]    plainText[\"fa:fa-file-alt Password: 'password123'\"] --&gt;|\"Insecure: Easily readable if database is breached\"| dataThief[\"fa:fa-user-secret Data Thief\"]endstyle plainText stroke:#800style dataThief fill:#ffcccc,stroke:#800Hashing protects against breachesgraph LRsubgraph passwordProtection[\"fa:fa-shield-alt Password Protection\"]    user[\"fa:fa-user User\"] --&gt;|enter|plainText[\"fa:fa-file-alt Password: 'password123'\"]        plainText --&gt; hashFunction[\"fa:fa-key Hash Function (e.g., SHA-256)\"]    hashFunction --&gt;|\"Irreversible process\"| storedHash[\"fa:fa-lock Stored Hash (e.g., '9f86d0...1343c')\"]    databaseBreach[\"fa:fa-exclamation-triangle Database Breach\"] --&gt;|\"Cannot reverse hash to get password\"| storedHashendstyle passwordProtection fill:#e0ffe0,stroke:#080style user fill:#ccffcc,stroke:#080style hashFunction fill:#ccffcc,stroke:#080style storedHash fill:#cce5ff,stroke:#080style databaseBreach fill:#ffe0e0,stroke:#800  Insecure Plaintext: Storing passwords in plain text is like leaving your front door unlocked. If a database is breached, attackers can easily steal and use the passwords.  Hashing as Protection: Hashing transforms passwords into unique, scrambled codes (hashes). This process is irreversible, meaning even if hackers get the hashes, they can‚Äôt directly obtain the original passwords.Hashing is a one-way streetgraph LRsubgraph hashingConcepts[\"fa:fa-road Hashing Concepts\"]    input[\"fa:fa-unlock-alt Password: 'password123'\"] --&gt;|Hashing Algorithm| hashOutput[\"fa:fa-lock Hash: 9f86d0...1343c\"]    hashOutput -.-&gt;|\"fa:fa-ban Cannot be reversed\"| inputendstyle input fill:#cce5ee,stroke:#008style hashOutput fill:#cce5ff,stroke:#008  Hash Functions: A hash function takes an input (password) and produces a fixed-size string of characters (hash). Good hash functions are designed so different inputs result in very different outputs.  One-Way Street: Hash functions are one-way, meaning you can‚Äôt reverse the process to get the original password from the hash. This is essential for security.Salt and Pepper: Adding Flavor to Security üßÇüå∂Ô∏èSaltSalting makes each hash unique, even for identical passwords  Salt: A unique, random value added to each password before hashing. This ensures that even if two users have the same password, their hashes will be different.graph LRsubgraph salting[\"fa:fa-snowflake Salting\"]    password[\"password123\"]    salt1[\"fa:fa-snowflake Unique Salt 1\"]    salt2[\"fa:fa-snowflake Unique Salt 2\"]    password --&gt; hashFunction1[\"fa:fa-key Hash Function\"]    salt1 --&gt; hashFunction1    password1[\"password123\"] &amp; salt2 --&gt; hashFunction2[\"fa:fa-key Hash Function\"]    hashFunction1 --&gt;|\"Different Hashes\"| hashOutput1[\"Hash 1 + embed Salt 1\"]    hashFunction2 --&gt;|\"Different Hashes\"| hashOutput2[\"Hash 2 + embed Salt 2\"]endstyle salt1 fill:#ffebcc,stroke:#880style salt2 fill:#ffebcc,stroke:#880style hashFunction1 fill:#cce5ff,stroke:#008style hashFunction2 fill:#cce5ff,stroke:#008style hashOutput1 fill:#fff0cc,stroke:#880style hashOutput2 fill:#fff0cc,stroke:#880Sample code for salting and verifying passwordsconst bcrypt = require('bcrypt');const saltRounds = 10;// Hashing a passwordfunction hashPassword(password) {    const hash = bcrypt.hashSync(password, saltRounds);    return hash;}// Verifying a passwordfunction verifyPassword(storedHash, inputPassword) {    return bcrypt.compareSync(inputPassword, storedHash);}// Example usageconst password = '123445';const hash = hashPassword(password);console.log(`Hash: ${hash}`);const isValid = verifyPassword(hash, '123445');console.log(`Password is valid: ${isValid}`);Table: usersSample table structure for storing salted passwords:            id      username      email      password_hash                  1      joe      joe@gmail.com      hashed_password_with_salt_for_joe              2      doe      doe@gmail.com      hashed_password_with_salt_for_doe      PepperPepper adds a secret layergraph LRsubgraph peppering[\"fa:fa-pepper-hot Peppering\"]    plaintextPassword[\"fa:fa-keyboard Plaintext Password\"] --&gt; |fa:fa-plus|pepper[\"fa:fa-pepper-hot Secret Pepper\"]    saltedHash[\"üßÇ Salted Hash\"]    pepper --&gt; hashFunction    saltedHash --&gt; hashFunction    hashFunction --&gt; pepperedHash[\"fa:fa-lock Peppered Hash\"]endstyle plaintextPassword fill:#ffebcc,stroke:#800style pepper fill:#ffcccc,stroke:#800style saltedHash fill:#ccffcc,stroke:#080style hashFunction fill:#ccffcc,stroke:#080style pepperedHash fill:#fff0cc,stroke:#880  Pepper: A secret key added to the hashing process. Unlike salt, pepper is not stored with the hash, adding an extra layer of security. Typically kept in a separate, secure location such as in the application code or a secure environment variable. Even if the database is compromised, attackers won‚Äôt have access to the pepper.Sample code for peppering passwordsconst bcrypt = require('bcrypt');class PasswordService {    constructor(config) {        this.saltRounds = config.saltRounds || 10;        this.pepper = config.pepper;        if (!this.pepper) {            throw new Error('Missing required pepper configuration');        }    }    async hashPassword(password) {        try {            const pepperedPassword = password + this.pepper;  // Simple concatenation            return await bcrypt.hash(pepperedPassword, this.saltRounds);        } catch (error) {            throw new Error('Password hashing failed');        }    }    async verifyPassword(storedHash, inputPassword) {        try {            const pepperedPassword = inputPassword + this.pepper;  // Simple concatenation            return await bcrypt.compare(pepperedPassword, storedHash);        } catch (error) {            throw new Error('Password verification failed');        }    }}// sample usage( async () =&gt; {    const passwordService = new PasswordService({        saltRounds: 10,        pepper: 'salty'    });    const password = 'password123';    const hashedPassword = await passwordService.hashPassword(password);    console.log(hashedPassword);    const isPasswordCorrect = await passwordService.verifyPassword(hashedPassword, password);    console.log(isPasswordCorrect);} )()Popular Hashing Algorithms üßÆDifferent algorithms offer varying strengthsgraph LRsubgraph hashingAlgorithms[\"fa:fa-calculator Hashing Algorithms\"]    pbkdf2[\"fa:fa-calculator PBKDF2\"]    bcrypt[\"fa:fa-calculator BCrypt\"]    argon2[\"fa:fa-calculator Argon2\"]endpbkdf2 --&gt;|\"Many iterations, slower\"| strength[\"fa:fa-trophy Strength\"]bcrypt --&gt;|\"Memory-hard, even slower\"| strengthargon2 --&gt;|\"Memory-hard, customizable, current standard\"| strengthstyle hashingAlgorithms fill:#e0f0e0,stroke:#080style pbkdf2 fill:#ffebcc,stroke:#880style bcrypt fill:#cce5ff,stroke:#008style argon2 fill:#ccffcc,stroke:#080style strength fill:#ffe0e0,stroke:#800  PBKDF2: Uses multiple iterations of a hash function to make it slower and more resistant to brute-force attacks.  BCrypt: Specifically designed for password hashing. It‚Äôs slower than PBKDF2 and uses more memory, making it harder to crack.  Argon2: The current standard for password hashing. It‚Äôs memory-hard, highly customizable, and resistant to various attacks.Sample code for hashing and verifying passwords using Argon2const argon2 = require('argon2');async function hashPassword(password) {  // Hashing the password  const hash = await argon2.hash(password, {    type: argon2.argon2id, // Recommended variant    memoryCost: 2 ** 16,   // Adjust memory usage (higher is slower)    timeCost: 3,           // Adjust processing time (higher is slower)    parallelism: 1         // Number of parallel threads (usually 1)  });  return hash;}async function verifyPassword(password, hash) {  // Verifying the password against the stored hash  try {    if (await argon2.verify(hash, password)) {      return true; // Password matches    } else {      return false; // Password does not match    }  } catch (err) {    // Handle potential errors (e.g., invalid hash)    console.error('Error verifying password:', err);    return false;  }}// Example usageasync function main() {  const password = 'your_super_secret_password';  // Hash the password  const hashedPassword = await hashPassword(password);  console.log('Hashed password:', hashedPassword);  // Verify the password (replace with the actual stored hash)  const isMatch = await verifyPassword(password, hashedPassword);  console.log('Password match:', isMatch); }main();Avoiding Weak AlgorithmsOutdated algorithms like MD5 and SHA-1 should be avoided due to their vulnerability to collision attacks. DES is insecure because of its short 56-bit key, making it susceptible to brute-force attacks. LANMAN is weak as it splits passwords, converts them to uppercase, and uses DES, leading to easy brute-forcing. Using SHA-2 without salting is also insecure, as it leaves hashes open to rainbow table attacksgraph LRsubgraph outdatedAlgorithms[\"fa:fa-exclamation-triangle Outdated Algorithms to Avoid\"]    md5[\"fa:fa-hashtag MD5\"]    sha1[\"fa:fa-hashtag SHA-1\"]    des[\"fa:fa-lock DES\"]    lanman[\"fa:fa-key LANMAN\"]    sha2NoSalt[\"fa:fa-hashtag SHA-2 (without salt)\"]endmd5 --&gt; |\"Vulnerable to collision attacks and fast brute-force attacks\"| insecure[\"fa:fa-user-secret Insecure\"]sha1 --&gt; |\"Vulnerable to collision attacks\"| insecuredes --&gt; |\"56-bit key length is too short, easily brute-forced\"| insecurelanman --&gt; |\"Splits password, converts to uppercase, uses DES, easily brute-forced\"| insecuresha2NoSalt --&gt; |\"Without salt, vulnerable to rainbow table attacks\"| insecurestyle md5 fill:#ffcccc,stroke:#800style sha1 fill:#ffcccc,stroke:#800style des fill:#ffcccc,stroke:#800style lanman fill:#ffcccc,stroke:#800style sha2NoSalt fill:#ffcccc,stroke:#800style insecure fill:#eee,stroke:#802Considerations for Secure Password Hashing üîígraph LRsubgraph bestPractices[\"fa:fa-thumbs-up Best Practices\"]    useArgon2[\"fa:fa-check-circle Use Argon2 (if possible)\"]    tuneParameters[\"fa:fa-sliders-h Tune parameters (memory, time)\"]    updateRegularly[\"fa:fa-sync Update algorithm if needed\"]endstyle bestPractices fill:#e0ffe0,stroke:#080style useArgon2 fill:#ccffcc,stroke:#080style tuneParameters fill:#fff0cc,stroke:#880style updateRegularly fill:#cce5ff,stroke:#008graph LRsubgraph advanced[\"fa:fa-star Advanced Techniques\"]    keyStretching[\"fa:fa-arrows-alt-h Key Stretching (e.g., bcrypt)\"]    hardwareModules[\"fa:fa-microchip Hardware Security Modules (HSMs)\"]endstyle advanced fill:#e0f0e0,stroke:#080style keyStretching fill:#cce5ff,stroke:#008style hardwareModules fill:#ccffcc,stroke:#080  Prioritize Argon2 as the recommended algorithm.  Tune parameters (memory, time) to balance security and performance based on your system‚Äôs resources.  Stay updated with the latest security guidelines and consider upgrading algorithms as needed.  Key stretching: Applies the hash function multiple times to further slow down attackers.  Hardware Security Modules (HSMs): Dedicated hardware for secure cryptographic operations, offering an extra layer of protection.Use Hash to ensure data integrityHash functions are not only used for password hashing but also for ensuring data integrity. Hash-based message authentication codes (HMACs) combine a secret key with a hash function to create a secure way to verify data integrity.graph    subgraph message    style message fill:#ccffcc,stroke:#,stroke-width:2px    data[\"üíΩ data\"]    subgraph hash[\"HMAC\"]        subgraph sha[\"‚úÇÔ∏è SHA-256\"]            style sha fill:#ccffff,stroke:#,stroke-width:2px            data1[\"üíΩ data\"]            secret[\"„äôÔ∏è secret\"]        end  endendCreate a hash-based message authenticationconst crypto = require('crypto');const secretKey = 'your-secret-key';const data = 'your-message-data';const hmac = crypto.createHmac('sha256', secretKey);hmac.update(data);const hash = hmac.digest('hex');console.log('HMAC:', hash);Verify the integrity of the data using the HMACconst receivedData = 'your-message-data';const receivedHMAC = 'received-hmac-value';  // This should be the HMAC value received along with the dataconst verifyHmac = crypto.createHmac('sha256', secretKey);verifyHmac.update(receivedData);const newHash = verifyHmac.digest('hex');if (newHash === receivedHMAC) {    console.log('Data integrity verified');} else {    console.log('Data has been tampered with');}Keywords To Remembergraph    subgraph      data[\"üíΩ\"]    salt[\"üßÇ\"]  end  subgraph      hmac[\"#Ô∏è\"]    secret[\"„äôÔ∏è\"]  end  pepper[\"üå∂Ô∏è\"]   hash[\"‚úÇÔ∏è\"]    "
  },
  
  {
    "title": "Public key cryptography",
    "url": "/posts/public-key-cryptography/",
    "categories": "cryptography",
    "tags": "security, cryptography",
    "date": "2024-06-30 07:00:00 +0700",
    





    
    "snippet": "Public key cryptography is a method of encrypting or signing data with two different keys and making one of the keys, the public key, available for anyone to use. The other key is known as the priv...",
    "content": "Public key cryptography is a method of encrypting or signing data with two different keys and making one of the keys, the public key, available for anyone to use. The other key is known as the private key. Data encrypted with the public key can only be decrypted with the private key.PEM Format for Keysgraph TD    pemFile[\"fa:fa-file-code PEM File for Public Key\"] --&gt; header[\"fa:fa-header Header\"]    pemFile --&gt; base64Content[\"fa:fa-file-code Base64 Encoded Content\"]    pemFile --&gt; footer[\"fa:fa-footer Footer\"]    base64Content --&gt; originalBinaryData[\"fa:fa-binary Original Binary Data\"]    originalBinaryData --&gt; publicKeyComponents[\"fa:fa-key Public Key Components\"]    header --&gt;|Public Key| headerContent[\"fa:fa-key -----BEGIN PUBLIC KEY-----\"]    footer --&gt;|Public Key| footerContent[\"fa:fa-key -----END PUBLIC KEY-----\"]    classDef header fill:#ffcc00,stroke:#333,stroke-width:2px;    classDef footer fill:#ff6600,stroke:#333,stroke-width:2px;    classDef data fill:#66ccff,stroke:#333,stroke-width:2px;    style pemFile fill:#ffffff,stroke:#333,stroke-width:2px;    style header fill:#ffcc00,stroke:#333,stroke-width:2px;    style base64Content fill:#66ccff,stroke:#333,stroke-width:2px;    style footer fill:#ff6600,stroke:#333,stroke-width:2px;    style originalBinaryData fill:#66ccff,stroke:#333,stroke-width:2px;    style publicKeyComponents fill:#66ccff,stroke:#333,stroke-width:2px;    subgraph pemStructure[\"PEM File Structure\"]        pemFile        header        base64Content        footer    endThe PEM (Privacy-Enhanced Mail) format is a widely used format for encoding cryptographic keys and certificates. It uses Base64 encoding to represent binary data and typically includes header and footer lines that identify the type of encoded data. For example, a PEM-encoded RSA private key might look like this:-----BEGIN RSA PRIVATE KEY-----MIIEowIBAAKCAQEA7XwH4q2+1T...-----END RSA PRIVATE KEY-----PEM files can contain various types of data, such as private keys, public keys, and certificates, and are commonly used in SSL/TLS configurations and other cryptographic applications.RSA algorithm OverviewThe RSA algorithm relies on prime numbers to generate a modulus and exponents that form the public and private keys. These keys are stored in PEM files, which facilitate secure key exchange and storage. RSA key sizes vary, with larger sizes offering better securityCommon key sizes include:  1024 bits: Considered insecure for most applications today.  2048 bits: Provides a good balance between security and performance, widely used.  3072 bits: Offers higher security, recommended for more sensitive data.  4096 bits: Provides very high security but at the cost of performance, used in highly secure environments.Even if a hacker can find the public key and ciphertext, it takes an enormous amount of computational effort to determine the secret data. The resources required for such an attack (time, computational power, and energy) are far beyond the reach of any individual or organization, making RSA a robust choice for securing data in transit.Encrypt with the recipient‚Äôs public key (Confidentiality).graph TD    subgraph senders[\"fa:fa-users Many Senders\"]        sender1[\"fa:fa-user Sender 1\"]:::inputNode        sender2[\"fa:fa-user Sender 2\"]:::inputNode        sender3[\"fa:fa-user Sender 3\"]:::inputNode        publicKey[\"fa:fa-key Public Key\"]    end        recipient[\"fa:fa-user-shield Recipient\"]:::outputNode        subgraph recipient[\"fa:fa-user-shield Recipient\"]        privateKey[\"fa:fa-lock Private Key\"]    end    senders --&gt; encryptionProcess[\"fa:fa-lock Encryption Process \"]:::encryptionNode    encryptionProcess --&gt; ciphertext[\"fa:fa-file-code Ciphertext\"]:::outputNode    ciphertext --&gt; recipient    classDef inputNode fill:#ffcccc,stroke:#333,stroke-width:2px;    classDef encryptionNode fill:#e6f2ff,stroke:#0066cc,stroke-width:2px;    classDef outputNode fill:#ccffcc,stroke:#333,stroke-width:2px;This ensures that only the recipient, who holds the corresponding private key, can decrypt the message. This is the most common scenario for secure communication.  Secure Messaging Apps (End-to-End Encryption):Apps like Signal, WhatsApp, and Telegram use public key encryption to ensure that only the intended recipient can read your messages. Your message is encrypted with the recipient‚Äôs public key, and only their private key can decrypt it.Signing with your private key (Authentication and Integrity)graph TD        subgraph sender[\"fa:fa-user Sender\"]        privateKey[\"fa:fa-lock Private Key\"]    end    signingProcess[\"fa:fa-lock Signing Process (Sender's Private Key)\"]:::encryptionNode    signature[\"fa:fa-file-signature Signature\"]:::outputNode    sender --&gt; signingProcess    signingProcess --&gt; signature    subgraph recipients[\"fa:fa-users Recipients\"]        recipient1[\"fa:fa-user Recipient 1\"]:::outputNode        recipient2[\"fa:fa-user Recipient 2\"]:::outputNode        recipient3[\"fa:fa-user Recipient 3\"]:::outputNode          publicKey2[\"fa:fa-key Public Key\"]    end    signature --&gt; recipients    classDef inputNode fill:#ffcccc,stroke:#333,stroke-width:2px;    classDef encryptionNode fill:#e6f2ff,stroke:#0066cc,stroke-width:2px;    classDef outputNode fill:#ccffcc,stroke:#333,stroke-width:2px;This creates a digital signature that anyone can verify using your public key. This proves that the message came from you and hasn‚Äôt been altered.  Cryptocurrency Wallets:When you initiate a transaction, your wallet signs it with your private key. This signature proves you own the funds and authorize the transaction.Writing the Encryption CodeTo implement asymmetric encryption in Node.js, we need to create functions for encrypting and decrypting data using the RSA algorithm. We will use the crypto module, which provides cryptographic functionality.Encrypting Dataconst crypto = require('crypto');function encryptAsymmetric(publicKey, plaintext) {  const buffer = Buffer.from(plaintext, 'utf8');  const encrypted = crypto.publicEncrypt(publicKey, buffer);  return encrypted.toString('base64');}graph TDsubgraph input[\"fa:fa-key Input\"]plaintext[\"fa:fa-file-alt Plaintext\"]:::inputNodepublicKey[\"fa:fa-key Public Key\"]:::inputNodeendinput --&gt; encryptAsymmetric[\"fa:fa-lock Encryption (RSA)\"]:::encryptionNodesubgraph encryptAsymmetric[\"fa:fa-lock Encryption (RSA)\"]plaintextBuffer[\"fa:fa-file-alt Plaintext Buffer\"]:::processNode --&gt; publicEncrypt[\"fa:fa-lock Public Encrypt\"]:::processNodeendencryptAsymmetric --&gt; ciphertext[\"fa:fa-file-code Ciphertext\"]:::outputNodesubgraph output[\"fa:fa-file-export Output\"]ciphertextendclassDef inputNode fill:#ffcccc,stroke:#333,stroke-width:2px;classDef encryptionNode fill:#e6f2ff,stroke:#0066cc,stroke-width:2px;classDef processNode fill:#ccffcc,stroke:#333,stroke-width:2px;classDef outputNode fill:#ccf,stroke:#333,stroke-width:2px;Encryption Process: A Simplified Explanation  Input:          Plaintext: This is your original data‚Äîthe information you want to protect. It could be a text message, a document, or any digital data.      Public Key: This is the public part of the key pair. It can be shared with anyone who needs to encrypt data for you.        Encryption (RSA):          The encryption algorithm (RSA) takes the plaintext and the public key as input.      Inside the encryption ‚Äúmachine,‚Äù the plaintext is converted to a buffer and then encrypted using the public key.        Output:          Ciphertext: The result of encryption is the ciphertext. This is the transformed version of your original data. It is unreadable without the private key.      Decrypting Datafunction decryptAsymmetric(privateKey, ciphertext) {  const buffer = Buffer.from(ciphertext, 'base64');  const decrypted = crypto.privateDecrypt(privateKey, buffer);  return decrypted.toString('utf8');}graph TDsubgraph input[\"fa:fa-key Input\"]ciphertext[\"fa:fa-file-code Ciphertext\"]:::inputNodeprivateKey[\"fa:fa-lock Private Key\"]:::inputNodeendinput --&gt; decryptAsymmetric[\"fa:fa-unlock Decryption (RSA)\"]:::decryptionNodesubgraph decryptAsymmetric[\"fa:fa-unlock Decryption (RSA)\"]ciphertextBuffer[\"fa:fa-file-code Ciphertext Buffer\"]:::processNode --&gt; privateDecrypt[\"fa:fa-unlock Private Decrypt\"]:::processNodeenddecryptAsymmetric --&gt; plaintext[\"fa:fa-file-alt Plaintext\"]:::outputNodesubgraph output[\"fa:fa-file-import Output\"]plaintextendclassDef inputNode fill:#ffcccc,stroke:#333,stroke-width:2px;classDef decryptionNode fill:#e6f2ff,stroke:#0066cc,stroke-width:2px;classDef processNode fill:#ccffcc,stroke:#333,stroke-width:2px;classDef outputNode fill:#ccffcc,stroke:#333,stroke-width:2px;Decryption Process: A Simplified Explanation  Input:          Ciphertext: The encrypted data that needs to be decrypted.      Private Key: The private part of the key pair. It should be kept secret and only used by the recipient.        Decryption (RSA):          The decryption algorithm (RSA) takes the ciphertext and the private key as input.      Inside the decryption ‚Äúmachine,‚Äù the ciphertext is converted to a buffer and then decrypted using the private key.        Output:          Plaintext: The original data after decryption.      Codeconst crypto = require('crypto');// Function to encrypt data using the public keyfunction encryptAsymmetric(publicKey, plaintext) {  const buffer = Buffer.from(plaintext, 'utf8');  const encrypted = crypto.publicEncrypt(publicKey, buffer);  return encrypted.toString('base64');}// Function to decrypt data using the private keyfunction decryptAsymmetric(privateKey, ciphertext) {  const buffer = Buffer.from(ciphertext, 'base64');  const decrypted = crypto.privateDecrypt(privateKey, buffer);  return decrypted.toString('utf8');}// Example Usageconst { generateKeyPairSync } = require('crypto');const { publicKey, privateKey } = generateKeyPairSync('rsa', {  modulusLength: 2048,  publicKeyEncoding: {    type: 'spki',    format: 'pem'  },  privateKeyEncoding: {    type: 'pkcs8',    format: 'pem'  }});const originalText = 'This is a secret message.';const ciphertext = encryptAsymmetric(publicKey, originalText);console.log('Ciphertext:', ciphertext);const decryptedText = decryptAsymmetric(privateKey, ciphertext);console.log('Decrypted Text:', decryptedText);OutputCiphertext: GxfqW7TB5I7cZZlWKMnjYbBpXMbjYQidHOFrRsECC2ccdXyfSR9hRn52F8pWYs0cBZIkNqiF7+IJW9Lz3mYIicj625vfERxUTwpUfgqQbp4WJ+0qIJNmjv2CZ7gM/W508XxygIpEe4LxRsF+sVGlUjJBhvJMqopyIfF3mVqyUrqUXiAyBMrcf+TouIqFNc7c6rqMV13wsvLghP8XSrtDM7bVE0Szj14hTWwsDA9yz9N9PsvCM3D58nh8fLKA+8SZV6fgxcUyeH6cVQvBct2moLFIjckOns5TIKpVF90nt5XahXdGQndN88R9DXF/MokWoZQTD6T8t6uMitsuW8yk4Q==Decrypted Text: This is a secret message.Writing the Signing CodeSigning Data (Creating the Signature)const crypto = require('crypto');function signData(privateKey, data) {  // Create a Sign object using RSA-SHA256  const sign = crypto.createSign('RSA-SHA256');  // Update the Sign object with the data to be signed  sign.update(data);  // Generate the signature using the private key  const signature = sign.sign(privateKey, 'base64');  return signature;}graph TD%% StylinglinkStyle default stroke:#333,stroke-width:2px;subgraph input[\"fa:fa-key Input\"]data[\"fa:fa-file-alt Data\"]:::inputNodeprivateKey[\"fa:fa-lock Private Key\"]:::inputNodeendsubgraph hashProcess[\"fa:fa-hashtag Hashing Process\"]hashData[\"fa:fa-hashtag Hash Data (SHA-256)\"]:::processNodeendsubgraph signProcess[\"fa:fa-signature Signing Process\"]signHash[\"fa:fa-signature Sign Hash (RSA)\"]:::processNodeendinput --&gt; hashDatahashData --&gt; hashValue[\"fa:fa-hashtag Hash Value\"]:::intermediateNodehashValue --&gt; signHashprivateKey --&gt; signHashsignHash --&gt; signature[\"fa:fa-signature Signature\"]:::outputNodeclassDef inputNode fill:#ffcccc,stroke:#333,stroke-width:2px;classDef processNode fill:#ccffcc,stroke:#333,stroke-width:2px;classDef intermediateNode fill:#ffffcc,stroke:#333,stroke-width:2px;classDef outputNode fill:#ccf,stroke:#333,stroke-width:2px;Explanation:  Input:          data: The data you want to digitally sign (e.g., a document‚Äôs contents).      privateKey: Your private RSA key, which is kept secret.        Process (signData function):          Create a Sign Object: A Sign object is created using the crypto.createSign() method. The ‚ÄòRSA-SHA256‚Äô algorithm is specified, meaning the signature will be generated using RSA and the SHA-256 hash function for data integrity.     -      Update with Data: The sign.update(data) method feeds the data you want to sign into the Sign object.      Generate Signature: The sign.sign(privateKey, 'base64') method uses your private key to generate the signature.  The signature is returned in base64 encoding for easy transmission and storage.        Output:          signature: A unique string representing the signed data.      Verifying the Signature (Authentication)function verifySignature(publicKey, data, signature) {  // Create a Verify object using RSA-SHA256  const verify = crypto.createVerify('RSA-SHA256');  // Update the Verify object with the original data  verify.update(data);  // Verify the signature using the public key  return verify.verify(publicKey, signature, 'base64');}graph TDsubgraph input[\"fa:fa-key Input\"]data[\"fa:fa-file-alt Data\"]:::inputNodepublicKey[\"fa:fa-key Public Key\"]:::inputNodesignature[\"fa:fa-signature Signature\"]:::inputNodeendinput --&gt; verifySignature[\"fa:fa-check Verify (RSA-SHA256)\"]:::verifyNodesubgraph verifySignature[\"fa:fa-check Verify (RSA-SHA256)\"]updateData[\"fa:fa-pencil-alt Update Data\"]:::processNode --&gt; verify[\"fa:fa-check Verify\"]:::processNodeendverifySignature --&gt; result[\"fa:fa-check-circle Result\"]:::outputNodeclassDef inputNode fill:#ffcccc,stroke:#333,stroke-width:2px;classDef verifyNode fill:#e6f2ff,stroke:#0066cc,stroke-width:2px;classDef processNode fill:#ccffcc,stroke:#333,stroke-width:2px;classDef outputNode fill:#ccffcc,stroke:#333,stroke-width:2px;Explanation:  Input:          publicKey: The public RSA key associated with the private key that was used to create the signature.      data: The original data that was signed.      signature: The signature to be verified.        Process (verifySignature function):          Create a Verify Object: A Verify object is created using the crypto.createVerify() method. The ‚ÄòRSA-SHA256‚Äô algorithm is again specified.      Update with Data:  The verify.update(data) method feeds the original data into the Verify object.      Verify Signature: The verify.verify(publicKey, signature, 'base64') method uses the public key to verify the signature. It checks if the signature matches the data and was indeed created with the corresponding private key.        Output:          true or false:  The function returns true if the signature is valid (the data is authentic and unaltered), and false if the signature is invalid.      Codeconst crypto = require('crypto');const { generateKeyPairSync } = crypto; // Import generateKeyPairSync// ----- Digital Signature Functions -----function signData(privateKey, data) {  const sign = crypto.createSign('RSA-SHA256');   sign.update(data);  return sign.sign(privateKey, 'base64');}function verifySignature(publicKey, data, signature) {  const verify = crypto.createVerify('RSA-SHA256');  verify.update(data);  return verify.verify(publicKey, signature, 'base64');}// ----- Example Usage (Signing Only) -----const { publicKey, privateKey } = generateKeyPairSync('rsa', {  modulusLength: 2048,  publicKeyEncoding: { type: 'spki', format: 'pem' },  privateKeyEncoding: { type: 'pkcs8', format: 'pem' }});const dataToSign = 'This is the important document I want to sign.';// Sign the dataconst signature = signData(privateKey, dataToSign);console.log('Signature:', signature);// Verify the signature (just for demonstration)const isSignatureValid = verifySignature(publicKey, dataToSign, signature);console.log('Is Signature Valid?', isSignatureValid); OutputSignature: VU1dykMNDmXoiiQr+JAxg0ng2cfUyj77gZuJzUD6uk+2f7T7ll1TqEDufRhP37hLuD01ACdfXOnITCI4dpAAZkjzknPnlFfT436p4uSlTgEPCtu75fynnQl5WQeZQPVih6ffkk1gejzonh8fLsEFBKYH+efjNCsjNGlXZOIlqUC5sigFEzq0YbTiPkc4PNf/KUW5kaKNy682OktBOz9dqTc5zR4MTF6kRvgTQMysY3/N2HaZ9/eWf3YMF6bgltFBclVoMZn0z8MfXhnwp3kMeKhmtb26QvLjxJq7q2+G3YMxGV91jf0yxZl8gTMKIbD0lYrGELa4rCK0OS0JaIzWPA==Is Signature Valid? trueSecurity ConsiderationsKey Protection  Keep your private key absolutely confidential. Anyone with access to your private key can decrypt messages intended for you.  Share your public key freely. It‚Äôs designed to be public and used for encryption.  Consider using hardware security modules (HSMs) for secure key storage.Key Size  Use a sufficiently large key size (e.g., 2048 or 4096 bits) to ensure the security of your encrypted data.Handling Sensitive DataEnsure that sensitive data, such as keys and plaintext, are securely handled in your application. Avoid logging sensitive information and use secure memory management practices.Hashing and Encryption in Blockchain Transactions and MiningBlockchain technology uses a combination of hashing, encryption, and digital signatures to secure transactions and create new blocks in the chain. Here‚Äôs a simplified overview of how these cryptographic techniques work together in a blockchain system:graph LR    subgraph Keys[\"fa:fa-key Keys\"]        style Keys fill:#C2E0C6,stroke:#5FB483        PrivateKey[\"fa:fa-lock Private Key (Sender)\"]        PublicKey[\"fa:fa-unlock Public Key (Sender/Recipient)\"]    end    PrivateKey --\"fa:fa-pen Signs\"--&gt; TransactionSignature    PublicKey --\"fa:fa-check Verifies\"--&gt; TransactionSignature    subgraph Transaction[\"fa:fa-file-alt Transaction\"]      style Transaction fill:#E1D5E7,stroke:#9673A6      TransactionSignature[\"fa:fa-signature Digital Signature\"]      subgraph TransactionData[\"fa:fa-database Transaction Data\"]        style TransactionData fill:#FFF2CC,stroke:#D6B656        TransactionDetails[\"fa:fa-info-circle Transaction Details&lt;br/&gt;(Sender, Recipient, Amount)\"]    end    end    subgraph Verification[\"fa:fa-shield-alt Verification\"]      style Verification fill:#D4EDDA,stroke:#4CAF50      BlockchainNetwork[\"fa:fa-network-wired Blockchain Network\"]    end    Transaction --\"fa:fa-check Verified by\"--&gt; BlockchainNetwork    subgraph Block[\"fa:fa-cube Block (Example)\"]        style Block fill:#F0F0F0,stroke:#888888        BlockHeader[\"fa:fa-header Block Header\"]        BlockData[\"fa:fa-database Block Data (Multiple Transactions)\"]    end    Transaction -.-&gt; BlockData    BlockHeader --&gt; BlockHash[\"fa:fa-fingerprint Block Hash\"]    BlockHeader --&gt; PreviousBlockHash[\"fa:fa-link Previous Block's Hash\"]    BlockHeader --&gt; Nonce[\"fa:fa-random Nonce\"]  Transaction Signing: Transactions are digitally signed by the sender‚Äôs private key. Verified by the recipient and network nodes using the sender‚Äôs public key.  Block Hashing: Each block contains a header and transaction data. Miners repeatedly hash the block header with different nonce to find a valid hash, a unique code representing the block‚Äôs contents. This hash must meet specific criteria set by the blockchain‚Äôs protocol. The first miner to find such a hash gets to add the block to the blockchain and is rewarded with cryptocurrency.Keywords To Remembergraph   subgraph 1[\" \"]    publicKey[\"fa:fa-key Public Key\"]:::keyNode    privateKey[\"fa:fa-lock Private Key\"]:::lockNode    pem[\"fa:fa-file-code PEM Format\"]:::pemNode  end   subgraph 2[\" \"]    rsa[\"fa:fa-lock RSA\"]:::rsaNode    sha[\"fa:fa-scissors SHA256\"]:::shaNode  end  subgraph 3[\" \"]    crypto[\"fa:fa-b Crypto Module\"]:::cryptoNode    ciphertext[\"fa:fa-file-code Ciphertext\"]:::cipherNode    encryption[\"fa:fa-lock Encryption\"]:::encryptionNode    signing[\"fa:fa-signature Signing\"]:::signingNode  end   subgraph 4[\" \"]    authenticity[\"fa:fa-shield-alt Authenticity\"]:::authenticityNode    integrity[\"fa:fa-check Integrity\"]:::integrityNode  end  subgraph 5[\" \"]    transaction[\"fa:fa-paper-plane Transaction\"]:::authenticityNode    block[\"fa:fa-boxes Block\"]:::integrityNode    nounce[\"fa:fa-gem Nounce\"]:::cipherNode  endclassDef keyNode fill:#ffcc99,stroke:#333,stroke-width:2px;classDef lockNode fill:#99ccff,stroke:#333,stroke-width:2px;classDef pemNode fill:#ccff99,stroke:#333,stroke-width:2px;classDef rsaNode fill:#ffccff,stroke:#333,stroke-width:2px;classDef shaNode fill:#ff9999,stroke:#333,stroke-width:2px;classDef cryptoNode fill:#ccccff,stroke:#333,stroke-width:2px;classDef cipherNode fill:#ffff99,stroke:#333,stroke-width:2px;classDef encryptionNode fill:#99ffcc,stroke:#333,stroke-width:2px;classDef signingNode fill:#ffcc66,stroke:#333,stroke-width:2px;classDef authenticityNode fill:#ff9966,stroke:#333,stroke-width:2px;classDef integrityNode fill:#99ff66,stroke:#333,stroke-width:2px;  "
  },
  
  {
    "title": "Symmetric encryption",
    "url": "/posts/symmetric-encryption/",
    "categories": "cryptography",
    "tags": "security, cryptography",
    "date": "2024-06-29 07:00:00 +0700",
    





    
    "snippet": "Symmetric encryption is a type of encryption where the same key is used for both encryption and decryption. We will cover the basics of encryption, how to implement it using Node.js, and how to ens...",
    "content": "Symmetric encryption is a type of encryption where the same key is used for both encryption and decryption. We will cover the basics of encryption, how to implement it using Node.js, and how to ensure the security of your encrypted data.Introduction to Symmetric Encryptiongraph TD;  symmetricEncryption[\"fa:fa-lock Symmetric Encryption\"]:::mainNode  useCasesSymmetric[\"fa:fa-key single key for encryption and decryption\"]:::useCaseNode  aesGcm[\"fa:fa-key AES-256-GCM\"]:::overviewNode  symmetricEncryption --&gt;  useCasesSymmetric  symmetricEncryption --&gt;  aesGcm  subgraph useCases[\"Use Cases\"]    securingData[\"fa:fa-database Securing Data at Rest\"]:::dataNode    protectingData[\"fa:fa-bus Protecting Data in Transit\"]:::transitNode    encryptingInfo[\"fa:fa-lock Encrypting Sensitive Information\"]:::infoNode  end  useCasesSymmetric --&gt; securingData  useCasesSymmetric --&gt; protectingData  useCasesSymmetric --&gt; encryptingInfo  classDef mainNode fill:#ffcccc,stroke:#333,stroke-width:2px;  classDef useCaseNode fill:#ccffcc,stroke:#333,stroke-width:2px;  classDef overviewNode fill:#ccccff,stroke:#333,stroke-width:2px;  classDef useCaseSubgraph fill:#f9f9f9,stroke:#333,stroke-width:2px;  classDef dataNode fill:#ffecb3,stroke:#333,stroke-width:2px;  classDef transitNode fill:#c3e6cb,stroke:#333,stroke-width:2px;  classDef infoNode fill:#b3e5fc,stroke:#333,stroke-width:2px;Symmetric encryption is a method of encryption where a single key is used to both encrypt and decrypt data. This means that both the sender and receiver must have access to the same key. Symmetric encryption is generally faster and less complex than asymmetric encryption, making it suitable for encrypting large amounts of data.Use Cases of Symmetric EncryptionSymmetric encryption is commonly used in various scenarios, including:  Securing data at rest (e.g., encrypting files on disk)  Protecting data in transit (e.g., securing network communication)  Encrypting sensitive information in databasesAES-256-GCM AES (Advanced Encryption Standard) is a widely used symmetric encryption algorithm. The AES-256-GCM (Galois/Counter Mode) variant provides both encryption and authentication, ensuring data confidentiality and integrity.Writing the Encryption CodeTo implement symmetric encryption in Node.js, we need to create functions for encrypting and decrypting data using the AES-256-GCM algorithm. We will use the crypto module, which provides cryptographic functionality.const crypto = require('crypto');function encryptSymmetric(key, plaintext) {  const iv = crypto.randomBytes(12);   const cipher = crypto.createCipheriv('aes-256-gcm', Buffer.from(key, 'base64'), iv);  let ciphertext = cipher.update(plaintext, 'utf8', 'base64');  ciphertext += cipher.final('base64');  const tag = cipher.getAuthTag().toString('base64');  return {    ciphertext,    iv: iv.toString('base64'),     tag   };}function decryptSymmetric(key, ciphertext, iv, tag) {  const decipher = crypto.createDecipheriv('aes-256-gcm', Buffer.from(key, 'base64'), Buffer.from(iv, 'base64'));  decipher.setAuthTag(Buffer.from(tag, 'base64'));  let plaintext = decipher.update(ciphertext, 'base64', 'utf8');  plaintext += decipher.final('utf8');  return plaintext;}encryptSymmetricgraph TD        subgraph input[\"fa:fa-key Input\"]        plaintext[\"fa:fa-file-alt Plaintext\"]:::inputNode        key[\"fa:fa-key Key\"]:::inputNode    end    input --&gt; encryptSymmetric[\"fa:fa-lock Encryption (AES-256-GCM)\"]:::encryptionNode    subgraph encryptSymmetric[\"fa:fa-lock Encryption (AES-256-GCM)\"]            iv[\"fa:fa-random Initialization Vector (IV)\"]:::processNode --&gt; cipher[\"fa:fa-lock Cipher\"]:::processNode     end    encryptSymmetric --&gt; ciphertext[\"fa:fa-file-code Ciphertext\"]:::outputNode    encryptSymmetric --&gt; authTag[\"fa:fa-tag Authentication Tag\"]:::outputNode    encryptSymmetric --&gt; ivOutput[\"fa:fa-random Initialization Vector (IV)\"]:::outputNode    subgraph output[\"fa:fa-file-export Output\"]        ciphertext        authTag        ivOutput    endclassDef inputNode fill:#ffcccc,stroke:#333,stroke-width:2px;classDef encryptionNode fill:#e6f2ff,stroke:#0066cc,stroke-width:2px;classDef processNode fill:#ccffcc,stroke:#333,stroke-width:2px;classDef outputNode fill:#ccf,stroke:#333,stroke-width:2px;Encryption Process: A Simplified Explanation  Input:          Plaintext: This is your original data‚Äîthe information you want to protect. It could be a text message, a document, or any digital data.      Key: This is your secret password. It‚Äôs a string of characters or numbers that you and the recipient of the encrypted data must both know.      Initialization Vector (IV): This is a random value generated for each encryption operation. It adds an extra layer of security to prevent identical plaintexts from resulting in the same ciphertext.        Encryption (AES-256-GCM):          The encryption algorithm (AES-256-GCM) is like a powerful machine. It takes the plaintext, key, and IV as input.      Inside the encryption ‚Äúmachine,‚Äù these inputs are combined through a complex mathematical process. This process scrambles the plaintext in a way that‚Äôs impossible to understand without the correct key and IV.        Output:          Ciphertext: The result of encryption is the ciphertext. This is the transformed version of your original data. It is unreadable without the key and IV.      Authentication Tag: This is a unique code generated during the encryption process. It‚Äôs like a digital signature that ensures the ciphertext hasn‚Äôt been tampered with.      decryptSymmetricgraph TD        subgraph input[\"fa:fa-key Input\"]        ciphertext[\"fa:fa-file-code Ciphertext\"]:::inputNode        key[\"fa:fa-key Key\"]:::inputNode        ivInput[\"fa:fa-random Initialization Vector (IV)\"]:::inputNode        authTag[\"fa:fa-tag Authentication Tag\"]:::inputNode    end    input --&gt; decryptSymmetric[\"fa:fa-unlock Decryption (AES-256-GCM)\"]:::decryptionNode    subgraph decryptSymmetric[\"fa:fa-unlock Decryption (AES-256-GCM)\"]        decipher[\"fa:fa-unlock Decipher\"]:::processNode        decipher --&gt; plaintext1[\"fa:fa-file-alt Plaintext\"]:::processNode    end    decryptSymmetric --&gt; plaintext[\"fa:fa-file-alt Plaintext\"]:::outputNode    subgraph output[\"fa:fa-file-import Output\"]        plaintext    endclassDef inputNode fill:#ffcccc,stroke:#333,stroke-width:2px;classDef decryptionNode fill:#e6f2ff,stroke:#0066cc,stroke-width:2px;classDef processNode fill:#ccffcc,stroke:#333,stroke-width:2px;classDef outputNode fill:#ccf,stroke:#333,stroke-width:2px;  Input:          Ciphertext: The encrypted data that needs to be decrypted.      Key: The secret key used for both encryption and decryption.      Initialization Vector (IV): The IV used during encryption, needed for decryption.      Authentication Tag: The tag used to verify the integrity and authenticity of the data.        Decryption Process (AES-256-GCM):          Decipher: The process of transforming ciphertext back into plaintext using the AES-256-GCM algorithm.      Plaintext: The original data after decryption.        Output:          Plaintext: The result of the decryption process.      Security ConsiderationsKey ManagementProper key management is crucial for the security of your encrypted data. Store keys securely and avoid hardcoding them in your source code.Using a Strong IVAlways use a strong and random initialization vector (IV) for each encryption operation. Reusing an IV can compromise the security of your encrypted data.Handling Sensitive DataEnsure that sensitive data, such as keys and plaintext, are securely handled in your application. Avoid logging sensitive information and use secure memory management practices.Full Source Codeconst crypto = require('crypto');function encryptSymmetric(key, plaintext) {  // Generate a random initialization vector (IV)  const iv = crypto.randomBytes(12);   // Create a Cipher object with the AES algorithm, key, and IV  const cipher = crypto.createCipheriv('aes-256-gcm', Buffer.from(key, 'base64'), iv);  // Encrypt the plaintext  let ciphertext = cipher.update(plaintext, 'utf8', 'base64');  ciphertext += cipher.final('base64');  // Retrieve the authentication tag  const tag = cipher.getAuthTag().toString('base64');  return {    ciphertext,    iv: iv.toString('base64'),     tag   };}function decryptSymmetric(key, ciphertext, iv, tag) {  // Create a Decipher object with the same algorithm, key, and IV  const decipher = crypto.createDecipheriv('aes-256-gcm', Buffer.from(key, 'base64'), Buffer.from(iv, 'base64'));  // Set the authentication tag  decipher.setAuthTag(Buffer.from(tag, 'base64'));  // Decrypt the ciphertext  let plaintext = decipher.update(ciphertext, 'base64', 'utf8');  plaintext += decipher.final('utf8');  return plaintext;}// Example Usageconst originalText = 'This is a secret message.';const key = crypto.randomBytes(32).toString('base64');const { ciphertext, iv, tag } = encryptSymmetric(key, originalText);console.log('Ciphertext:', ciphertext);console.log('IV:', iv);console.log('Tag:', tag);const decryptedText = decryptSymmetric(key, ciphertext, iv, tag);console.log('Decrypted Text:', decryptedText);Keywords To Remembergraph   subgraph \" \"    AES[\"fa:fa-lock AES-256-GCM\"]:::componentNode2    plaintext[\"fa:fa-file-alt Plaintext\"]:::componentNode2    ciphertext[\"fa:fa-file-code Ciphertext\"]:::componentNode2  end  subgraph \" \"    cryptoModule[\"fa:fa-b Crypto Module\"]:::componentNode    key[\"fa:fa-key key\"]:::componentNode    IV[\"fa:fa-random Initialization Vector (IV)\"]:::componentNode    authTag[\"fa:fa-tag Authentication Tag\"]:::componentNode    ciphertext[\"fa:fa-file-code Ciphertext\"]:::componentNode  end  subgraph \" \"    dataAtRest[\"fa:fa-database Data at Rest\"]:::stateNode    dataInTransit[\"fa:fa-bus Data in Transit\"]:::stateNode  endclassDef componentNode fill:#e6f2ff,stroke:#0066cc,stroke-width:2px;classDef componentNode2 fill:#eeffaa,stroke:#eeeeaa,stroke-width:2px;classDef stateNode fill:#ccffcc,stroke:#333,stroke-width:2px;Next Post: Hashing Passwords"
  }
  
]

